{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91bfe73",
   "metadata": {},
   "source": [
    "# Model Comparison and Analysis\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ded568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"results_dir\": \"model_training_results\",\n",
    "    \"output_dir\": \"model_comparison_analysis\",\n",
    "    \"plot_style\": {\n",
    "        \"figsize\": (15, 8),\n",
    "        \"dpi\": 300,\n",
    "        \"style\": \"whitegrid\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for subdir in ['plots', 'reports']:\n",
    "    os.makedirs(f\"{CONFIG['output_dir']}/{subdir}\", exist_ok=True)\n",
    "\n",
    "print(f\"📊 Model Comparison and Analysis\")\n",
    "print(f\"Results directory: {CONFIG['results_dir']}\")\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff72fc8",
   "metadata": {},
   "source": [
    "## Load Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a050782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_model_metrics(results_dir):\n",
    "    \"\"\"Load all model metrics from individual JSON files\"\"\"\n",
    "    metrics_dir = f\"{results_dir}/metrics\"\n",
    "    \n",
    "    if not os.path.exists(metrics_dir):\n",
    "        print(f\"❌ Metrics directory not found: {metrics_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    all_metrics = []\n",
    "    json_files = [f for f in os.listdir(metrics_dir) if f.endswith('.json') and not f.endswith('_error.json')]\n",
    "    \n",
    "    print(f\"📁 Found {len(json_files)} metric files\")\n",
    "    \n",
    "    for filename in json_files:\n",
    "        filepath = os.path.join(metrics_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            \n",
    "            # Skip error files or incomplete metrics\n",
    "            if 'error' in metrics or metrics.get('accuracy') is None:\n",
    "                continue\n",
    "                \n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load {filename}: {e}\")\n",
    "    \n",
    "    if not all_metrics:\n",
    "        print(\"❌ No valid metrics found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    print(f\"✅ Loaded {len(df)} successful model runs\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_label_mapping(results_dir):\n",
    "    \"\"\"Load label encoder mapping\"\"\"\n",
    "    mapping_path = f\"{results_dir}/label_encoders/label_mapping.json\"\n",
    "    \n",
    "    if os.path.exists(mapping_path):\n",
    "        with open(mapping_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(\"⚠️ Label mapping not found\")\n",
    "        return {}\n",
    "\n",
    "# Load all data\n",
    "df_metrics = load_all_model_metrics(CONFIG[\"results_dir\"])\n",
    "label_mapping = load_label_mapping(CONFIG[\"results_dir\"])\n",
    "\n",
    "if df_metrics.empty:\n",
    "    print(\"❌ No results found! Run training script first.\")\n",
    "else:\n",
    "    print(f\"📊 Ready to analyze {len(df_metrics)} model configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4780afa8",
   "metadata": {},
   "source": [
    "## Process and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_derived_metrics(df):\n",
    "    \"\"\"Add derived metrics and clean up data\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Extract model short names\n",
    "    df['model_short_name'] = df['model_name'].str.split('/').str[-1]\n",
    "    \n",
    "    # Create display names\n",
    "    df['display_name'] = df['model_short_name'] + '_config_' + df['config_id'].astype(str)\n",
    "    \n",
    "    # Add hyperparameter info if available\n",
    "    if 'hyperparameters' in df.columns:\n",
    "        df['learning_rate'] = df['hyperparameters'].apply(lambda x: x.get('learning_rate', 'N/A') if isinstance(x, dict) else 'N/A')\n",
    "        df['batch_size'] = df['hyperparameters'].apply(lambda x: x.get('batch_size', 'N/A') if isinstance(x, dict) else 'N/A')\n",
    "        df['max_length'] = df['hyperparameters'].apply(lambda x: x.get('max_length', 'N/A') if isinstance(x, dict) else 'N/A')\n",
    "        df['epochs'] = df['hyperparameters'].apply(lambda x: x.get('epochs', 'N/A') if isinstance(x, dict) else 'N/A')\n",
    "    \n",
    "    # Sort by performance\n",
    "    df = df.sort_values('f1_macro', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_model_performance(df):\n",
    "    \"\"\"Analyze overall model performance\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    analysis = {\n",
    "        'total_models': len(df),\n",
    "        'unique_model_types': df['model_short_name'].nunique(),\n",
    "        'best_model': {\n",
    "            'name': df.iloc[0]['model_name'],\n",
    "            'run_name': df.iloc[0]['run_name'],\n",
    "            'f1_macro': df.iloc[0]['f1_macro'],\n",
    "            'accuracy': df.iloc[0]['accuracy']\n",
    "        },\n",
    "        'performance_stats': {\n",
    "            'f1_macro': {\n",
    "                'mean': df['f1_macro'].mean(),\n",
    "                'std': df['f1_macro'].std(),\n",
    "                'min': df['f1_macro'].min(),\n",
    "                'max': df['f1_macro'].max()\n",
    "            },\n",
    "            'accuracy': {\n",
    "                'mean': df['accuracy'].mean(),\n",
    "                'std': df['accuracy'].std(),\n",
    "                'min': df['accuracy'].min(),\n",
    "                'max': df['accuracy'].max()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Process the data\n",
    "df_clean = add_derived_metrics(df_metrics)\n",
    "analysis = analyze_model_performance(df_clean)\n",
    "\n",
    "if analysis:\n",
    "    print(\"📊 PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total configurations tested: {analysis['total_models']}\")\n",
    "    print(f\"Unique model types: {analysis['unique_model_types']}\")\n",
    "    print(f\"Best model: {analysis['best_model']['name'].split('/')[-1]}\")\n",
    "    print(f\"Best F1-Macro: {analysis['best_model']['f1_macro']:.4f}\")\n",
    "    print(f\"Best Accuracy: {analysis['best_model']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bcc16",
   "metadata": {},
   "source": [
    "## Create Comprehensive Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_comparison_plot(df):\n",
    "    \"\"\"Create comprehensive model comparison plots\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"❌ No data to plot\")\n",
    "        return None\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot 1: F1-Macro vs Accuracy Scatter\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    scatter = ax1.scatter(df['accuracy'], df['f1_macro'], \n",
    "                         c=df.index, cmap='viridis', alpha=0.7, s=100)\n",
    "    ax1.set_xlabel('Accuracy')\n",
    "    ax1.set_ylabel('F1-Macro Score')\n",
    "    ax1.set_title('F1-Macro vs Accuracy')\n",
    "    \n",
    "    # Add labels for best models\n",
    "    for i, row in df.head(3).iterrows():\n",
    "        ax1.annotate(row['model_short_name'], \n",
    "                    (row['accuracy'], row['f1_macro']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    # Plot 2: Top 10 Models Bar Chart\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    top_models = df.head(10)\n",
    "    bars = ax2.barh(range(len(top_models)), top_models['f1_macro'], \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(top_models))))\n",
    "    ax2.set_xlabel('F1-Macro Score')\n",
    "    ax2.set_title('Top 10 Models by F1-Macro')\n",
    "    ax2.set_yticks(range(len(top_models)))\n",
    "    ax2.set_yticklabels([name[:20] + '...' if len(name) > 20 else name \n",
    "                        for name in top_models['display_name']], fontsize=8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "    \n",
    "    # Plot 3: Model Type Performance\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    model_performance = df.groupby('model_short_name')['f1_macro'].agg(['mean', 'max', 'count'])\n",
    "    model_performance = model_performance.sort_values('mean', ascending=True)\n",
    "    \n",
    "    bars = ax3.barh(range(len(model_performance)), model_performance['mean'])\n",
    "    ax3.set_xlabel('Average F1-Macro Score')\n",
    "    ax3.set_title('Average Performance by Model Type')\n",
    "    ax3.set_yticks(range(len(model_performance)))\n",
    "    ax3.set_yticklabels(model_performance.index, fontsize=8)\n",
    "    \n",
    "    # Plot 4: Hyperparameter Analysis - Learning Rate\n",
    "    if 'learning_rate' in df.columns:\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        lr_performance = df.groupby('learning_rate')['f1_macro'].mean().sort_values()\n",
    "        bars = ax4.bar(range(len(lr_performance)), lr_performance.values)\n",
    "        ax4.set_xlabel('Learning Rate')\n",
    "        ax4.set_ylabel('Average F1-Macro')\n",
    "        ax4.set_title('Performance by Learning Rate')\n",
    "        ax4.set_xticks(range(len(lr_performance)))\n",
    "        ax4.set_xticklabels([f'{lr:.0e}' if isinstance(lr, float) else str(lr) \n",
    "                            for lr in lr_performance.index], rotation=45)\n",
    "    \n",
    "    # Plot 5: Hyperparameter Analysis - Max Length\n",
    "    if 'max_length' in df.columns:\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        ml_performance = df.groupby('max_length')['f1_macro'].mean().sort_index()\n",
    "        bars = ax5.bar(range(len(ml_performance)), ml_performance.values)\n",
    "        ax5.set_xlabel('Max Length')\n",
    "        ax5.set_ylabel('Average F1-Macro')\n",
    "        ax5.set_title('Performance by Max Length')\n",
    "        ax5.set_xticks(range(len(ml_performance)))\n",
    "        ax5.set_xticklabels(ml_performance.index, rotation=45)\n",
    "    \n",
    "    # Plot 6: Performance Distribution\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.hist(df['f1_macro'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax6.axvline(df['f1_macro'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {df[\"f1_macro\"].mean():.3f}')\n",
    "    ax6.axvline(df['f1_macro'].median(), color='green', linestyle='--', \n",
    "               label=f'Median: {df[\"f1_macro\"].median():.3f}')\n",
    "    ax6.set_xlabel('F1-Macro Score')\n",
    "    ax6.set_ylabel('Frequency')\n",
    "    ax6.set_title('F1-Macro Score Distribution')\n",
    "    ax6.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = f\"{CONFIG['output_dir']}/plots/comprehensive_model_comparison.png\"\n",
    "    plt.savefig(plot_path, dpi=CONFIG['plot_style']['dpi'], bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return plot_path\n",
    "\n",
    "# Create the main comparison plot\n",
    "plot_path = create_comprehensive_comparison_plot(df_clean)\n",
    "\n",
    "if plot_path:\n",
    "    print(f\"📊 Comprehensive comparison plot saved: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97a88f",
   "metadata": {},
   "source": [
    "## Generate Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detailed_performance_report(df, analysis, label_mapping):\n",
    "    \"\"\"Create detailed performance report\"\"\"\n",
    "    if df.empty:\n",
    "        return None\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# 📊 COMPREHENSIVE MODEL PERFORMANCE REPORT\")\n",
    "    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"## 🎯 Executive Summary\")\n",
    "    report.append(f\"- **Total Models Evaluated**: {analysis['total_models']}\")\n",
    "    report.append(f\"- **Unique Model Types**: {analysis['unique_model_types']}\")\n",
    "    report.append(f\"- **Best Model**: {analysis['best_model']['name'].split('/')[-1]}\")\n",
    "    report.append(f\"- **Best F1-Macro Score**: {analysis['best_model']['f1_macro']:.4f}\")\n",
    "    report.append(f\"- **Best Accuracy**: {analysis['best_model']['accuracy']:.4f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Performance Statistics\n",
    "    report.append(\"## 📈 Performance Statistics\")\n",
    "    stats = analysis['performance_stats']\n",
    "    report.append(\"### F1-Macro Scores\")\n",
    "    report.append(f\"- Mean: {stats['f1_macro']['mean']:.4f} ± {stats['f1_macro']['std']:.4f}\")\n",
    "    report.append(f\"- Range: {stats['f1_macro']['min']:.4f} - {stats['f1_macro']['max']:.4f}\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"### Accuracy Scores\")\n",
    "    report.append(f\"- Mean: {stats['accuracy']['mean']:.4f} ± {stats['accuracy']['std']:.4f}\")\n",
    "    report.append(f\"- Range: {stats['accuracy']['min']:.4f} - {stats['accuracy']['max']:.4f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Top 10 Models\n",
    "    report.append(\"## 🏆 Top 10 Models\")\n",
    "    top_10 = df.head(10)\n",
    "    for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "        model_name = row['model_name'].split('/')[-1]\n",
    "        report.append(f\"{i:2d}. **{model_name}** (Config {row['config_id']})\")\n",
    "        report.append(f\"    - F1-Macro: {row['f1_macro']:.4f}\")\n",
    "        report.append(f\"    - Accuracy: {row['accuracy']:.4f}\")\n",
    "        if 'learning_rate' in row:\n",
    "            report.append(f\"    - Learning Rate: {row['learning_rate']}\")\n",
    "            report.append(f\"    - Batch Size: {row['batch_size']}\")\n",
    "            report.append(f\"    - Max Length: {row['max_length']}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Model Type Analysis\n",
    "    report.append(\"## 🔬 Model Type Analysis\")\n",
    "    model_analysis = df.groupby('model_short_name').agg({\n",
    "        'f1_macro': ['count', 'mean', 'std', 'max'],\n",
    "        'accuracy': ['mean', 'max']\n",
    "    }).round(4)\n",
    "    \n",
    "    for model_type in model_analysis.index:\n",
    "        report.append(f\"### {model_type}\")\n",
    "        row = model_analysis.loc[model_type]\n",
    "        report.append(f\"- Configurations tested: {row[('f1_macro', 'count')]}\")\n",
    "        report.append(f\"- Average F1-Macro: {row[('f1_macro', 'mean')]:.4f} ± {row[('f1_macro', 'std')]:.4f}\")\n",
    "        report.append(f\"- Best F1-Macro: {row[('f1_macro', 'max')]:.4f}\")\n",
    "        report.append(f\"- Best Accuracy: {row[('accuracy', 'max')]:.4f}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Hyperparameter Analysis\n",
    "    if 'learning_rate' in df.columns:\n",
    "        report.append(\"## ⚙️ Hyperparameter Analysis\")\n",
    "        \n",
    "        # Learning Rate\n",
    "        lr_analysis = df.groupby('learning_rate')['f1_macro'].agg(['count', 'mean', 'std']).round(4)\n",
    "        report.append(\"### Learning Rate Impact\")\n",
    "        for lr in lr_analysis.index:\n",
    "            row = lr_analysis.loc[lr]\n",
    "            report.append(f\"- **{lr}**: {row['mean']:.4f} ± {row['std']:.4f} (n={row['count']})\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Max Length\n",
    "        if 'max_length' in df.columns:\n",
    "            ml_analysis = df.groupby('max_length')['f1_macro'].agg(['count', 'mean', 'std']).round(4)\n",
    "            report.append(\"### Max Length Impact\")\n",
    "            for ml in sorted(ml_analysis.index):\n",
    "                row = ml_analysis.loc[ml]\n",
    "                report.append(f\"- **{ml}**: {row['mean']:.4f} ± {row['std']:.4f} (n={row['count']})\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Label Information\n",
    "    if label_mapping:\n",
    "        report.append(\"## 🏷️ Label Information\")\n",
    "        report.append(\"### Class Labels\")\n",
    "        for label_id, label_name in label_mapping.items():\n",
    "            report.append(f\"- {label_id}: {label_name}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"## 💡 Recommendations\")\n",
    "    \n",
    "    # Best model recommendation\n",
    "    best_model = df.iloc[0]\n",
    "    report.append(f\"1. **Use {best_model['model_short_name']}** as primary model\")\n",
    "    report.append(f\"   - Run name: {best_model['run_name']}\")\n",
    "    \n",
    "    # Hyperparameter recommendations\n",
    "    if 'learning_rate' in df.columns:\n",
    "        best_lr = df.loc[df['f1_macro'].idxmax(), 'learning_rate']\n",
    "        avg_performance_by_lr = df.groupby('learning_rate')['f1_macro'].mean()\n",
    "        best_avg_lr = avg_performance_by_lr.idxmax()\n",
    "        report.append(f\"2. **Optimal Learning Rate**: {best_avg_lr} (average performance)\")\n",
    "        \n",
    "        if 'max_length' in df.columns:\n",
    "            best_ml = df.loc[df['f1_macro'].idxmax(), 'max_length']\n",
    "            report.append(f\"3. **Optimal Max Length**: {best_ml}\")\n",
    "    \n",
    "    # Model diversity recommendation\n",
    "    top_3_models = df.head(3)['model_short_name'].unique()\n",
    "    if len(top_3_models) > 1:\n",
    "        report.append(f\"4. **Consider Ensemble**: Top performing model types are {', '.join(top_3_models)}\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(\"*Report generated by Model Comparison Analysis*\")\n",
    "    \n",
    "    # Save report\n",
    "    report_path = f\"{CONFIG['output_dir']}/reports/performance_report.md\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# Generate the detailed report\n",
    "report_path = create_detailed_performance_report(df_clean, analysis, label_mapping)\n",
    "\n",
    "if report_path:\n",
    "    print(f\"📝 Performance report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f52ca",
   "metadata": {},
   "source": [
    "## Save Complete Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60189a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "if not df_clean.empty:\n",
    "    results_path = f\"{CONFIG['output_dir']}/complete_model_comparison.csv\"\n",
    "    df_clean.to_csv(results_path, index=False)\n",
    "    print(f\"💾 Complete results saved: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e4987",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_final_summary(df, analysis):\n",
    "    \"\"\"Print final analysis summary\"\"\"\n",
    "    if df.empty or not analysis:\n",
    "        print(\"❌ No analysis results available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✅ ANALYSIS COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"📊 Results Summary:\")\n",
    "    print(f\"   - Total configurations analyzed: {analysis['total_models']}\")\n",
    "    print(f\"   - Best model: {analysis['best_model']['name'].split('/')[-1]}\")\n",
    "    print(f\"   - Best F1-Macro: {analysis['best_model']['f1_macro']:.4f}\")\n",
    "    print(f\"   - Best Accuracy: {analysis['best_model']['accuracy']:.4f}\")\n",
    "    print(f\"\\n📁 Files Generated:\")\n",
    "    print(f\"   - Comprehensive plot: {CONFIG['output_dir']}/plots/comprehensive_model_comparison.png\")\n",
    "    print(f\"   - Performance report: {CONFIG['output_dir']}/reports/performance_report.md\")\n",
    "    print(f\"   - Complete results: {CONFIG['output_dir']}/complete_model_comparison.csv\")\n",
    "    print(f\"\\n🎯 Analysis saved to: {CONFIG['output_dir']}\")\n",
    "\n",
    "# Print the final summary\n",
    "print_final_summary(df_clean, analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
