model,ece,brier_score,accuracy,avg_confidence,total_predictions
roberta-base_config_1,0.017144533880597113,0.19703311887415897,0.26865671641791045,0.28580125029850756,67
bert-base-uncased_config_3,0.13803981955223882,0.23805093918408735,0.417910447761194,0.32915496791044774,67
deberta-v3-base_config_2,0.06656411402985071,0.17627213218921403,0.22388059701492538,0.2827317292537313,67
deberta-v3-large_config_3,0.05503051283582089,0.16935630414904274,0.208955223880597,0.2639857367164179,67
roberta-large_config_2,0.13595698611940304,0.20734896209132342,0.5522388059701493,0.6120124192537313,67
Llama-3.2-1B-Instruct_config_1,0.3105092567164179,0.3080085497091556,0.34328358208955223,0.6537928388059702,67
DialoGPT-medium_config_1,0.04973047686567162,0.20422128748394164,0.26865671641791045,0.3173314562686568,67
deberta-v3-base_config_3,0.03364455940298508,0.1821693768306744,0.23880597014925373,0.2724505295522388,67
deberta-v3-large_config_2,0.06642100149253732,0.19901995555396415,0.29850746268656714,0.3416123041791045,67
roberta-large_config_3,0.06923099462686567,0.2159290189759734,0.3283582089552239,0.2888083519402986,67
distilbert-base-uncased_config_1,0.07636209328358212,0.1704606081917023,0.208955223880597,0.28531731716417913,67
bert-base-uncased_config_2,0.08738405880597011,0.2049856669050834,0.373134328358209,0.396464772238806,67
Llama-3.2-1B-Instruct_config_3,0.3008400219402985,0.3067642125005084,0.2835820895522388,0.5844221114925373,67
roberta-large_config_1,0.06288726597014935,0.18572655083063386,0.23880597014925373,0.3016932361194031,67
DialoGPT-medium_config_3,0.11094384074626865,0.20520230650287694,0.2537313432835821,0.3491026756716418,67
deberta-v3-base_config_1,0.05944274985074626,0.17129525724509398,0.208955223880597,0.26839797373134333,67
distilbert-base-uncased_config_3,0.04744647029850747,0.21335802920418234,0.34328358208955223,0.3171498855223881,67
roberta-base_config_2,0.14097446641791045,0.2459369138084,0.417910447761194,0.4671220971641791,67
distilbert-base-uncased_config_2,0.09483069597014926,0.2488077823994724,0.4925373134328358,0.4045079076119404,67
bert-base-uncased_config_1,0.12538321746268657,0.2182927989435213,0.3283582089552239,0.43648456552238807,67
roberta-base_config_3,0.12233903940298509,0.1956979349311373,0.26865671641791045,0.37740691402985077,67
Llama-3.2-1B-Instruct_config_2,0.3283496365671642,0.3296605292253675,0.3283582089552239,0.656707845522388,67
deberta-v3-large_config_1,0.006977883432835896,0.19581716702661447,0.26865671641791045,0.27563459985074634,67
DialoGPT-medium_config_2,0.11052909671641792,0.21342916135903772,0.26865671641791045,0.3414448017910448,67
