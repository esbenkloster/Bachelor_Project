{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067cd35b",
   "metadata": {},
   "source": [
    "# Class Consolidation Experiment\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b802c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CONFIG = {\n",
    "    \"data_path\": \"enhanced_dataset/training_ready_dataset.csv\",\n",
    "    \"splits_dir\": \"model_training_results/config\",\n",
    "    \"output_dir\": \"class_consolidation_results\",\n",
    "    \"model_name\": \"roberta-large\",\n",
    "    \"experiment_name\": \"source_issues_focal_loss\",\n",
    "    \"class_map\": {\n",
    "        \"unreliable source\": \"source_issues\",\n",
    "        \"slanted\": \"source_issues\",\n",
    "        \"false\": \"false\",\n",
    "        \"repurposed\": \"repurposed\",\n",
    "        \"decorative\": \"decorative\"\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"epochs\": 4,\n",
    "        \"lr\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_len\": 512,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"focal_gamma\": 2.0,\n",
    "    },\n",
    "    \"eval_steps\": 20,\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf588e",
   "metadata": {},
   "source": [
    "## Data Loading and Class Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_consolidate_data():\n",
    "    \"\"\"\n",
    "    Load data and apply class consolidation mapping.\n",
    "    Merges 'unreliable source' and 'slanted' into 'source_issues' category.\n",
    "    \"\"\"\n",
    "    \n",
    "    base = pd.read_csv(CONFIG['data_path'])\n",
    "    splits = {\n",
    "        k: pd.read_csv(f\"{CONFIG['splits_dir']}/{k}_split.csv\") \n",
    "        for k in ['train', 'val', 'test']\n",
    "    }\n",
    "    \n",
    "    def consolidate_classes(df):\n",
    "        df = df.copy()\n",
    "        df['label'] = df['label'].map(CONFIG['class_map'])\n",
    "        return df.dropna(subset=['label'])\n",
    "    \n",
    "    consolidated_splits = {k: consolidate_classes(df) for k, df in splits.items()}\n",
    "    \n",
    "    print(\"Class Distribution After Consolidation:\")\n",
    "    for split_name, split_df in consolidated_splits.items():\n",
    "        print(f\"\\n{split_name.upper()} SET:\")\n",
    "        class_counts = split_df['label'].value_counts()\n",
    "        for label, count in class_counts.items():\n",
    "            percentage = (count / len(split_df)) * 100\n",
    "            print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return base, consolidated_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9432ee",
   "metadata": {},
   "source": [
    "## Dataset Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae56596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(base, splits):\n",
    "    \"\"\"\n",
    "    Tokenize text data and encode labels for model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "    label_encoder = LabelEncoder().fit(splits['train']['label'])\n",
    "    \n",
    "    print(f\"\\nLabel Encoding:\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {label}: {i}\")\n",
    "    \n",
    "    def process_split(df):\n",
    "        df['label_id'] = label_encoder.transform(df['label'])\n",
    "        ds = Dataset.from_pandas(df[['text', 'label_id']].rename(columns={'label_id': 'label'}))\n",
    "        ds = ds.map(\n",
    "            lambda ex: tokenizer(\n",
    "                ex['text'], \n",
    "                truncation=True, \n",
    "                padding='max_length', \n",
    "                max_length=CONFIG['train_args']['max_len']\n",
    "            ), \n",
    "            batched=True\n",
    "        )\n",
    "        ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        return ds\n",
    "    \n",
    "    processed_datasets = {k: process_split(df) for k, df in splits.items()}\n",
    "    \n",
    "    return tokenizer, label_encoder, processed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87642986",
   "metadata": {},
   "source": [
    "## Focal Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1409798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss implementation for handling class imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        if self.alpha is not None:\n",
    "            self.alpha = self.alpha.to(logits.device)\n",
    "        \n",
    "        ce_loss = torch.nn.functional.cross_entropy(\n",
    "            logits, targets, reduction='none', weight=self.alpha\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "class FocalTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom trainer implementing focal loss for class-balanced training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, class_weights=None, gamma=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = FocalLoss(alpha=class_weights, gamma=gamma)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d0e146",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consolidated_model():\n",
    "    \"\"\"\n",
    "    Train RoBERTa model with class consolidation and focal loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    base, splits = load_and_consolidate_data()\n",
    "    tokenizer, label_encoder, datasets = prepare_datasets(base, splits)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CONFIG['model_name'], \n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights for balanced training\n",
    "    y_train = splits['train']['label'].map(\n",
    "        dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    ).values\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced', \n",
    "        classes=np.unique(y_train), \n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    print(f\"\\nClass Weights for Balanced Training:\")\n",
    "    for i, (label, weight) in enumerate(zip(label_encoder.classes_, class_weights)):\n",
    "        print(f\"  {label}: {weight:.3f}\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/tmp\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=CONFIG['eval_steps'],\n",
    "        save_steps=CONFIG['save_steps'],\n",
    "        logging_steps=CONFIG['logging_steps'],\n",
    "        per_device_train_batch_size=CONFIG['train_args']['batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['train_args']['batch_size'],\n",
    "        learning_rate=CONFIG['train_args']['lr'],\n",
    "        num_train_epochs=CONFIG['train_args']['epochs'],\n",
    "        weight_decay=CONFIG['train_args']['weight_decay'],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(eval_pred.label_ids, predictions),\n",
    "            \"f1\": f1_score(eval_pred.label_ids, predictions, average='macro')\n",
    "        }\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = FocalTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets['train'],\n",
    "        eval_dataset=datasets['val'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights_tensor,\n",
    "        gamma=CONFIG['train_args']['focal_gamma']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nStarting model training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer, label_encoder, splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed6147",
   "metadata": {},
   "source": [
    "## Performance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance(trainer, label_encoder, splits):\n",
    "    \"\"\"\n",
    "    Comprehensive performance analysis and results export.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = os.path.join(CONFIG['output_dir'], \"analysis_results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare test dataset for prediction\n",
    "    tokenizer = trainer.tokenizer\n",
    "    test_texts = splits['test']['text'].tolist()\n",
    "    test_labels = label_encoder.transform(splits['test']['label'])\n",
    "    \n",
    "    test_dataset = Dataset.from_dict({\n",
    "        'text': test_texts,\n",
    "        'label': test_labels\n",
    "    })\n",
    "    \n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda ex: tokenizer(\n",
    "            ex['text'], \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=CONFIG['train_args']['max_len']\n",
    "        ), \n",
    "        batched=True\n",
    "    )\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions_output = trainer.predict(test_dataset)\n",
    "    y_true = predictions_output.label_ids\n",
    "    y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "    overall_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\nOverall Test Performance:\")\n",
    "    print(f\"  Accuracy: {overall_accuracy:.3f}\")\n",
    "    print(f\"  Macro F1: {overall_f1:.3f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_report = classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=label_encoder.classes_, \n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPer-Class Performance:\")\n",
    "    for class_name in label_encoder.classes_:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"  {class_name}:\")\n",
    "        print(f\"    Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"    Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"    F1-Score: {metrics['f1-score']:.3f}\")\n",
    "        print(f\"    Support: {int(metrics['support'])}\")\n",
    "    \n",
    "    # Export detailed classification report\n",
    "    with open(f\"{results_dir}/classification_report.txt\", \"w\") as f:\n",
    "        f.write(\"CLASS CONSOLIDATION EXPERIMENT - DETAILED RESULTS\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(\"OVERALL PERFORMANCE:\\n\")\n",
    "        f.write(f\"  Test Accuracy: {overall_accuracy:.3f}\\n\")\n",
    "        f.write(f\"  Macro F1-Score: {overall_f1:.3f}\\n\\n\")\n",
    "        f.write(\"PER-CLASS METRICS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        for class_name in label_encoder.classes_:\n",
    "            metrics = class_report[class_name]\n",
    "            f.write(f\"\\n{class_name.upper()}:\\n\")\n",
    "            f.write(f\"  Precision: {metrics['precision']:.3f}\\n\")\n",
    "            f.write(f\"  Recall: {metrics['recall']:.3f}\\n\")\n",
    "            f.write(f\"  F1-Score: {metrics['f1-score']:.3f}\\n\")\n",
    "            f.write(f\"  Support: {int(metrics['support'])} samples\\n\")\n",
    "        \n",
    "        # Highlight source_issues performance for thesis\n",
    "        if 'source_issues' in class_report:\n",
    "            source_metrics = class_report['source_issues']\n",
    "            f.write(f\"\\nKEY FINDING - SOURCE_ISSUES PERFORMANCE:\\n\")\n",
    "            f.write(f\"  F1-Score: {source_metrics['f1-score']:.3f}\\n\")\n",
    "            f.write(f\"  Recall: {source_metrics['recall']:.3f} ({source_metrics['recall']*100:.0f}%)\\n\")\n",
    "            f.write(f\"  Precision: {source_metrics['precision']:.3f}\\n\")\n",
    "    \n",
    "    # Export thesis summary\n",
    "    with open(f\"{results_dir}/thesis_summary.txt\", \"w\") as f:\n",
    "        f.write(\"THESIS SUMMARY - CLASS CONSOLIDATION RESULTS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(\"APPROACH:\\n\")\n",
    "        f.write(\"- Merged 'unreliable source' and 'slanted' into 'source_issues'\\n\")\n",
    "        f.write(\"- Used focal loss for class balancing\\n\")\n",
    "        f.write(\"- Applied RoBERTa-large with optimized hyperparameters\\n\\n\")\n",
    "        \n",
    "        if 'source_issues' in class_report:\n",
    "            source_metrics = class_report['source_issues']\n",
    "            f.write(\"KEY RESULTS:\\n\")\n",
    "            f.write(f\"- source_issues F1-score: {source_metrics['f1-score']:.2f}\\n\")\n",
    "            f.write(f\"- source_issues recall: {source_metrics['recall']*100:.0f}%\\n\")\n",
    "            f.write(f\"- Overall macro F1: {overall_f1:.3f}\\n\\n\")\n",
    "    \n",
    "    return class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24d457",
   "metadata": {},
   "source": [
    "## Execute Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_class_consolidation_experiment():\n",
    "    \"\"\"\n",
    "    Execute complete class consolidation experiment.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"CLASS CONSOLIDATION EXPERIMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Merging 'unreliable source' and 'slanted' into 'source_issues'\")\n",
    "    print(\"Using focal loss and class-balanced training\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        trainer, label_encoder, splits = train_consolidated_model()\n",
    "        \n",
    "        # Analyze performance\n",
    "        results = analyze_model_performance(trainer, label_encoder, splits)\n",
    "        \n",
    "        print(f\"\\nExperiment completed successfully.\")\n",
    "        print(f\"Check class_consolidation_results/analysis_results/ for detailed outputs.\")\n",
    "        \n",
    "        return trainer, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during experiment: {e}\")\n",
    "        return None, None\n",
    "\n",
    "trainer, results = run_class_consolidation_experiment()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
