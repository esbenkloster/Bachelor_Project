{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2c58b2",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Script\n",
    "\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "import joblib\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Suppress warnings and set reproducibility\n",
    "warnings.filterwarnings('ignore')\n",
    "set_seed(42)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d4ffd",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"input_file\": \"rel_typ.xlsx\",\n",
    "    \"output_dir\": \"model_training_results\",\n",
    "    \n",
    "    \"models\": [\n",
    "        \"distilbert-base-uncased\",\n",
    "        \"bert-base-uncased\",\n",
    "        \"roberta-base\",\n",
    "        \"roberta-large\",\n",
    "        \"microsoft/DialoGPT-medium\",\n",
    "        \"facebook/bart-base\",\n",
    "        \"facebook/bart-large-mnli\",\n",
    "        \"google/electra-base-discriminator\",\n",
    "        \"microsoft/deberta-v3-base\",\n",
    "        \"microsoft/deberta-v3-large\",\n",
    "        \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    ],\n",
    "    \n",
    "    \"training\": {\n",
    "        \"base_epochs\": 2,\n",
    "        \"base_max_length\": 1024,\n",
    "        \"base_batch_size\": 2,\n",
    "        \"base_learning_rate\": 5e-5,\n",
    "        \"base_weight_decay\": 0.01,\n",
    "        \"base_warmup_steps\": 50,\n",
    "        \"eval_steps\": 25,\n",
    "        \"save_steps\": 50,\n",
    "    },\n",
    "    \n",
    "    \"hyperparameter_tuning\": {\n",
    "        \"enabled\": True,\n",
    "        \"max_total_runs\": 30,\n",
    "        \"configs_per_model\": 3,\n",
    "        \"learning_rates\": [3e-5, 5e-5, 1e-4],\n",
    "        \"batch_sizes\": [1, 2],\n",
    "        \"epochs\": [2, 3],\n",
    "        \"max_lengths\": [1024, 512, 384, 256],\n",
    "        \"weight_decays\": [0.01]\n",
    "    },\n",
    "    \n",
    "    \"evaluation\": {\n",
    "        \"confidence_bins\": 5,\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf4f6e",
   "metadata": {},
   "source": [
    "## Directory Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ecf9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_structure(base_dir):\n",
    "    \"\"\"Create organized directory structure for results\"\"\"\n",
    "    dirs = [\n",
    "        f\"{base_dir}/metrics\",\n",
    "        f\"{base_dir}/predictions\", \n",
    "        f\"{base_dir}/label_encoders\",\n",
    "        f\"{base_dir}/config\"\n",
    "    ]\n",
    "    for dir_path in dirs:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    return dirs\n",
    "\n",
    "print(\"Creating directory structure...\")\n",
    "create_directory_structure(CONFIG[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029c8fc",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path):\n",
    "    \"\"\"Load and clean the dataset\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: File {file_path} not found!\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    \n",
    "    required_columns = ['tweet_text', 'note_text', 'label']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"❌ Error: Missing required columns: {missing_columns}\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Original labels:\\n{df['label'].value_counts()}\")\n",
    "    \n",
    "    # Clean text columns\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].astype(str).fillna(\"\").str.strip()\n",
    "    df[\"note_text\"] = df[\"note_text\"].astype(str).fillna(\"\").str.strip()\n",
    "    \n",
    "    # Clean and standardize labels\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip().str.lower()\n",
    "    \n",
    "    print(f\"After label cleaning:\\n{df['label'].value_counts()}\")\n",
    "    \n",
    "    # Remove irrelevant and probative labels\n",
    "    excluded_labels = [\"irrelevant\", \"probative\"]\n",
    "    initial_count = len(df)\n",
    "    df = df[~df[\"label\"].isin(excluded_labels)]\n",
    "    print(f\"Removed {initial_count - len(df)} rows with excluded labels {excluded_labels}\")\n",
    "    \n",
    "    # Remove rows with missing labels\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"❌ Error: No data remaining after cleaning!\")\n",
    "        return None\n",
    "    \n",
    "    if len(df['label'].unique()) < 2:\n",
    "        print(\"❌ Error: Need at least 2 different labels for classification!\")\n",
    "        return None\n",
    "    \n",
    "    # Create combined text\n",
    "    df[\"text\"] = df[\"tweet_text\"] + \" [SEP] \" + df[\"note_text\"]\n",
    "    \n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "    print(f\"Final label distribution:\\n{df['label'].value_counts()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_label_encoder(df):\n",
    "    \"\"\"Prepare and save label encoder\"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"label_id\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "    \n",
    "    encoder_path = f\"{CONFIG['output_dir']}/label_encoders/label_encoder.pkl\"\n",
    "    joblib.dump(label_encoder, encoder_path)\n",
    "    \n",
    "    label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "    mapping_path = f\"{CONFIG['output_dir']}/label_encoders/label_mapping.json\"\n",
    "    with open(mapping_path, 'w') as f:\n",
    "        json.dump(label_mapping, f, indent=2)\n",
    "    \n",
    "    print(f\"Label encoder saved to {encoder_path}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return df, label_encoder\n",
    "\n",
    "def split_data(df):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "    min_samples = 10\n",
    "    if len(df) < min_samples:\n",
    "        print(f\"❌ Error: Dataset too small ({len(df)} samples). Need at least {min_samples} samples.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    class_counts = df['label_id'].value_counts()\n",
    "    min_class_size = class_counts.min()\n",
    "    if min_class_size < 3:\n",
    "        print(f\"❌ Error: Some classes have too few samples (minimum: {min_class_size})\")\n",
    "        print(\"Class distribution:\", class_counts)\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        train_val_df, test_df = train_test_split(\n",
    "            df, \n",
    "            test_size=CONFIG[\"evaluation\"][\"test_size\"], \n",
    "            stratify=df[\"label_id\"], \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        val_size_adjusted = CONFIG[\"evaluation\"][\"val_size\"] / (1 - CONFIG[\"evaluation\"][\"test_size\"])\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_val_df, \n",
    "            test_size=val_size_adjusted, \n",
    "            stratify=train_val_df[\"label_id\"], \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "        \n",
    "        # Save splits\n",
    "        train_df.to_csv(f\"{CONFIG['output_dir']}/config/train_split.csv\", index=False)\n",
    "        val_df.to_csv(f\"{CONFIG['output_dir']}/config/val_split.csv\", index=False)\n",
    "        test_df.to_csv(f\"{CONFIG['output_dir']}/config/test_split.csv\", index=False)\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Error splitting data: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aebb19",
   "metadata": {},
   "source": [
    "## Hyperparameter Configuration Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d234c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperparameter_configs(model_name, num_configs=3):\n",
    "    \"\"\"Generate hyperparameter configurations for a model\"\"\"\n",
    "    hp_config = CONFIG[\"hyperparameter_tuning\"]\n",
    "    \n",
    "    # Model-specific adjustments based on known characteristics\n",
    "    if \"llama\" in model_name.lower():\n",
    "        batch_sizes = [1]\n",
    "        learning_rates = [1e-5, 3e-5, 5e-5]\n",
    "        max_lengths = [512, 384, 256]\n",
    "    elif \"deberta\" in model_name.lower():\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = [1e-5, 2e-5, 3e-5]\n",
    "        max_lengths = [1024, 512, 384, 256]\n",
    "    elif \"dialogpt\" in model_name.lower():\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = [1e-5, 2e-5, 3e-5]\n",
    "        max_lengths = [512, 384, 256]\n",
    "    elif (\"bart\" in model_name.lower() and \"mnli\" not in model_name.lower()):\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = [1e-5, 2e-5, 3e-5]\n",
    "        max_lengths = [1024, 512, 384, 256]\n",
    "    elif \"bart\" in model_name.lower() and \"mnli\" in model_name.lower():\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = [1e-5, 2e-5]\n",
    "        max_lengths = [1024, 512, 384, 256]\n",
    "    elif \"electra\" in model_name.lower():\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = [2e-5, 3e-5, 5e-5]\n",
    "        max_lengths = [1024, 512, 384, 256]\n",
    "    elif \"roberta\" in model_name.lower():\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = hp_config[\"learning_rates\"]\n",
    "        max_lengths = [1024, 512, 384, 256]\n",
    "    elif \"bert-base\" in model_name.lower():\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = hp_config[\"learning_rates\"]\n",
    "        max_lengths = [1024, 512, 384, 256]\n",
    "    elif \"distilbert\" in model_name.lower():\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = hp_config[\"learning_rates\"]\n",
    "        max_lengths = [512, 384, 256]\n",
    "    else:\n",
    "        batch_sizes = [1, 2]\n",
    "        learning_rates = hp_config[\"learning_rates\"]\n",
    "        max_lengths = [512, 384, 256]\n",
    "    \n",
    "    all_combinations = list(itertools.product(\n",
    "        learning_rates,\n",
    "        batch_sizes,\n",
    "        hp_config[\"epochs\"],\n",
    "        max_lengths,\n",
    "        hp_config[\"weight_decays\"]\n",
    "    ))\n",
    "    \n",
    "    if len(all_combinations) > num_configs:\n",
    "        selected_combinations = random.sample(all_combinations, num_configs)\n",
    "    else:\n",
    "        selected_combinations = all_combinations\n",
    "    \n",
    "    configs = []\n",
    "    for i, (lr, bs, epochs, max_len, wd) in enumerate(selected_combinations):\n",
    "        config = {\n",
    "            \"config_id\": i + 1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": bs,\n",
    "            \"epochs\": epochs,\n",
    "            \"max_length\": max_len,\n",
    "            \"weight_decay\": wd,\n",
    "            \"gradient_accumulation_steps\": max(1, 4 // bs)\n",
    "        }\n",
    "        configs.append(config)\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb461ba",
   "metadata": {},
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0582775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer, max_length):\n",
    "    \"\"\"Tokenize the data\"\"\"\n",
    "    dataset = Dataset.from_pandas(df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"}))\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=50)\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    \"\"\"Compute class weights for imbalanced data\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted'),\n",
    "        'precision_macro': precision_score(labels, predictions, average='macro'),\n",
    "        'recall_macro': recall_score(labels, predictions, average='macro'),\n",
    "    }\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        else:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a7b7d",
   "metadata": {},
   "source": [
    "## Model Training with Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0958035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name, train_df, val_df, test_df, class_weights, label_encoder, hp_config=None):\n",
    "    \"\"\"Train and evaluate a single model with specific hyperparameters\"\"\"\n",
    "    model_short_name = model_name.split(\"/\")[-1]\n",
    "    \n",
    "    if hp_config is None:\n",
    "        hp_config = {\n",
    "            \"config_id\": 0,\n",
    "            \"learning_rate\": CONFIG[\"training\"][\"base_learning_rate\"],\n",
    "            \"batch_size\": CONFIG[\"training\"][\"base_batch_size\"],\n",
    "            \"epochs\": CONFIG[\"training\"][\"base_epochs\"],\n",
    "            \"max_length\": CONFIG[\"training\"][\"base_max_length\"],\n",
    "            \"weight_decay\": CONFIG[\"training\"][\"base_weight_decay\"],\n",
    "            \"gradient_accumulation_steps\": 1\n",
    "        }\n",
    "    \n",
    "    config_id = hp_config[\"config_id\"]\n",
    "    run_name = f\"{model_short_name}_config_{config_id}\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🚀 Training: {model_name}\")\n",
    "    print(f\"📋 Config {config_id}: LR={hp_config['learning_rate']}, BS={hp_config['batch_size']}, \"\n",
    "          f\"Epochs={hp_config['epochs']}, MaxLen={hp_config['max_length']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Clear memory before each run\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"Loading tokenizer and model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        \n",
    "        # Handle missing pad token\n",
    "        if tokenizer.pad_token is None:\n",
    "            if tokenizer.eos_token:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            else:\n",
    "                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        # Load model\n",
    "        model_kwargs = {\n",
    "            \"num_labels\": NUM_LABELS,\n",
    "            \"low_cpu_mem_usage\": False,\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model_kwargs[\"torch_dtype\"] = torch.float32\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model_kwargs[\"device_map\"] = None\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        # BART-specific model configuration\n",
    "        if \"bart\" in model_name.lower():\n",
    "            if hasattr(model.config, 'forced_bos_token_id'):\n",
    "                model.config.forced_bos_token_id = None\n",
    "                print(\"🔧 Disabled forced_bos_token_id for BART classification\")\n",
    "            \n",
    "            if hasattr(model.config, 'decoder_start_token_id') and model.config.decoder_start_token_id is None:\n",
    "                model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "                print(f\"🔧 Set decoder_start_token_id to {tokenizer.pad_token_id}\")\n",
    "        \n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f\"Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        print(\"Tokenizing datasets...\")\n",
    "        train_dataset = tokenize_data(train_df, tokenizer, hp_config[\"max_length\"])\n",
    "        val_dataset = tokenize_data(val_df, tokenizer, hp_config[\"max_length\"])\n",
    "        test_dataset = tokenize_data(test_df, tokenizer, hp_config[\"max_length\"])\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args_dict = {\n",
    "            \"output_dir\": f\"./temp_training_{run_name}\",\n",
    "            \"eval_strategy\": \"steps\",\n",
    "            \"eval_steps\": CONFIG[\"training\"][\"eval_steps\"],\n",
    "            \"save_steps\": CONFIG[\"training\"][\"save_steps\"],\n",
    "            \"num_train_epochs\": hp_config[\"epochs\"],\n",
    "            \"per_device_train_batch_size\": hp_config[\"batch_size\"],\n",
    "            \"per_device_eval_batch_size\": hp_config[\"batch_size\"],\n",
    "            \"gradient_accumulation_steps\": hp_config[\"gradient_accumulation_steps\"],\n",
    "            \"learning_rate\": hp_config[\"learning_rate\"],\n",
    "            \"weight_decay\": hp_config[\"weight_decay\"],\n",
    "            \"warmup_steps\": CONFIG[\"training\"][\"base_warmup_steps\"],\n",
    "            \"logging_steps\": 10,\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"load_best_model_at_end\": True,\n",
    "            \"metric_for_best_model\": \"eval_f1_macro\",\n",
    "            \"greater_is_better\": True,\n",
    "            \"report_to\": \"none\",\n",
    "            \"dataloader_num_workers\": 0,\n",
    "            \"remove_unused_columns\": True,\n",
    "            \"dataloader_pin_memory\": False,\n",
    "            \"max_grad_norm\": 1.0,\n",
    "            \"save_total_limit\": 1,\n",
    "            \"prediction_loss_only\": False,\n",
    "            \"ddp_find_unused_parameters\": False,\n",
    "            \"optim\": \"adamw_torch\",\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available() and device == \"cuda\":\n",
    "            training_args_dict[\"fp16\"] = True\n",
    "            training_args_dict[\"fp16_opt_level\"] = \"O1\"\n",
    "            print(\"🚀 Using FP16 for GPU training\")\n",
    "        else:\n",
    "            training_args_dict[\"fp16\"] = False\n",
    "            training_args_dict[\"bf16\"] = False\n",
    "            print(\"🔧 Using FP32 for CPU training\")\n",
    "        \n",
    "        training_args = TrainingArguments(**training_args_dict)\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = WeightedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            class_weights=class_weights\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"Starting training...\")\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"Evaluating on test set...\")\n",
    "        test_results = trainer.predict(test_dataset)\n",
    "        \n",
    "        # Extract predictions and probabilities\n",
    "        logits = test_results.predictions\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "        predicted_labels = np.argmax(probabilities, axis=1)\n",
    "        true_labels = test_results.label_ids\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'model_name': model_name,\n",
    "            'run_name': run_name,\n",
    "            'config_id': config_id,\n",
    "            'hyperparameters': hp_config,\n",
    "            'accuracy': accuracy_score(true_labels, predicted_labels),\n",
    "            'f1_macro': f1_score(true_labels, predicted_labels, average='macro'),\n",
    "            'f1_weighted': f1_score(true_labels, predicted_labels, average='weighted'),\n",
    "            'precision_macro': precision_score(true_labels, predicted_labels, average='macro'),\n",
    "            'recall_macro': recall_score(true_labels, predicted_labels, average='macro'),\n",
    "            'train_time': train_result.metrics.get('train_runtime', 0),\n",
    "            'train_loss': train_result.metrics.get('train_loss', 0),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        per_class_f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "        for i, f1 in enumerate(per_class_f1):\n",
    "            class_name = label_encoder.inverse_transform([i])[0]\n",
    "            metrics[f'f1_{class_name}'] = f1\n",
    "        \n",
    "        print(f\"✅ Results - Accuracy: {metrics['accuracy']:.4f}, F1-Macro: {metrics['f1_macro']:.4f}\")\n",
    "        \n",
    "        # Create detailed results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'text': test_df['text'].values,\n",
    "            'true_label': label_encoder.inverse_transform(true_labels),\n",
    "            'predicted_label': label_encoder.inverse_transform(predicted_labels),\n",
    "            'confidence': confidence_scores,\n",
    "            'correct': true_labels == predicted_labels\n",
    "        })\n",
    "        \n",
    "        # Add probability columns for each class\n",
    "        for i, class_name in enumerate(label_encoder.classes_):\n",
    "            results_df[f'prob_{class_name}'] = probabilities[:, i]\n",
    "        \n",
    "        # Save predictions and metrics\n",
    "        predictions_path = f\"{CONFIG['output_dir']}/predictions/{run_name}_predictions.csv\"\n",
    "        results_df.to_csv(predictions_path, index=False)\n",
    "        \n",
    "        metrics_path = f\"{CONFIG['output_dir']}/metrics/{run_name}_metrics.json\"\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2, default=str)\n",
    "        \n",
    "        # Clean up temporary training directory\n",
    "        import shutil\n",
    "        temp_dir = f\"./temp_training_{run_name}\"\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        \n",
    "        return metrics, results_df, run_name, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error training {model_name} (config {config_id}): {str(e)}\"\n",
    "        print(error_msg)\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        \n",
    "        error_metrics = {\n",
    "            'model_name': model_name,\n",
    "            'run_name': run_name,\n",
    "            'config_id': config_id,\n",
    "            'hyperparameters': hp_config,\n",
    "            'error': str(e),\n",
    "            'error_type': type(e).__name__,\n",
    "            'accuracy': None,\n",
    "            'f1_macro': None,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        error_path = f\"{CONFIG['output_dir']}/metrics/{run_name}_error.json\"\n",
    "        with open(error_path, 'w') as f:\n",
    "            json.dump(error_metrics, f, indent=2, default=str)\n",
    "        \n",
    "        return error_metrics, None, run_name, False\n",
    "        \n",
    "    finally:\n",
    "        # Aggressive cleanup\n",
    "        try:\n",
    "            del model, trainer, tokenizer\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def train_and_evaluate_model_with_fallback(model_name, train_df, val_df, test_df, class_weights, label_encoder, hp_config=None):\n",
    "    \"\"\"Train model with adaptive sequence length fallback\"\"\"\n",
    "    \n",
    "    fallback_lengths = [1024, 512, 384, 256, 128]\n",
    "    \n",
    "    original_max_length = hp_config[\"max_length\"] if hp_config else CONFIG[\"training\"][\"base_max_length\"]\n",
    "    \n",
    "    if original_max_length in fallback_lengths:\n",
    "        start_idx = fallback_lengths.index(original_max_length)\n",
    "        lengths_to_try = fallback_lengths[start_idx:]\n",
    "    else:\n",
    "        lengths_to_try = [original_max_length] + fallback_lengths\n",
    "    \n",
    "    last_error = None\n",
    "    \n",
    "    for attempt, max_length in enumerate(lengths_to_try):\n",
    "        print(f\"\\n🔄 Attempt {attempt + 1}: Trying max_length={max_length}\")\n",
    "        \n",
    "        current_config = hp_config.copy() if hp_config else {\n",
    "            \"config_id\": 0,\n",
    "            \"learning_rate\": CONFIG[\"training\"][\"base_learning_rate\"],\n",
    "            \"batch_size\": CONFIG[\"training\"][\"base_batch_size\"],\n",
    "            \"epochs\": CONFIG[\"training\"][\"base_epochs\"],\n",
    "            \"weight_decay\": CONFIG[\"training\"][\"base_weight_decay\"],\n",
    "            \"gradient_accumulation_steps\": 1\n",
    "        }\n",
    "        current_config[\"max_length\"] = max_length\n",
    "        \n",
    "        try:\n",
    "            metrics, results_df, run_name, success = train_and_evaluate_model(\n",
    "                model_name, train_df, val_df, test_df, class_weights, label_encoder, current_config\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"✅ Success with max_length={max_length}\")\n",
    "                return metrics, results_df, run_name, success\n",
    "            else:\n",
    "                last_error = metrics.get('error', 'Unknown error')\n",
    "                print(f\"❌ Failed with max_length={max_length}: {last_error}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "            print(f\"❌ Exception with max_length={max_length}: {last_error}\")\n",
    "            \n",
    "            if any(keyword in str(e).lower() for keyword in ['memory', 'cuda', 'out of memory', 'oom']):\n",
    "                print(f\"🔄 Memory error detected, trying smaller sequence length...\")\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    model_short_name = model_name.split(\"/\")[-1]\n",
    "    config_id = hp_config[\"config_id\"] if hp_config else 0\n",
    "    run_name = f\"{model_short_name}_config_{config_id}\"\n",
    "    \n",
    "    error_metrics = {\n",
    "        'model_name': model_name,\n",
    "        'run_name': run_name,\n",
    "        'config_id': config_id,\n",
    "        'hyperparameters': hp_config,\n",
    "        'error': f\"All fallback attempts failed. Last error: {last_error}\",\n",
    "        'error_type': 'FallbackExhausted',\n",
    "        'accuracy': None,\n",
    "        'f1_macro': None,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    return error_metrics, None, run_name, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1736f3a",
   "metadata": {},
   "source": [
    "## Main Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae333f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"🎯 STARTING MULTI-MODEL TRAINING WITH HYPERPARAMETER TUNING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n📊 Loading and preparing data...\")\n",
    "    df = load_and_clean_data(CONFIG[\"input_file\"])\n",
    "    if df is None:\n",
    "        print(\"❌ Failed to load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    df, label_encoder = prepare_label_encoder(df)\n",
    "    train_df, val_df, test_df = split_data(df)\n",
    "    \n",
    "    if train_df is None or val_df is None or test_df is None:\n",
    "        print(\"❌ Failed to split data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    global NUM_LABELS\n",
    "    NUM_LABELS = len(label_encoder.classes_)\n",
    "    print(f\"Number of classes: {NUM_LABELS}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weights(train_df['label_id'].values)\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    # Store results\n",
    "    all_metrics = []\n",
    "    successful_runs = 0\n",
    "    failed_runs = 0\n",
    "    total_runs = 0\n",
    "    \n",
    "    print(f\"\\n📊 Hyperparameter tuning: {CONFIG['hyperparameter_tuning']['configs_per_model']} configs per model\")\n",
    "    print(f\"🎯 Maximum total runs: {CONFIG['hyperparameter_tuning']['max_total_runs']}\")\n",
    "    \n",
    "    # Train each model\n",
    "    for model_name in CONFIG[\"models\"]:\n",
    "        if total_runs >= CONFIG['hyperparameter_tuning']['max_total_runs']:\n",
    "            print(f\"\\n⚠️ Reached maximum total runs, stopping...\")\n",
    "            break\n",
    "        \n",
    "        model_short_name = model_name.split(\"/\")[-1]\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"🔬 HYPERPARAMETER TUNING FOR: {model_name}\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        # Generate hyperparameter configurations\n",
    "        hp_configs = generate_hyperparameter_configs(model_name, CONFIG[\"hyperparameter_tuning\"][\"configs_per_model\"])\n",
    "        print(f\"Generated {len(hp_configs)} hyperparameter configurations\")\n",
    "        \n",
    "        for hp_config in hp_configs:\n",
    "            if total_runs >= CONFIG['hyperparameter_tuning']['max_total_runs']:\n",
    "                break\n",
    "            \n",
    "            total_runs += 1\n",
    "            \n",
    "            metrics, results_df, run_name, success = train_and_evaluate_model_with_fallback(\n",
    "                model_name, train_df, val_df, test_df, class_weights, label_encoder, hp_config\n",
    "            )\n",
    "            \n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            if success:\n",
    "                successful_runs += 1\n",
    "                print(f\"✅ Successful run: {run_name}\")\n",
    "            else:\n",
    "                failed_runs += 1\n",
    "                print(f\"❌ Failed run: {run_name}\")\n",
    "        \n",
    "        # Print model summary\n",
    "        successful_model_runs = [m for m in all_metrics if m['model_name'] == model_name and m.get('accuracy') is not None]\n",
    "        if successful_model_runs:\n",
    "            best_run = max(successful_model_runs, key=lambda x: x['f1_macro'])\n",
    "            print(f\"\\n🏆 Best configuration for {model_short_name}:\")\n",
    "            print(f\"   Run: {best_run['run_name']}\")\n",
    "            print(f\"   F1-Macro: {best_run['f1_macro']:.4f}\")\n",
    "            print(f\"   Accuracy: {best_run['accuracy']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"📊 TRAINING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total runs attempted: {total_runs}\")\n",
    "    print(f\"Successful runs: {successful_runs}\")\n",
    "    print(f\"Failed runs: {failed_runs}\")\n",
    "    print(f\"Success rate: {successful_runs/total_runs*100:.1f}%\" if total_runs > 0 else \"No runs completed\")\n",
    "    \n",
    "    if successful_runs > 0:\n",
    "        # Save results summary\n",
    "        metrics_summary_path = f\"{CONFIG['output_dir']}/metrics/all_models_summary.csv\"\n",
    "        df_all_metrics = pd.DataFrame(all_metrics)\n",
    "        df_all_metrics.to_csv(metrics_summary_path, index=False)\n",
    "        \n",
    "        # Find best model\n",
    "        successful_models = df_all_metrics[df_all_metrics['accuracy'].notna()]\n",
    "        if len(successful_models) > 0:\n",
    "            best_model_idx = successful_models['f1_macro'].idxmax()\n",
    "            best_model = successful_models.loc[best_model_idx]\n",
    "            print(f\"\\n🏆 BEST MODEL: {best_model['model_name']}\")\n",
    "            print(f\"   Run: {best_model['run_name']}\")\n",
    "            print(f\"   Accuracy: {best_model['accuracy']:.4f}\")\n",
    "            print(f\"   F1-Macro: {best_model['f1_macro']:.4f}\")\n",
    "            \n",
    "            best_model_info = {\n",
    "                'model_name': best_model['model_name'],\n",
    "                'run_name': best_model['run_name'],\n",
    "                'metrics': best_model.to_dict()\n",
    "            }\n",
    "            \n",
    "            with open(f\"{CONFIG['output_dir']}/config/best_model.json\", 'w') as f:\n",
    "                json.dump(best_model_info, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"✅ TRAINING COMPLETE!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"📁 All results saved to: {CONFIG['output_dir']}\")\n",
    "        print(f\"🏆 Best model info saved to: {CONFIG['output_dir']}/config/best_model.json\")\n",
    "        print(f\"📊 Summary metrics: {metrics_summary_path}\")\n",
    "    else:\n",
    "        print(\"\\n❌ No successful model training runs. Please check your data and configuration.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
