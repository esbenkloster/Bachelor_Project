{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d338c4",
   "metadata": {},
   "source": [
    "# Metadata Enhanced Classification Experiment\n",
    "\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "import joblib\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "set_seed(42)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"GPU Available: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055068db",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    \"enhanced_data_path\": \"enhanced_dataset/training_ready_dataset.csv\",\n",
    "    \"splits_data_dir\": \"model_training_results/config\",\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"metadata_classification_results\",\n",
    "    \"experiment_name\": \"oversampling_plus_metadata\",\n",
    "    \n",
    "    # Model configuration\n",
    "    \"target_model\": \"roberta-large\",\n",
    "    \"problem_class\": \"unreliable source\",\n",
    "    \n",
    "    # Best strategy from previous experiments\n",
    "    \"training_config\": {\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"batch_size\": 2,\n",
    "        \"max_length\": 512,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"class_weight_multiplier\": 3.0,\n",
    "        \"oversample_ratio\": 3,\n",
    "        \"use_focal_loss\": False,\n",
    "        \"warmup_steps\": 20,\n",
    "    },\n",
    "    \n",
    "    # Metadata configuration\n",
    "    \"metadata_config\": {\n",
    "        \"use_metadata\": True,\n",
    "        \"method\": \"concatenation\",\n",
    "        \"key_features\": [\n",
    "            \"misleadingunverifiedclaimasfact\",\n",
    "            \"misleadingmissingimportantcontext\",\n",
    "            \"misleadingfactualerror\",\n",
    "            \"misleadingoutdatedinformation\",\n",
    "            \"misleadingmanipulatedmedia\",\n",
    "            \"misleadingsatire\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Baseline results for comparison\n",
    "    \"baseline_results\": {\n",
    "        \"unreliable_source_f1\": 0.4000,\n",
    "        \"false_f1\": 0.3500,  # Approximate baseline\n",
    "        \"repurposed_f1\": 0.3000,  # Approximate baseline\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for subdir in ['predictions', 'reports']:\n",
    "    os.makedirs(f\"{CONFIG['output_dir']}/{subdir}\", exist_ok=True)\n",
    "\n",
    "print(\"Metadata Enhanced Classification Experiment\")\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")\n",
    "print(f\"Target problem: {CONFIG['problem_class']} classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc364b2",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_enhanced_dataset():\n",
    "    \"\"\"Load enhanced dataset with metadata and existing splits\"\"\"\n",
    "    \n",
    "    print(\"Loading enhanced dataset with metadata...\")\n",
    "    \n",
    "    # Load enhanced dataset\n",
    "    try:\n",
    "        if CONFIG[\"enhanced_data_path\"].endswith('.xlsx'):\n",
    "            enhanced_df = pd.read_excel(CONFIG[\"enhanced_data_path\"])\n",
    "        else:\n",
    "            enhanced_df = pd.read_csv(CONFIG[\"enhanced_data_path\"])\n",
    "        \n",
    "        print(f\"Enhanced dataset loaded: {enhanced_df.shape}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Enhanced dataset not found at {CONFIG['enhanced_data_path']}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Load existing splits\n",
    "    try:\n",
    "        train_df = pd.read_csv(f\"{CONFIG['splits_data_dir']}/train_split.csv\")\n",
    "        val_df = pd.read_csv(f\"{CONFIG['splits_data_dir']}/val_split.csv\")\n",
    "        test_df = pd.read_csv(f\"{CONFIG['splits_data_dir']}/test_split.csv\")\n",
    "        \n",
    "        print(f\"Splits loaded - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find splits in {CONFIG['splits_data_dir']}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    return enhanced_df, train_df, val_df, test_df\n",
    "\n",
    "def analyze_metadata_coverage(enhanced_df, metadata_features):\n",
    "    \"\"\"Analyze metadata feature coverage and distribution\"\"\"\n",
    "    \n",
    "    print(f\"\\nAnalyzing metadata coverage for {len(metadata_features)} features...\")\n",
    "    \n",
    "    available_features = []\n",
    "    for feature in metadata_features:\n",
    "        # Check both with and without 'meta_' prefix\n",
    "        feature_variants = [feature, f'meta_{feature}']\n",
    "        found = False\n",
    "        \n",
    "        for variant in feature_variants:\n",
    "            if variant in enhanced_df.columns:\n",
    "                available_features.append(variant)\n",
    "                coverage = enhanced_df[variant].notna().mean()\n",
    "                unique_vals = enhanced_df[variant].nunique()\n",
    "                print(f\"   {variant}: {coverage:.1%} coverage, {unique_vals} unique values\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"   {feature}: Not found in dataset\")\n",
    "    \n",
    "    if not available_features:\n",
    "        print(\"WARNING: No metadata features found! Running without metadata enhancement.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nUsing {len(available_features)} metadata features for enhancement\")\n",
    "    return available_features\n",
    "\n",
    "def merge_splits_with_metadata(train_df, val_df, test_df, enhanced_df, metadata_features):\n",
    "    \"\"\"Merge original splits with metadata features\"\"\"\n",
    "    \n",
    "    print(f\"\\nMerging splits with metadata features...\")\n",
    "    \n",
    "    def safe_merge(split_df, split_name):\n",
    "        \"\"\"Safely merge split with enhanced dataset\"\"\"\n",
    "        \n",
    "        # Try to merge on text column\n",
    "        if 'text' in split_df.columns and 'text' in enhanced_df.columns:\n",
    "            print(f\"   Merging {split_name} on text column\")\n",
    "            \n",
    "            merged_df = split_df.merge(\n",
    "                enhanced_df[['text'] + metadata_features], \n",
    "                on='text', \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            coverage = merged_df[metadata_features].notna().all(axis=1).mean()\n",
    "            print(f\"   {split_name} metadata coverage: {coverage:.1%}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   Could not merge {split_name}, using original split with default metadata\")\n",
    "            merged_df = split_df.copy()\n",
    "            for feature in metadata_features:\n",
    "                merged_df[feature] = 0\n",
    "        \n",
    "        return merged_df\n",
    "    \n",
    "    enhanced_train = safe_merge(train_df, \"Training\")\n",
    "    enhanced_val = safe_merge(val_df, \"Validation\")\n",
    "    enhanced_test = safe_merge(test_df, \"Test\")\n",
    "    \n",
    "    return enhanced_train, enhanced_val, enhanced_test\n",
    "\n",
    "def create_enhanced_text_with_metadata(df, metadata_features):\n",
    "    \"\"\"Create enhanced text by concatenating metadata signals\"\"\"\n",
    "    \n",
    "    print(f\"Creating enhanced text with metadata signals...\")\n",
    "    \n",
    "    enhanced_texts = []\n",
    "    signal_stats = Counter()\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        base_text = df.iloc[i]['text']\n",
    "        \n",
    "        # Extract active metadata signals\n",
    "        active_signals = []\n",
    "        for feature in metadata_features:\n",
    "            value = df.iloc[i].get(feature, 0)\n",
    "            if pd.notna(value) and value == 1:\n",
    "                signal = feature.replace('misleading', 'MISLEADING_').replace('meta_', '')\n",
    "                active_signals.append(signal.upper())\n",
    "                signal_stats[signal] += 1\n",
    "        \n",
    "        # Create enhanced text with metadata\n",
    "        if active_signals:\n",
    "            enhanced_text = f\"{base_text} [METADATA] {' '.join(active_signals)}\"\n",
    "        else:\n",
    "            enhanced_text = f\"{base_text} [METADATA] NO_SIGNALS\"\n",
    "        \n",
    "        enhanced_texts.append(enhanced_text)\n",
    "    \n",
    "    print(f\"   Enhanced {len(enhanced_texts)} text samples\")\n",
    "    print(f\"   Most common signals: {dict(signal_stats.most_common(3))}\")\n",
    "    \n",
    "    return enhanced_texts\n",
    "\n",
    "def oversample_minority_class(train_df, target_class, ratio=3):\n",
    "    \"\"\"Apply oversampling strategy\"\"\"\n",
    "    \n",
    "    print(f\"\\nApplying oversampling strategy...\")\n",
    "    print(f\"   Target class: '{target_class}'\")\n",
    "    print(f\"   Oversampling ratio: {ratio}x\")\n",
    "    \n",
    "    # Get samples of target class\n",
    "    target_samples = train_df[train_df['label'] == target_class]\n",
    "    other_samples = train_df[train_df['label'] != target_class]\n",
    "    \n",
    "    print(f\"   Original '{target_class}' samples: {len(target_samples)}\")\n",
    "    print(f\"   Other samples: {len(other_samples)}\")\n",
    "    \n",
    "    if len(target_samples) == 0:\n",
    "        print(f\"   WARNING: No samples found for '{target_class}'!\")\n",
    "        return train_df\n",
    "    \n",
    "    # Oversample by repeating samples with slight variations\n",
    "    oversampled_targets = []\n",
    "    for rep in range(ratio):\n",
    "        target_copy = target_samples.copy()\n",
    "        if rep > 0:\n",
    "            target_copy = target_copy.copy()\n",
    "            target_copy['text'] = target_copy['text'] + f\" [AUG_{rep}]\"\n",
    "        oversampled_targets.append(target_copy)\n",
    "    \n",
    "    oversampled_target = pd.concat(oversampled_targets, ignore_index=True)\n",
    "    \n",
    "    # Combine with other samples\n",
    "    balanced_df = pd.concat([other_samples, oversampled_target], ignore_index=True)\n",
    "    \n",
    "    print(f\"   After oversampling '{target_class}' samples: {len(oversampled_target)}\")\n",
    "    print(f\"   Total training samples: {len(balanced_df)}\")\n",
    "    print(f\"   New class distribution:\")\n",
    "    print(balanced_df['label'].value_counts())\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "# Load and prepare data\n",
    "enhanced_df, train_df, val_df, test_df = load_enhanced_dataset()\n",
    "\n",
    "if enhanced_df is not None:\n",
    "    print(f\"Ready to enhance {len(enhanced_df)} samples\")\n",
    "    \n",
    "    # Analyze and select metadata features\n",
    "    metadata_features = analyze_metadata_coverage(\n",
    "        enhanced_df, \n",
    "        CONFIG[\"metadata_config\"][\"key_features\"]\n",
    "    )\n",
    "    \n",
    "    # Merge splits with metadata\n",
    "    enhanced_train, enhanced_val, enhanced_test = merge_splits_with_metadata(\n",
    "        train_df, val_df, test_df, enhanced_df, metadata_features\n",
    "    )\n",
    "    \n",
    "    # Create enhanced text features\n",
    "    if CONFIG[\"metadata_config\"][\"use_metadata\"] and metadata_features:\n",
    "        print(f\"\\nEnhancing text with metadata...\")\n",
    "        enhanced_train['enhanced_text'] = create_enhanced_text_with_metadata(enhanced_train, metadata_features)\n",
    "        enhanced_val['enhanced_text'] = create_enhanced_text_with_metadata(enhanced_val, metadata_features)\n",
    "        enhanced_test['enhanced_text'] = create_enhanced_text_with_metadata(enhanced_test, metadata_features)\n",
    "        \n",
    "        # Use enhanced text as the main text\n",
    "        enhanced_train['text'] = enhanced_train['enhanced_text']\n",
    "        enhanced_val['text'] = enhanced_val['enhanced_text']\n",
    "        enhanced_test['text'] = enhanced_test['enhanced_text']\n",
    "        \n",
    "        print(f\"   Text enhanced with {len(metadata_features)} metadata features\")\n",
    "    else:\n",
    "        print(f\"   Using original text without metadata enhancement\")\n",
    "    \n",
    "    # Apply oversampling strategy\n",
    "    enhanced_train = oversample_minority_class(\n",
    "        enhanced_train,\n",
    "        CONFIG[\"problem_class\"],\n",
    "        CONFIG[\"training_config\"][\"oversample_ratio\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Data preparation complete\")\n",
    "else:\n",
    "    print(\"Data preparation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b482c",
   "metadata": {},
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_df, target_class, multiplier=3.0):\n",
    "    \"\"\"Calculate class weights with emphasis on target class\"\"\"\n",
    "    \n",
    "    print(f\"\\nCalculating class weights...\")\n",
    "    print(f\"   Target class: '{target_class}'\")\n",
    "    print(f\"   Weight multiplier: {multiplier}x\")\n",
    "    \n",
    "    label_counts = train_df['label'].value_counts()\n",
    "    print(f\"   Class distribution: {dict(label_counts)}\")\n",
    "    \n",
    "    # Calculate balanced weights\n",
    "    unique_labels = train_df['label'].unique()\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(unique_labels)\n",
    "    \n",
    "    y = label_encoder.transform(train_df['label'])\n",
    "    base_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    \n",
    "    # Apply multiplier to target class\n",
    "    class_weights = {}\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        if label == target_class:\n",
    "            class_weights[i] = base_weights[i] * multiplier\n",
    "        else:\n",
    "            class_weights[i] = base_weights[i]\n",
    "    \n",
    "    print(f\"   Final class weights:\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        print(f\"     {label}: {class_weights[i]:.3f}\")\n",
    "    \n",
    "    # Convert to tensor\n",
    "    weight_tensor = torch.tensor([class_weights[i] for i in range(len(label_encoder.classes_))], dtype=torch.float)\n",
    "    \n",
    "    return weight_tensor, class_weights\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with class weights\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        else:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_metrics = {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted'),\n",
    "        'precision_macro': precision_score(labels, predictions, average='macro'),\n",
    "        'recall_macro': recall_score(labels, predictions, average='macro'),\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_f1 = f1_score(labels, predictions, average=None)\n",
    "    per_class_precision = precision_score(labels, predictions, average=None)\n",
    "    per_class_recall = recall_score(labels, predictions, average=None)\n",
    "    \n",
    "    # Add per-class metrics\n",
    "    for i in range(len(per_class_f1)):\n",
    "        overall_metrics[f'f1_class_{i}'] = per_class_f1[i]\n",
    "        overall_metrics[f'precision_class_{i}'] = per_class_precision[i]\n",
    "        overall_metrics[f'recall_class_{i}'] = per_class_recall[i]\n",
    "    \n",
    "    return overall_metrics\n",
    "\n",
    "def tokenize_data(df, tokenizer, max_length):\n",
    "    \"\"\"Tokenize the data for training\"\"\"\n",
    "    \n",
    "    print(f\"🔧 Tokenizing {len(df)} samples...\")\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"}))\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=50)\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    print(f\"   ✅ Tokenization complete\")\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864dcb8f",
   "metadata": {},
   "source": [
    "## Execute Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c802bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_metadata_enhanced_model():\n",
    "    \"\"\"Complete training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    if enhanced_df is None:\n",
    "        print(\"❌ Cannot proceed - data not loaded\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"🚀 METADATA ENHANCED CLASSIFICATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    unique_labels = enhanced_train['label'].unique()\n",
    "    label_encoder.fit(unique_labels)\n",
    "    \n",
    "    print(f\"\\n🏷️ Label mapping:\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        print(f\"   {i}: {label}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    enhanced_train['label_id'] = label_encoder.transform(enhanced_train['label'])\n",
    "    enhanced_val['label_id'] = label_encoder.transform(enhanced_val['label'])\n",
    "    enhanced_test['label_id'] = label_encoder.transform(enhanced_test['label'])\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights, class_weights_dict = calculate_class_weights(\n",
    "        enhanced_train,\n",
    "        CONFIG[\"problem_class\"],\n",
    "        CONFIG[\"training_config\"][\"class_weight_multiplier\"]\n",
    "    )\n",
    "    \n",
    "    # Prepare model and tokenizer\n",
    "    print(f\"\\n🤖 Loading {CONFIG['target_model']}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"target_model\"], use_fast=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CONFIG[\"target_model\"],\n",
    "        num_labels=num_labels,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"   ✅ Model loaded on {device}\")\n",
    "    print(f\"   📊 Parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(f\"\\n🔧 Preparing datasets...\")\n",
    "    train_dataset = tokenize_data(enhanced_train, tokenizer, CONFIG[\"training_config\"][\"max_length\"])\n",
    "    val_dataset = tokenize_data(enhanced_val, tokenizer, CONFIG[\"training_config\"][\"max_length\"])\n",
    "    test_dataset = tokenize_data(enhanced_test, tokenizer, CONFIG[\"training_config\"][\"max_length\"])\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{CONFIG['output_dir']}/temp\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        # save_steps=40, \n",
    "        num_train_epochs=CONFIG[\"training_config\"][\"epochs\"],\n",
    "        per_device_train_batch_size=CONFIG[\"training_config\"][\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"training_config\"][\"batch_size\"],\n",
    "        learning_rate=CONFIG[\"training_config\"][\"learning_rate\"],\n",
    "        weight_decay=CONFIG[\"training_config\"][\"weight_decay\"],\n",
    "        warmup_steps=CONFIG[\"training_config\"][\"warmup_steps\"],\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\", \n",
    "        load_best_model_at_end=False, \n",
    "        metric_for_best_model=\"eval_f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        max_grad_norm=1.0,\n",
    "        save_total_limit=0,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        optim=\"adamw_torch\",\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🚀 Starting training...\")\n",
    "    print(f\"   Strategy: {CONFIG['experiment_name']}\")\n",
    "    print(f\"   Enhanced with metadata: {CONFIG['metadata_config']['use_metadata']}\")\n",
    "    print(f\"   Metadata features: {len(metadata_features)}\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(f\"✅ Training completed!\")\n",
    "    print(f\"   Training time: {train_result.metrics.get('train_runtime', 0):.0f} seconds\")\n",
    "    print(f\"   Final training loss: {train_result.metrics.get('train_loss', 0):.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n📊 EVALUATING MODEL ON TEST SET\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    test_results = trainer.predict(test_dataset)\n",
    "    \n",
    "    # Extract predictions and probabilities\n",
    "    logits = test_results.predictions\n",
    "    probabilities = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    predicted_labels = np.argmax(probabilities, axis=1)\n",
    "    true_labels = test_results.label_ids\n",
    "    confidence_scores = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Convert back to label names\n",
    "    true_label_names = label_encoder.inverse_transform(true_labels)\n",
    "    predicted_label_names = label_encoder.inverse_transform(predicted_labels)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1_macro = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    print(f\"📈 Overall Results:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Macro: {f1_macro:.4f}\")\n",
    "    print(f\"   F1-Weighted: {f1_weighted:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "    per_class_precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "    per_class_recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "    \n",
    "    print(f\"\\n🎯 Per-Class Results:\")\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        print(f\"   {class_name}:\")\n",
    "        print(f\"      F1: {per_class_f1[i]:.4f}\")\n",
    "        print(f\"      Precision: {per_class_precision[i]:.4f}\")\n",
    "        print(f\"      Recall: {per_class_recall[i]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'per_class_f1': per_class_f1,\n",
    "        'per_class_precision': per_class_precision,\n",
    "        'per_class_recall': per_class_recall,\n",
    "        'label_encoder': label_encoder,\n",
    "        'true_labels': true_label_names,\n",
    "        'predicted_labels': predicted_label_names,\n",
    "        'confidence_scores': confidence_scores\n",
    "    }\n",
    "\n",
    "# Execute the training and evaluation\n",
    "if enhanced_df is not None:\n",
    "    results = train_and_evaluate_metadata_enhanced_model()\n",
    "else:\n",
    "    results = None\n",
    "    print(\"❌ Cannot execute training - data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a0969",
   "metadata": {},
   "source": [
    "## Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_and_generate_report(results):\n",
    "    \"\"\"Save detailed results and generate comprehensive report\"\"\"\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"❌ No results to save\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n💾 Saving results and generating report...\")\n",
    "    \n",
    "    # Extract results\n",
    "    accuracy = results['accuracy']\n",
    "    f1_macro = results['f1_macro']\n",
    "    f1_weighted = results['f1_weighted']\n",
    "    per_class_f1 = results['per_class_f1']\n",
    "    per_class_precision = results['per_class_precision']\n",
    "    per_class_recall = results['per_class_recall']\n",
    "    label_encoder = results['label_encoder']\n",
    "    \n",
    "    # Create detailed results text\n",
    "    results_text = []\n",
    "    results_text.append(\"# Metadata Enhanced Classification Results\")\n",
    "    results_text.append(\"\")\n",
    "    \n",
    "    # Overall results\n",
    "    results_text.append(\"## Overall Results\")\n",
    "    results_text.append(f\"Accuracy: {accuracy:.4f}\")\n",
    "    results_text.append(f\"F1-Macro: {f1_macro:.4f}\")\n",
    "    results_text.append(f\"F1-Weighted: {f1_weighted:.4f}\")\n",
    "    results_text.append(\"\")\n",
    "    \n",
    "    # Per-class results\n",
    "    results_text.append(\"## Per-Class Results\")\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        results_text.append(f\"{class_name}:\")\n",
    "        results_text.append(f\"  F1: {per_class_f1[i]:.4f}\")\n",
    "        results_text.append(f\"  Precision: {per_class_precision[i]:.4f}\")\n",
    "        results_text.append(f\"  Recall: {per_class_precision[i]:.4f}\")\n",
    "        results_text.append(\"\")\n",
    "    \n",
    "    # Comparison with baseline\n",
    "    baseline = CONFIG[\"baseline_results\"]\n",
    "    results_text.append(\"## Comparison with Baseline\")\n",
    "    results_text.append(\"| Category | Baseline F1 | Enhanced F1 | Change |\")\n",
    "    results_text.append(\"|----------|-------------|-------------|--------|\")\n",
    "    \n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        baseline_key = f\"{class_name.replace(' ', '_')}_f1\"\n",
    "        baseline_f1 = baseline.get(baseline_key, 0.0)\n",
    "        enhanced_f1 = per_class_f1[i]\n",
    "        change = enhanced_f1 - baseline_f1\n",
    "        \n",
    "        results_text.append(f\"| {class_name} | {baseline_f1:.4f} | {enhanced_f1:.4f} | {change:+.4f} |\")\n",
    "    \n",
    "    results_text.append(\"\")\n",
    "    \n",
    "    # Key findings\n",
    "    results_text.append(\"## Key Findings\")\n",
    "    \n",
    "    # Check improvements/degradations\n",
    "    unreliable_source_idx = None\n",
    "    false_idx = None\n",
    "    repurposed_idx = None\n",
    "    \n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        if class_name == \"unreliable source\":\n",
    "            unreliable_source_idx = i\n",
    "        elif class_name == \"false\":\n",
    "            false_idx = i\n",
    "        elif class_name == \"repurposed\":\n",
    "            repurposed_idx = i\n",
    "    \n",
    "    if false_idx is not None:\n",
    "        false_improvement = per_class_f1[false_idx] - baseline.get(\"false_f1\", 0.0)\n",
    "        results_text.append(f\"- False content classification: F1={per_class_f1[false_idx]:.4f} (improvement: {false_improvement:+.4f})\")\n",
    "    \n",
    "    if repurposed_idx is not None:\n",
    "        repurposed_improvement = per_class_f1[repurposed_idx] - baseline.get(\"repurposed_f1\", 0.0)\n",
    "        results_text.append(f\"- Repurposed content classification: F1={per_class_f1[repurposed_idx]:.4f} (improvement: {repurposed_improvement:+.4f})\")\n",
    "    \n",
    "    if unreliable_source_idx is not None:\n",
    "        unreliable_change = per_class_f1[unreliable_source_idx] - baseline.get(\"unreliable_source_f1\", 0.0)\n",
    "        results_text.append(f\"- Unreliable source classification: F1={per_class_f1[unreliable_source_idx]:.4f} (change: {unreliable_change:+.4f})\")\n",
    "    \n",
    "    results_text.append(\"\")\n",
    "    results_text.append(\"## Configuration\")\n",
    "    results_text.append(f\"Model: {CONFIG['target_model']}\")\n",
    "    results_text.append(f\"Metadata features used: {len(metadata_features)}\")\n",
    "    results_text.append(f\"Oversampling ratio: {CONFIG['training_config']['oversample_ratio']}x\")\n",
    "    results_text.append(f\"Learning rate: {CONFIG['training_config']['learning_rate']}\")\n",
    "    results_text.append(f\"Epochs: {CONFIG['training_config']['epochs']}\")\n",
    "    results_text.append(\"\")\n",
    "    \n",
    "    # Save results text\n",
    "    results_file_path = f\"{CONFIG['output_dir']}/reports/metadata_classification_results.txt\"\n",
    "    with open(results_file_path, 'w') as f:\n",
    "        f.write('\\n'.join(results_text))\n",
    "    \n",
    "    # Save detailed predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'true_label': results['true_labels'],\n",
    "        'predicted_label': results['predicted_labels'],\n",
    "        'confidence': results['confidence_scores'],\n",
    "        'correct': results['true_labels'] == results['predicted_labels']\n",
    "    })\n",
    "    \n",
    "    predictions_path = f\"{CONFIG['output_dir']}/predictions/detailed_predictions.csv\"\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "    \n",
    "    # Save metrics as JSON\n",
    "    metrics = {\n",
    "        'experiment_name': CONFIG['experiment_name'],\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'metadata_enhanced': CONFIG['metadata_config']['use_metadata'],\n",
    "        'metadata_features_count': len(metadata_features),\n",
    "        'per_class_results': {}\n",
    "    }\n",
    "    \n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        metrics['per_class_results'][class_name] = {\n",
    "            'f1': float(per_class_f1[i]),\n",
    "            'precision': float(per_class_precision[i]),\n",
    "            'recall': float(per_class_recall[i])\n",
    "        }\n",
    "    \n",
    "    metrics_path = f\"{CONFIG['output_dir']}/reports/classification_metrics.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Results saved:\")\n",
    "    print(f\"   - Detailed results: {results_file_path}\")\n",
    "    print(f\"   - Predictions: {predictions_path}\")\n",
    "    print(f\"   - Metrics: {metrics_path}\")\n",
    "    \n",
    "    return results_file_path\n",
    "\n",
    "# Save results and generate report\n",
    "if results is not None:\n",
    "    report_path = save_results_and_generate_report(results)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✅ METADATA ENHANCED CLASSIFICATION COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"📊 Overall F1-Macro: {results['f1_macro']:.4f}\")\n",
    "    print(f\"📊 Overall Accuracy: {results['accuracy']:.4f}\")\n",
    "    \n",
    "    # Show key results for each class\n",
    "    for i, class_name in enumerate(results['label_encoder'].classes_):\n",
    "        print(f\"📊 {class_name}: F1={results['per_class_f1'][i]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📁 All results saved to: {CONFIG['output_dir']}\")\n",
    "    print(f\"📄 Detailed report: {report_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Experiment failed - no results to report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a7579",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92916d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n🧹 Memory cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
