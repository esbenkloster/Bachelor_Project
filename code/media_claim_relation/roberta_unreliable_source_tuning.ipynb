{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f043b768",
   "metadata": {},
   "source": [
    "# Specialized Fine-tuning for Unreliable Source Classification\n",
    "\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "import joblib\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "set_seed(42)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a07b13",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"input_data_dir\": \"model_training_results/config\",  # Where splits are saved\n",
    "    \"output_dir\": \"unreliable_source_tuning_results\",\n",
    "    \"target_model\": \"roberta-large\",\n",
    "    \"problem_class\": \"unreliable source\",\n",
    "    \n",
    "    # Five class imbalance strategies\n",
    "    \"training_strategies\": [\n",
    "        {\n",
    "            \"name\": \"heavy_class_weights\",\n",
    "            \"description\": \"Heavy weighting for unreliable source class\",\n",
    "            \"epochs\": 4,\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 1,\n",
    "            \"max_length\": 512,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"class_weight_multiplier\": 10.0,\n",
    "            \"use_focal_loss\": False,\n",
    "            \"oversample\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"focal_loss\",\n",
    "            \"description\": \"Focal loss to handle hard examples\",\n",
    "            \"epochs\": 4,\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 1,\n",
    "            \"max_length\": 512,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"class_weight_multiplier\": 5.0,\n",
    "            \"use_focal_loss\": True,\n",
    "            \"focal_alpha\": 0.25,\n",
    "            \"focal_gamma\": 2.0,\n",
    "            \"oversample\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"longer_training\",\n",
    "            \"description\": \"More epochs with lower learning rate\",\n",
    "            \"epochs\": 6,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"batch_size\": 1,\n",
    "            \"max_length\": 512,\n",
    "            \"weight_decay\": 0.005,\n",
    "            \"class_weight_multiplier\": 8.0,\n",
    "            \"use_focal_loss\": False,\n",
    "            \"oversample\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"oversampling\",\n",
    "            \"description\": \"Oversample minority class to balance dataset\",\n",
    "            \"epochs\": 3,\n",
    "            \"learning_rate\": 3e-5,\n",
    "            \"batch_size\": 2,\n",
    "            \"max_length\": 512,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"class_weight_multiplier\": 3.0,\n",
    "            \"use_focal_loss\": False,\n",
    "            \"oversample\": True,\n",
    "            \"oversample_ratio\": 3\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"combined_approach\",\n",
    "            \"description\": \"Oversampling + focal loss + class weights\",\n",
    "            \"epochs\": 4,\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 1,\n",
    "            \"max_length\": 384,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"class_weight_multiplier\": 6.0,\n",
    "            \"use_focal_loss\": True,\n",
    "            \"focal_alpha\": 0.25,\n",
    "            \"focal_gamma\": 1.5,\n",
    "            \"oversample\": True,\n",
    "            \"oversample_ratio\": 2\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"evaluation\": {\n",
    "        \"eval_steps\": 20,\n",
    "        \"save_steps\": 40,\n",
    "        \"logging_steps\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for subdir in ['predictions', 'plots', 'reports', 'metrics']:\n",
    "    os.makedirs(f\"{CONFIG['output_dir']}/{subdir}\", exist_ok=True)\n",
    "\n",
    "print(\"üéØ Unreliable Source Classification Improvement\")\n",
    "print(f\"Target class: {CONFIG['problem_class']}\")\n",
    "print(f\"Strategies to test: {len(CONFIG['training_strategies'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965afcfa",
   "metadata": {},
   "source": [
    "## Load Existing Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca87eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_splits(data_dir):\n",
    "    \"\"\"Load the existing train/val/test splits\"\"\"\n",
    "    print(f\"Loading existing data splits from {data_dir}...\")\n",
    "    \n",
    "    try:\n",
    "        train_df = pd.read_csv(f\"{data_dir}/train_split.csv\")\n",
    "        val_df = pd.read_csv(f\"{data_dir}/val_split.csv\")\n",
    "        test_df = pd.read_csv(f\"{data_dir}/test_split.csv\")\n",
    "        \n",
    "        print(f\"‚úÖ Loaded splits:\")\n",
    "        print(f\"   Train: {len(train_df)} samples\")\n",
    "        print(f\"   Validation: {len(val_df)} samples\") \n",
    "        print(f\"   Test: {len(test_df)} samples\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        print(f\"\\nüìä Class distribution:\")\n",
    "        print(\"Training set:\")\n",
    "        print(train_df['label'].value_counts())\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Could not find splits in {data_dir}\")\n",
    "        print(\"Run the main training script first.\")\n",
    "        return None, None, None\n",
    "\n",
    "def prepare_label_encoder(train_df):\n",
    "    \"\"\"Prepare label encoder from training data\"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    unique_labels = train_df['label'].unique()\n",
    "    label_encoder.fit(unique_labels)\n",
    "    \n",
    "    print(f\"Label mapping: {dict(enumerate(label_encoder.classes_))}\")\n",
    "    return label_encoder\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, test_df = load_existing_splits(CONFIG[\"input_data_dir\"])\n",
    "\n",
    "if train_df is None:\n",
    "    print(\"‚ùå Failed to load data splits. Exiting.\")\n",
    "else:\n",
    "    label_encoder = prepare_label_encoder(train_df)\n",
    "    \n",
    "    # Check problem class distribution\n",
    "    train_problem_count = (train_df['label'] == CONFIG['problem_class']).sum()\n",
    "    val_problem_count = (val_df['label'] == CONFIG['problem_class']).sum()\n",
    "    test_problem_count = (test_df['label'] == CONFIG['problem_class']).sum()\n",
    "    \n",
    "    print(f\"\\nüéØ Problem class '{CONFIG['problem_class']}' distribution:\")\n",
    "    print(f\"   Training: {train_problem_count} examples\")\n",
    "    print(f\"   Validation: {val_problem_count} examples\")\n",
    "    print(f\"   Test: {test_problem_count} examples\")\n",
    "    \n",
    "    if train_problem_count < 5:\n",
    "        print(\"‚ö†Ô∏è WARNING: Very few training examples for the problem class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ff671",
   "metadata": {},
   "source": [
    "## Data Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efaae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_minority_class(train_df, target_class, ratio=2):\n",
    "    \"\"\"Oversample the target class to improve balance\"\"\"\n",
    "    print(f\"\\nüîÑ Oversampling '{target_class}' by {ratio}x...\")\n",
    "    \n",
    "    # Get samples of target class\n",
    "    target_samples = train_df[train_df['label'] == target_class]\n",
    "    other_samples = train_df[train_df['label'] != target_class]\n",
    "    \n",
    "    print(f\"Original {target_class} samples: {len(target_samples)}\")\n",
    "    \n",
    "    # Oversample by repeating samples\n",
    "    oversampled_target = pd.concat([target_samples] * ratio, ignore_index=True)\n",
    "    \n",
    "    # Add some noise to avoid exact duplicates\n",
    "    for i in range(len(target_samples), len(oversampled_target)):\n",
    "        oversampled_target.at[i, 'text'] += \" \"\n",
    "    \n",
    "    # Combine with other samples\n",
    "    balanced_df = pd.concat([other_samples, oversampled_target], ignore_index=True)\n",
    "    \n",
    "    print(f\"After oversampling {target_class} samples: {len(oversampled_target)}\")\n",
    "    print(f\"Total training samples: {len(balanced_df)}\")\n",
    "    print(f\"New class distribution:\")\n",
    "    print(balanced_df['label'].value_counts())\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def calculate_class_weights(train_df, target_class, multiplier=5.0):\n",
    "    \"\"\"Calculate class weights with special emphasis on target class\"\"\"\n",
    "    label_counts = train_df['label'].value_counts()\n",
    "    \n",
    "    # Calculate base balanced weights\n",
    "    unique_labels = train_df['label'].unique()\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(unique_labels)\n",
    "    \n",
    "    y = label_encoder.transform(train_df['label'])\n",
    "    base_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    \n",
    "    # Create weight dictionary\n",
    "    class_weights = {}\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        if label == target_class:\n",
    "            class_weights[i] = base_weights[i] * multiplier\n",
    "        else:\n",
    "            class_weights[i] = base_weights[i]\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Class weights (with {multiplier}x multiplier for '{target_class}'):\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        print(f\"   {label}: {class_weights[i]:.3f}\")\n",
    "    \n",
    "    # Convert to tensor\n",
    "    weight_tensor = torch.tensor([class_weights[i] for i in range(len(label_encoder.classes_))], dtype=torch.float)\n",
    "    \n",
    "    return weight_tensor, class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1acac",
   "metadata": {},
   "source": [
    "## Custom Loss Functions and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with configurable loss functions\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights=None, use_focal_loss=False, focal_alpha=0.25, focal_gamma=2.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        \n",
    "        if use_focal_loss:\n",
    "            self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        \n",
    "        if self.use_focal_loss:\n",
    "            loss = self.focal_loss(logits, labels)\n",
    "        else:\n",
    "            if self.class_weights is not None:\n",
    "                loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "            else:\n",
    "                loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def tokenize_data(df, tokenizer, max_length):\n",
    "    \"\"\"Tokenize the data\"\"\"\n",
    "    dataset = Dataset.from_pandas(df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"}))\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=50)\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    \"\"\"Compute evaluation metrics with focus on target class\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_metrics = {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted'),\n",
    "        'precision_macro': precision_score(labels, predictions, average='macro'),\n",
    "        'recall_macro': recall_score(labels, predictions, average='macro'),\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_f1 = f1_score(labels, predictions, average=None)\n",
    "    per_class_precision = precision_score(labels, predictions, average=None)\n",
    "    per_class_recall = recall_score(labels, predictions, average=None)\n",
    "    \n",
    "    # Add metrics for each class\n",
    "    for i in range(len(per_class_f1)):\n",
    "        overall_metrics[f'f1_class_{i}'] = per_class_f1[i]\n",
    "        overall_metrics[f'precision_class_{i}'] = per_class_precision[i]\n",
    "        overall_metrics[f'recall_class_{i}'] = per_class_recall[i]\n",
    "    \n",
    "    return overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c240ff",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_roberta_with_strategy(train_df, val_df, test_df, label_encoder, strategy_config):\n",
    "    \"\"\"Train RoBERTa-Large with a specific strategy\"\"\"\n",
    "    \n",
    "    strategy_name = strategy_config[\"name\"]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ Training Strategy: {strategy_name}\")\n",
    "    print(f\"üìã Description: {strategy_config['description']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Prepare training data (with potential oversampling)\n",
    "    current_train_df = train_df.copy()\n",
    "    if strategy_config.get(\"oversample\", False):\n",
    "        current_train_df = oversample_minority_class(\n",
    "            current_train_df, \n",
    "            CONFIG[\"problem_class\"], \n",
    "            strategy_config.get(\"oversample_ratio\", 2)\n",
    "        )\n",
    "    \n",
    "    # Encode labels\n",
    "    current_train_df[\"label_id\"] = label_encoder.transform(current_train_df[\"label\"])\n",
    "    val_df_copy = val_df.copy()\n",
    "    val_df_copy[\"label_id\"] = label_encoder.transform(val_df_copy[\"label\"])\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy[\"label_id\"] = label_encoder.transform(test_df_copy[\"label\"])\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights, class_weights_dict = calculate_class_weights(\n",
    "        current_train_df, \n",
    "        CONFIG[\"problem_class\"], \n",
    "        strategy_config[\"class_weight_multiplier\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"Loading tokenizer and model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"target_model\"], use_fast=True)\n",
    "        \n",
    "        # Handle missing pad token\n",
    "        if tokenizer.pad_token is None:\n",
    "            if tokenizer.eos_token:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            else:\n",
    "                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        # Load model\n",
    "        num_labels = len(label_encoder.classes_)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CONFIG[\"target_model\"],\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        # Move to device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f\"Model loaded on {device}. Parameters: {model.num_parameters():,}\")\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        print(\"Tokenizing datasets...\")\n",
    "        train_dataset = tokenize_data(current_train_df, tokenizer, strategy_config[\"max_length\"])\n",
    "        val_dataset = tokenize_data(val_df_copy, tokenizer, strategy_config[\"max_length\"])\n",
    "        test_dataset = tokenize_data(test_df_copy, tokenizer, strategy_config[\"max_length\"])\n",
    "        \n",
    "        # Training arguments - temporary output, no model saving\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./temp_training_{strategy_name}\",  # Temporary directory\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=CONFIG[\"evaluation\"][\"eval_steps\"],\n",
    "            save_steps=9999999,  # Effectively disable saving\n",
    "            num_train_epochs=strategy_config[\"epochs\"],\n",
    "            per_device_train_batch_size=strategy_config[\"batch_size\"],\n",
    "            per_device_eval_batch_size=strategy_config[\"batch_size\"],\n",
    "            learning_rate=strategy_config[\"learning_rate\"],\n",
    "            weight_decay=strategy_config[\"weight_decay\"],\n",
    "            warmup_steps=20,\n",
    "            logging_steps=CONFIG[\"evaluation\"][\"logging_steps\"],\n",
    "            save_strategy=\"no\",  # Don't save checkpoints\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_f1_macro\",\n",
    "            greater_is_better=True,\n",
    "            report_to=\"none\",\n",
    "            dataloader_num_workers=0,\n",
    "            remove_unused_columns=True,\n",
    "            dataloader_pin_memory=False,\n",
    "            max_grad_norm=1.0,\n",
    "            save_total_limit=0,  # Don't save any checkpoints\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            optim=\"adamw_torch\",\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            class_weights=class_weights,\n",
    "            use_focal_loss=strategy_config.get(\"use_focal_loss\", False),\n",
    "            focal_alpha=strategy_config.get(\"focal_alpha\", 0.25),\n",
    "            focal_gamma=strategy_config.get(\"focal_gamma\", 2.0)\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"Starting training...\")\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Note: Model checkpoints are not saved to reduce storage requirements\n",
    "        # Models can be recreated using the same configuration if needed\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"Evaluating on test set...\")\n",
    "        test_results = trainer.predict(test_dataset)\n",
    "        \n",
    "        # Extract predictions and probabilities\n",
    "        logits = test_results.predictions\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "        predicted_labels = np.argmax(probabilities, axis=1)\n",
    "        true_labels = test_results.label_ids\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "        \n",
    "        # Convert back to label names\n",
    "        true_label_names = label_encoder.inverse_transform(true_labels)\n",
    "        predicted_label_names = label_encoder.inverse_transform(predicted_labels)\n",
    "        \n",
    "        # Calculate detailed metrics\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        f1_macro = f1_score(true_labels, predicted_labels, average='macro')\n",
    "        f1_weighted = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "        \n",
    "        # Per-class metrics\n",
    "        per_class_f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "        per_class_precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "        per_class_recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "        \n",
    "        # Focus on problem class\n",
    "        problem_class_id = label_encoder.transform([CONFIG[\"problem_class\"]])[0]\n",
    "        problem_class_f1 = per_class_f1[problem_class_id]\n",
    "        problem_class_precision = per_class_precision[problem_class_id]\n",
    "        problem_class_recall = per_class_recall[problem_class_id]\n",
    "        \n",
    "        # Count correct predictions for problem class\n",
    "        problem_class_mask = true_labels == problem_class_id\n",
    "        problem_class_correct = (predicted_labels[problem_class_mask] == problem_class_id).sum()\n",
    "        problem_class_total = problem_class_mask.sum()\n",
    "        problem_class_accuracy = problem_class_correct / problem_class_total if problem_class_total > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            'strategy_name': strategy_name,\n",
    "            'strategy_config': strategy_config,\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'problem_class_f1': problem_class_f1,\n",
    "            'problem_class_precision': problem_class_precision,\n",
    "            'problem_class_recall': problem_class_recall,\n",
    "            'problem_class_accuracy': problem_class_accuracy,\n",
    "            'problem_class_correct': int(problem_class_correct),\n",
    "            'problem_class_total': int(problem_class_total),\n",
    "            'train_time': train_result.metrics.get('train_runtime', 0),\n",
    "            'train_loss': train_result.metrics.get('train_loss', 0),\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics for all classes\n",
    "        for i, class_name in enumerate(label_encoder.classes_):\n",
    "            metrics[f'f1_{class_name}'] = per_class_f1[i]\n",
    "            metrics[f'precision_{class_name}'] = per_class_precision[i]\n",
    "            metrics[f'recall_{class_name}'] = per_class_recall[i]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Strategy Results:\")\n",
    "        print(f\"   Overall Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   Overall F1-Macro: {f1_macro:.4f}\")\n",
    "        print(f\"   üéØ {CONFIG['problem_class']} F1: {problem_class_f1:.4f}\")\n",
    "        print(f\"   üéØ {CONFIG['problem_class']} Accuracy: {problem_class_accuracy:.4f} ({problem_class_correct}/{problem_class_total})\")\n",
    "        \n",
    "        # Create detailed results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'text': test_df_copy['text'].values,\n",
    "            'true_label': true_label_names,\n",
    "            'predicted_label': predicted_label_names,\n",
    "            'confidence': confidence_scores,\n",
    "            'correct': true_labels == predicted_labels\n",
    "        })\n",
    "        \n",
    "        # Add probability columns for each class\n",
    "        for i, class_name in enumerate(label_encoder.classes_):\n",
    "            results_df[f'prob_{class_name}'] = probabilities[:, i]\n",
    "        \n",
    "        # Save results\n",
    "        predictions_path = f\"{CONFIG['output_dir']}/predictions/{strategy_name}_predictions.csv\"\n",
    "        results_df.to_csv(predictions_path, index=False)\n",
    "        \n",
    "        metrics_path = f\"{CONFIG['output_dir']}/metrics/{strategy_name}_metrics.json\"\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2, default=str)\n",
    "        \n",
    "        # Clean up temporary training directory\n",
    "        import shutil\n",
    "        temp_dir = f\"./temp_training_{strategy_name}\"\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        \n",
    "        return metrics, results_df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in strategy {strategy_name}: {str(e)}\")\n",
    "        \n",
    "        error_metrics = {\n",
    "            'strategy_name': strategy_name,\n",
    "            'strategy_config': strategy_config,\n",
    "            'error': str(e),\n",
    "            'error_type': type(e).__name__,\n",
    "            'accuracy': None,\n",
    "            'f1_macro': None,\n",
    "            'problem_class_f1': None\n",
    "        }\n",
    "        \n",
    "        return error_metrics, None, False\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        try:\n",
    "            del model, trainer, tokenizer\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab964e5",
   "metadata": {},
   "source": [
    "## Execute All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef16d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_strategies():\n",
    "    \"\"\"Execute all class imbalance strategies\"\"\"\n",
    "    \n",
    "    if train_df is None:\n",
    "        print(\"‚ùå No training data available\")\n",
    "        return None, None\n",
    "    \n",
    "    all_metrics = []\n",
    "    successful_runs = 0\n",
    "    failed_runs = 0\n",
    "    \n",
    "    print(f\"\\nüöÄ Testing {len(CONFIG['training_strategies'])} class imbalance strategies...\")\n",
    "    \n",
    "    for i, strategy_config in enumerate(CONFIG[\"training_strategies\"], 1):\n",
    "        print(f\"\\nüìã Strategy {i}/{len(CONFIG['training_strategies'])}: {strategy_config['name']}\")\n",
    "        \n",
    "        metrics, results_df, success = train_roberta_with_strategy(\n",
    "            train_df, val_df, test_df, label_encoder, strategy_config\n",
    "        )\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        if success:\n",
    "            successful_runs += 1\n",
    "            print(f\"‚úÖ Strategy '{strategy_config['name']}' completed successfully\")\n",
    "        else:\n",
    "            failed_runs += 1\n",
    "            print(f\"‚ùå Strategy '{strategy_config['name']}' failed\")\n",
    "    \n",
    "    return all_metrics, successful_runs, failed_runs\n",
    "\n",
    "# Run all strategies\n",
    "all_metrics, successful_runs, failed_runs = run_all_strategies()\n",
    "\n",
    "if all_metrics:\n",
    "    print(f\"\\nüìä Strategy execution complete:\")\n",
    "    print(f\"   Successful: {successful_runs}\")\n",
    "    print(f\"   Failed: {failed_runs}\")\n",
    "else:\n",
    "    print(\"‚ùå No strategies executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4685a",
   "metadata": {},
   "source": [
    "## Create Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68443c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_strategy_comparison_plot(all_metrics):\n",
    "    \"\"\"Create comparison plot of different strategies\"\"\"\n",
    "    \n",
    "    successful_metrics = [m for m in all_metrics if m.get('accuracy') is not None]\n",
    "    \n",
    "    if not successful_metrics:\n",
    "        print(\"‚ùå No successful strategies to compare\")\n",
    "        return None\n",
    "    \n",
    "    df_metrics = pd.DataFrame(successful_metrics)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    strategies = df_metrics['strategy_name'].tolist()\n",
    "    \n",
    "    # Plot 1: Overall F1-Macro scores\n",
    "    bars1 = ax1.bar(strategies, df_metrics['f1_macro'], color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Overall F1-Macro Score by Strategy')\n",
    "    ax1.set_ylabel('F1-Macro Score')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars1, df_metrics['f1_macro']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Problem class F1 scores\n",
    "    bars2 = ax2.bar(strategies, df_metrics['problem_class_f1'], color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title(f'{CONFIG[\"problem_class\"]} F1-Score by Strategy')\n",
    "    ax2.set_ylabel('F1-Score')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars2, df_metrics['problem_class_f1']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 3: Problem class accuracy\n",
    "    correct_counts = df_metrics['problem_class_correct'].tolist()\n",
    "    total_counts = df_metrics['problem_class_total'].tolist()\n",
    "    \n",
    "    bars3 = ax3.bar(strategies, df_metrics['problem_class_accuracy'], color='lightgreen', alpha=0.7)\n",
    "    ax3.set_title(f'{CONFIG[\"problem_class\"]} Accuracy by Strategy')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, correct, total in zip(bars3, correct_counts, total_counts):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, height + 0.01, \n",
    "                f'{correct}/{total}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 4: Training time comparison\n",
    "    bars4 = ax4.bar(strategies, df_metrics['train_time'], color='orange', alpha=0.7)\n",
    "    ax4.set_title('Training Time by Strategy')\n",
    "    ax4.set_ylabel('Training Time (seconds)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars4, df_metrics['train_time']):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{value:.0f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = f\"{CONFIG['output_dir']}/plots/strategy_comparison.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return plot_path\n",
    "\n",
    "# Create comparison plot\n",
    "if all_metrics:\n",
    "    plot_path = create_strategy_comparison_plot(all_metrics)\n",
    "    if plot_path:\n",
    "        print(f\"üìä Strategy comparison plot saved: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7489b",
   "metadata": {},
   "source": [
    "## Generate Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381428b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_report(all_metrics):\n",
    "    \"\"\"Create detailed analysis report\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# üéØ Unreliable Source Classification Improvement Report\")\n",
    "    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    successful_strategies = [m for m in all_metrics if m.get('accuracy') is not None]\n",
    "    failed_strategies = [m for m in all_metrics if m.get('accuracy') is None]\n",
    "    \n",
    "    report.append(\"## üéØ Executive Summary\")\n",
    "    report.append(f\"- **Problem**: '{CONFIG['problem_class']}' class had 0% F1-score\")\n",
    "    report.append(f\"- **Strategies Tested**: {len(CONFIG['training_strategies'])}\")\n",
    "    report.append(f\"- **Successful Strategies**: {len(successful_strategies)}\")\n",
    "    report.append(f\"- **Failed Strategies**: {len(failed_strategies)}\")\n",
    "    \n",
    "    if successful_strategies:\n",
    "        best_strategy = max(successful_strategies, key=lambda x: x['problem_class_f1'])\n",
    "        report.append(f\"- **Best Strategy**: {best_strategy['strategy_name']}\")\n",
    "        report.append(f\"- **Best {CONFIG['problem_class']} F1-Score**: {best_strategy['problem_class_f1']:.4f}\")\n",
    "        report.append(f\"- **Improvement**: From 0.00 to {best_strategy['problem_class_f1']:.2f}\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Strategy Results\n",
    "    report.append(\"## üìä Class Imbalance Strategy Results\")\n",
    "    \n",
    "    if successful_strategies:\n",
    "        # Sort by problem class F1 score\n",
    "        successful_strategies.sort(key=lambda x: x['problem_class_f1'], reverse=True)\n",
    "        \n",
    "        for i, strategy in enumerate(successful_strategies, 1):\n",
    "            report.append(f\"### {i}. {strategy['strategy_name']}\")\n",
    "            report.append(f\"**Description**: {strategy['strategy_config']['description']}\")\n",
    "            report.append(f\"- **Overall F1-Macro**: {strategy['f1_macro']:.4f}\")\n",
    "            report.append(f\"- **üéØ {CONFIG['problem_class']} F1**: {strategy['problem_class_f1']:.4f}\")\n",
    "            report.append(f\"- **üéØ {CONFIG['problem_class']} Accuracy**: {strategy['problem_class_accuracy']:.4f}\")\n",
    "            report.append(f\"- **üéØ Correct Predictions**: {strategy['problem_class_correct']}/{strategy['problem_class_total']}\")\n",
    "            \n",
    "            # Configuration details\n",
    "            config = strategy['strategy_config']\n",
    "            report.append(f\"- **Key Parameters**:\")\n",
    "            report.append(f\"  - Class Weight Multiplier: {config['class_weight_multiplier']}\")\n",
    "            report.append(f\"  - Focal Loss: {config.get('use_focal_loss', False)}\")\n",
    "            report.append(f\"  - Oversampling: {config.get('oversample', False)}\")\n",
    "            if config.get('oversample', False):\n",
    "                report.append(f\"  - Oversample Ratio: {config.get('oversample_ratio', 'N/A')}\")\n",
    "            \n",
    "            # Performance assessment\n",
    "            if strategy['problem_class_f1'] >= 0.4:\n",
    "                report.append(\"- ‚úÖ **Significant improvement achieved**\")\n",
    "            elif strategy['problem_class_f1'] >= 0.2:\n",
    "                report.append(\"- ‚ö†Ô∏è **Moderate improvement**\")\n",
    "            elif strategy['problem_class_f1'] > 0:\n",
    "                report.append(\"- üîÑ **Limited improvement**\")\n",
    "            else:\n",
    "                report.append(\"- ‚ùå **No improvement**\")\n",
    "            \n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Technical Analysis\n",
    "    if successful_strategies:\n",
    "        report.append(\"## üî¨ Technical Analysis\")\n",
    "        \n",
    "        # Analyze what worked\n",
    "        oversampling_strategies = [s for s in successful_strategies if s['strategy_config'].get('oversample', False)]\n",
    "        focal_loss_strategies = [s for s in successful_strategies if s['strategy_config'].get('use_focal_loss', False)]\n",
    "        \n",
    "        if oversampling_strategies:\n",
    "            avg_f1_oversampling = np.mean([s['problem_class_f1'] for s in oversampling_strategies])\n",
    "            report.append(f\"- **Oversampling strategies**: Average F1-score {avg_f1_oversampling:.3f}\")\n",
    "        \n",
    "        if focal_loss_strategies:\n",
    "            avg_f1_focal = np.mean([s['problem_class_f1'] for s in focal_loss_strategies])\n",
    "            report.append(f\"- **Focal loss strategies**: Average F1-score {avg_f1_focal:.3f}\")\n",
    "        \n",
    "        # Best performing technique\n",
    "        best_f1 = max([s['problem_class_f1'] for s in successful_strategies])\n",
    "        best_technique = max(successful_strategies, key=lambda x: x['problem_class_f1'])['strategy_name']\n",
    "        \n",
    "        report.append(f\"- **Best performing technique**: {best_technique} (F1={best_f1:.3f})\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Conclusions\n",
    "    report.append(\"## üí° Key Findings\")\n",
    "    \n",
    "    if successful_strategies:\n",
    "        best_strategy = successful_strategies[0]\n",
    "        \n",
    "        if best_strategy['problem_class_f1'] >= 0.4:\n",
    "            report.append(\"1. **Problem partially solved** - achieved meaningful improvement\")\n",
    "            report.append(f\"2. **{best_strategy['strategy_name']} most effective** - achieved {best_strategy['problem_class_f1']:.2f} F1-score\")\n",
    "        elif best_strategy['problem_class_f1'] >= 0.2:\n",
    "            report.append(\"1. **Limited success** - some improvement but still challenging\")\n",
    "            report.append(\"2. **Class imbalance techniques help but insufficient**\")\n",
    "        else:\n",
    "            report.append(\"1. **Minimal improvement** - fundamental classification difficulty remains\")\n",
    "            report.append(\"2. **Consider class consolidation** - merge with similar categories\")\n",
    "        \n",
    "        # Check if oversampling was best\n",
    "        if best_strategy['strategy_config'].get('oversample', False):\n",
    "            report.append(\"3. **Oversampling most effective** - data augmentation helped most\")\n",
    "        else:\n",
    "            report.append(\"3. **Class weighting approaches preferred** - loss function modifications effective\")\n",
    "    else:\n",
    "        report.append(\"1. **All strategies failed** - fundamental semantic similarity issue\")\n",
    "        report.append(\"2. **Recommend class consolidation** - merge unreliable source with slanted\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(\"*Report generated by Unreliable Source Classification Improvement*\")\n",
    "    \n",
    "    # Save report\n",
    "    report_path = f\"{CONFIG['output_dir']}/reports/class_imbalance_analysis.md\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# Generate analysis report\n",
    "if all_metrics:\n",
    "    report_path = create_analysis_report(all_metrics)\n",
    "    if report_path:\n",
    "        print(f\"üìù Analysis report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb238d5f",
   "metadata": {},
   "source": [
    "## Save Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2428648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary results\n",
    "if all_metrics:\n",
    "    summary_path = f\"{CONFIG['output_dir']}/strategy_comparison_summary.csv\"\n",
    "    df_all_metrics = pd.DataFrame(all_metrics)\n",
    "    df_all_metrics.to_csv(summary_path, index=False)\n",
    "    print(f\"üíæ Strategy summary saved: {summary_path}\")\n",
    "\n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ CLASS IMBALANCE ANALYSIS COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"üìä Strategies tested: {len(CONFIG['training_strategies'])}\")\n",
    "    \n",
    "    # Find best strategy\n",
    "    successful_metrics = [m for m in all_metrics if m.get('accuracy') is not None]\n",
    "    if successful_metrics:\n",
    "        best_strategy = max(successful_metrics, key=lambda x: x['problem_class_f1'])\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST STRATEGY FOR '{CONFIG['problem_class']}':\")\n",
    "        print(f\"   Strategy: {best_strategy['strategy_name']}\")\n",
    "        print(f\"   F1-Score: {best_strategy['problem_class_f1']:.4f}\")\n",
    "        print(f\"   Improvement: From 0.00 to {best_strategy['problem_class_f1']:.2f}\")\n",
    "        print(f\"   Correct Predictions: {best_strategy['problem_class_correct']}/{best_strategy['problem_class_total']}\")\n",
    "        \n",
    "        if best_strategy['problem_class_f1'] > 0.3:\n",
    "            print(f\"   üéØ Significant improvement achieved!\")\n",
    "        elif best_strategy['problem_class_f1'] > 0:\n",
    "            print(f\"   ‚ö†Ô∏è Limited improvement - consider alternative approaches\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå No improvement - fundamental classification difficulty\")\n",
    "        \n",
    "        # Check if this matches thesis claim about oversampling\n",
    "        if best_strategy['strategy_name'] == 'minority_oversampling':\n",
    "            print(f\"   ‚úÖ Confirms thesis finding: oversampling was most effective\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Results saved to: {CONFIG['output_dir']}\")\n",
    "else:\n",
    "    print(\"‚ùå No results to save\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
