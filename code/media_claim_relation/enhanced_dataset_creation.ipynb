{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903c8317",
   "metadata": {},
   "source": [
    "# Enhanced Dataset Creation with Community Notes Metadata\n",
    "\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69dc076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b460d13",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebe576",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"rel_typ_file\": \"rel_typ.xlsx\",\n",
    "    \"metadata_file\": \"for_bsc_project.csv\", \n",
    "    \"output_dir\": \"enhanced_dataset\",\n",
    "    \"matching_method\": \"hybrid\",  # Options: \"text_similarity\", \"fuzzy_match\", \"hybrid\"\n",
    "    \"similarity_threshold\": 0.8,  # For text similarity matching\n",
    "    \"fuzzy_threshold\": 85,  # For fuzzy string matching\n",
    "    \n",
    "    # Metadata columns to include for classification\n",
    "    \"metadata_columns\": [\n",
    "        'misleadingOther',\n",
    "        'misleadingFactualError', \n",
    "        'misleadingManipulatedMedia',\n",
    "        'misleadingOutdatedInformation',\n",
    "        'misleadingMissingImportantContext',\n",
    "        'misleadingUnverifiedClaimAsFact',\n",
    "        'misleadingSatire',\n",
    "        'notMisleadingOther',\n",
    "        'notMisleadingFactuallyCorrect',\n",
    "        'notMisleadingOutdatedButNotWhenWritten',\n",
    "        'notMisleadingClearlySatire',\n",
    "        'notMisleadingPersonalOpinion',\n",
    "        'trustworthySources',\n",
    "        'believable',\n",
    "        'harmful',\n",
    "        'validationDifficulty'\n",
    "    ],\n",
    "    \n",
    "    # Additional useful columns\n",
    "    \"additional_columns\": [\n",
    "        'tweetId',\n",
    "        'noteId', \n",
    "        'classification',\n",
    "        'summary',\n",
    "        'created_at',\n",
    "        'favorite_count',\n",
    "        'retweet_count',\n",
    "        'reply_count',\n",
    "        'quote_count'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "for subdir in ['plots', 'reports']:\n",
    "    os.makedirs(f\"{CONFIG['output_dir']}/{subdir}\", exist_ok=True)\n",
    "\n",
    "print(f\"üîó Enhanced Dataset Creation with Community Notes Metadata\")\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f8048",
   "metadata": {},
   "source": [
    "## Load Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d339929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rel_typ_data(file_path):\n",
    "    \"\"\"Load the original rel_typ.xlsx dataset\"\"\"\n",
    "    print(f\"\\nüìä Loading original dataset: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"‚úÖ Loaded {len(df)} rows from {file_path}\")\n",
    "        \n",
    "        # Show basic info\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        print(f\"üè∑Ô∏è Labels: {df['label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Clean basic columns\n",
    "        df[\"tweet_text\"] = df[\"tweet_text\"].astype(str).fillna(\"\").str.strip()\n",
    "        df[\"note_text\"] = df[\"note_text\"].astype(str).fillna(\"\").str.strip()\n",
    "        df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "        \n",
    "        # Create combined text like in training\n",
    "        df[\"text\"] = df[\"tweet_text\"] + \" [SEP] \" + df[\"note_text\"]\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the original dataset\n",
    "rel_typ_df = load_rel_typ_data(CONFIG[\"rel_typ_file\"])\n",
    "\n",
    "if rel_typ_df is None:\n",
    "    print(\"‚ùå Failed to load rel_typ dataset\")\n",
    "else:\n",
    "    print(f\"üìä Ready to enhance {len(rel_typ_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2419fc",
   "metadata": {},
   "source": [
    "## Load Community Notes Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f81b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_file(file_path):\n",
    "    \"\"\"Load the large metadata CSV file\"\"\"\n",
    "    print(f\"\\nüìà Loading metadata file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Try to read with different encodings\n",
    "        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "        df = None\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding, low_memory=False)\n",
    "                print(f\"‚úÖ Loaded {len(df)} rows with {encoding} encoding\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if df is None:\n",
    "            print(\"‚ùå Could not read file with any encoding\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üìã Shape: {df.shape}\")\n",
    "        print(f\"üìã Available columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Check which metadata columns we actually have\n",
    "        available_metadata = [col for col in CONFIG[\"metadata_columns\"] if col in df.columns]\n",
    "        missing_metadata = [col for col in CONFIG[\"metadata_columns\"] if col not in df.columns]\n",
    "        \n",
    "        print(f\"‚úÖ Available metadata columns ({len(available_metadata)}): {available_metadata}\")\n",
    "        if missing_metadata:\n",
    "            print(f\"‚ö†Ô∏è Missing metadata columns ({len(missing_metadata)}): {missing_metadata}\")\n",
    "        \n",
    "        # Check for text columns we can use for matching\n",
    "        text_columns = [col for col in ['full_text', 'tweet_text', 'summary', 'text'] if col in df.columns]\n",
    "        print(f\"üìù Available text columns for matching: {text_columns}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the metadata file\n",
    "metadata_df = load_metadata_file(CONFIG[\"metadata_file\"])\n",
    "\n",
    "if metadata_df is None:\n",
    "    print(\"‚ùå Failed to load metadata dataset\")\n",
    "else:\n",
    "    print(f\"üìä Ready to match with {len(metadata_df)} metadata records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fc28e",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_matching(text):\n",
    "    \"\"\"Clean text for better matching\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'[@#]\\w+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_tweet_from_combined_text(combined_text):\n",
    "    \"\"\"Extract just the tweet part from 'tweet [SEP] note' format\"\"\"\n",
    "    if pd.isna(combined_text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(combined_text)\n",
    "    if '[SEP]' in text:\n",
    "        return text.split('[SEP]')[0].strip()\n",
    "    return text\n",
    "\n",
    "def text_similarity_matching(rel_typ_df, metadata_df, text_col_metadata='full_text'):\n",
    "    \"\"\"Match using TF-IDF cosine similarity\"\"\"\n",
    "    print(f\"\\nüîç Performing text similarity matching...\")\n",
    "    \n",
    "    # Prepare texts\n",
    "    rel_typ_texts = [clean_text_for_matching(extract_tweet_from_combined_text(text)) \n",
    "                    for text in rel_typ_df['text']]\n",
    "    \n",
    "    if text_col_metadata not in metadata_df.columns:\n",
    "        print(f\"‚ùå Column {text_col_metadata} not found in metadata\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    metadata_texts = [clean_text_for_matching(text) \n",
    "                     for text in metadata_df[text_col_metadata]]\n",
    "    \n",
    "    # Remove empty texts\n",
    "    rel_typ_valid = [(i, text) for i, text in enumerate(rel_typ_texts) if text and len(text) > 10]\n",
    "    metadata_valid = [(i, text) for i, text in enumerate(metadata_texts) if text and len(text) > 10]\n",
    "    \n",
    "    if not rel_typ_valid or not metadata_valid:\n",
    "        print(\"‚ùå No valid texts for matching\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üìä Matching {len(rel_typ_valid)} rel_typ texts with {len(metadata_valid)} metadata texts\")\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    all_texts = [text for _, text in rel_typ_valid] + [text for _, text in metadata_valid]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Split matrices\n",
    "    rel_typ_matrix = tfidf_matrix[:len(rel_typ_valid)]\n",
    "    metadata_matrix = tfidf_matrix[len(rel_typ_valid):]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(rel_typ_matrix, metadata_matrix)\n",
    "    \n",
    "    matches = []\n",
    "    for i, (rel_idx, _) in enumerate(rel_typ_valid):\n",
    "        best_match_idx = np.argmax(similarities[i])\n",
    "        best_similarity = similarities[i][best_match_idx]\n",
    "        \n",
    "        if best_similarity >= CONFIG[\"similarity_threshold\"]:\n",
    "            metadata_idx = metadata_valid[best_match_idx][0]\n",
    "            matches.append({\n",
    "                'rel_typ_idx': rel_idx,\n",
    "                'metadata_idx': metadata_idx,\n",
    "                'similarity_score': best_similarity,\n",
    "                'rel_typ_text': rel_typ_texts[rel_idx][:100] + \"...\",\n",
    "                'metadata_text': metadata_texts[metadata_idx][:100] + \"...\"\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(matches)} matches above threshold {CONFIG['similarity_threshold']}\")\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "def fuzzy_matching(rel_typ_df, metadata_df, text_col_metadata='full_text'):\n",
    "    \"\"\"Match using fuzzy string matching\"\"\"\n",
    "    print(f\"\\nüîç Performing fuzzy string matching...\")\n",
    "    \n",
    "    rel_typ_texts = [clean_text_for_matching(extract_tweet_from_combined_text(text)) \n",
    "                    for text in rel_typ_df['text']]\n",
    "    \n",
    "    if text_col_metadata not in metadata_df.columns:\n",
    "        print(f\"‚ùå Column {text_col_metadata} not found in metadata\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    metadata_texts = [clean_text_for_matching(text) \n",
    "                     for text in metadata_df[text_col_metadata]]\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    for i, rel_text in enumerate(rel_typ_texts):\n",
    "        if not rel_text or len(rel_text) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Find best fuzzy match\n",
    "        valid_metadata = [(j, text) for j, text in enumerate(metadata_texts) \n",
    "                         if text and len(text) > 10]\n",
    "        \n",
    "        if not valid_metadata:\n",
    "            continue\n",
    "        \n",
    "        metadata_texts_only = [text for _, text in valid_metadata]\n",
    "        best_match = process.extractOne(rel_text, metadata_texts_only, scorer=fuzz.ratio)\n",
    "        \n",
    "        if best_match and best_match[1] >= CONFIG[\"fuzzy_threshold\"]:\n",
    "            # Find the original index\n",
    "            matched_text = best_match[0]\n",
    "            metadata_idx = None\n",
    "            for j, text in valid_metadata:\n",
    "                if text == matched_text:\n",
    "                    metadata_idx = j\n",
    "                    break\n",
    "            \n",
    "            if metadata_idx is not None:\n",
    "                matches.append({\n",
    "                    'rel_typ_idx': i,\n",
    "                    'metadata_idx': metadata_idx,\n",
    "                    'fuzzy_score': best_match[1],\n",
    "                    'rel_typ_text': rel_text[:100] + \"...\",\n",
    "                    'metadata_text': matched_text[:100] + \"...\"\n",
    "                })\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(matches)} fuzzy matches above threshold {CONFIG['fuzzy_threshold']}\")\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "def hybrid_matching(rel_typ_df, metadata_df):\n",
    "    \"\"\"Combine multiple matching methods\"\"\"\n",
    "    print(f\"\\nüîç Performing hybrid matching...\")\n",
    "    \n",
    "    all_matches = []\n",
    "    \n",
    "    # Try different text columns in metadata\n",
    "    text_columns_to_try = ['full_text', 'summary', 'tweet_text', 'text']\n",
    "    available_text_cols = [col for col in text_columns_to_try if col in metadata_df.columns]\n",
    "    \n",
    "    for text_col in available_text_cols:\n",
    "        print(f\"\\nüìù Trying matching with column: {text_col}\")\n",
    "        \n",
    "        # Text similarity matching\n",
    "        similarity_matches = text_similarity_matching(rel_typ_df, metadata_df, text_col)\n",
    "        if not similarity_matches.empty:\n",
    "            similarity_matches['match_method'] = f'similarity_{text_col}'\n",
    "            similarity_matches['match_score'] = similarity_matches['similarity_score']\n",
    "            all_matches.append(similarity_matches)\n",
    "        \n",
    "        # Fuzzy matching  \n",
    "        fuzzy_matches = fuzzy_matching(rel_typ_df, metadata_df, text_col)\n",
    "        if not fuzzy_matches.empty:\n",
    "            fuzzy_matches['match_method'] = f'fuzzy_{text_col}'\n",
    "            fuzzy_matches['match_score'] = fuzzy_matches['fuzzy_score'] / 100  # Normalize to 0-1\n",
    "            all_matches.append(fuzzy_matches)\n",
    "    \n",
    "    if not all_matches:\n",
    "        print(\"‚ùå No matches found with any method\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all matches\n",
    "    combined_matches = pd.concat(all_matches, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates, keeping the best match for each rel_typ_idx\n",
    "    best_matches = combined_matches.loc[combined_matches.groupby('rel_typ_idx')['match_score'].idxmax()]\n",
    "    \n",
    "    print(f\"‚úÖ Final hybrid matching: {len(best_matches)} unique matches\")\n",
    "    \n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debff50",
   "metadata": {},
   "source": [
    "## Perform Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64756ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform matching using the configured method\n",
    "if rel_typ_df is not None and metadata_df is not None:\n",
    "    print(f\"\\nüîç Starting {CONFIG['matching_method']} matching...\")\n",
    "    \n",
    "    if CONFIG[\"matching_method\"] == \"hybrid\":\n",
    "        matches_df = hybrid_matching(rel_typ_df, metadata_df)\n",
    "    elif CONFIG[\"matching_method\"] == \"text_similarity\":\n",
    "        matches_df = text_similarity_matching(rel_typ_df, metadata_df)\n",
    "    elif CONFIG[\"matching_method\"] == \"fuzzy_match\":\n",
    "        matches_df = fuzzy_matching(rel_typ_df, metadata_df)\n",
    "    else:\n",
    "        print(f\"‚ùå Unknown matching method: {CONFIG['matching_method']}\")\n",
    "        matches_df = pd.DataFrame()\n",
    "    \n",
    "    if matches_df.empty:\n",
    "        print(\"‚ùå No matches found - cannot create enhanced dataset\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(matches_df)} matches for dataset enhancement\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform matching - datasets not loaded\")\n",
    "    matches_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836afdab",
   "metadata": {},
   "source": [
    "## Merge Datasets and Add Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef41b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(rel_typ_df, metadata_df, matches_df):\n",
    "    \"\"\"Merge the datasets based on matches\"\"\"\n",
    "    print(f\"\\nüîó Merging datasets...\")\n",
    "    \n",
    "    if matches_df.empty:\n",
    "        print(\"‚ùå No matches to merge\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare metadata columns\n",
    "    available_metadata_cols = [col for col in CONFIG[\"metadata_columns\"] if col in metadata_df.columns]\n",
    "    available_additional_cols = [col for col in CONFIG[\"additional_columns\"] if col in metadata_df.columns]\n",
    "    \n",
    "    all_cols_to_merge = available_metadata_cols + available_additional_cols\n",
    "    \n",
    "    print(f\"üìä Merging {len(all_cols_to_merge)} metadata columns: {all_cols_to_merge}\")\n",
    "    \n",
    "    # Create enhanced dataset\n",
    "    enhanced_data = []\n",
    "    \n",
    "    for _, match in matches_df.iterrows():\n",
    "        rel_idx = int(match['rel_typ_idx'])\n",
    "        meta_idx = int(match['metadata_idx'])\n",
    "        \n",
    "        # Get original data\n",
    "        rel_row = rel_typ_df.iloc[rel_idx].copy()\n",
    "        meta_row = metadata_df.iloc[meta_idx]\n",
    "        \n",
    "        # Add metadata\n",
    "        for col in all_cols_to_merge:\n",
    "            if col in metadata_df.columns:\n",
    "                rel_row[f'meta_{col}'] = meta_row[col]\n",
    "        \n",
    "        # Add matching info\n",
    "        rel_row['match_method'] = match['match_method']\n",
    "        rel_row['match_score'] = match['match_score']\n",
    "        rel_row['has_metadata'] = True\n",
    "        \n",
    "        enhanced_data.append(rel_row)\n",
    "    \n",
    "    # Add unmatched rows\n",
    "    matched_indices = set(matches_df['rel_typ_idx'].astype(int))\n",
    "    unmatched_indices = set(range(len(rel_typ_df))) - matched_indices\n",
    "    \n",
    "    print(f\"üìä Matched: {len(matched_indices)}, Unmatched: {len(unmatched_indices)}\")\n",
    "    \n",
    "    for idx in unmatched_indices:\n",
    "        rel_row = rel_typ_df.iloc[idx].copy()\n",
    "        \n",
    "        # Add empty metadata columns\n",
    "        for col in all_cols_to_merge:\n",
    "            rel_row[f'meta_{col}'] = np.nan\n",
    "        \n",
    "        rel_row['match_method'] = 'no_match'\n",
    "        rel_row['match_score'] = 0.0\n",
    "        rel_row['has_metadata'] = False\n",
    "        \n",
    "        enhanced_data.append(rel_row)\n",
    "    \n",
    "    enhanced_df = pd.DataFrame(enhanced_data)\n",
    "    \n",
    "    print(f\"‚úÖ Enhanced dataset created: {len(enhanced_df)} rows\")\n",
    "    print(f\"üìä Rows with metadata: {enhanced_df['has_metadata'].sum()}\")\n",
    "    print(f\"üìä Coverage: {enhanced_df['has_metadata'].sum() / len(enhanced_df) * 100:.1f}%\")\n",
    "    \n",
    "    return enhanced_df\n",
    "\n",
    "# Merge the datasets\n",
    "if not matches_df.empty:\n",
    "    enhanced_df = merge_datasets(rel_typ_df, metadata_df, matches_df)\n",
    "    \n",
    "    if enhanced_df is not None:\n",
    "        print(f\"üéØ Enhanced dataset ready with {enhanced_df['has_metadata'].sum()} samples containing metadata\")\n",
    "else:\n",
    "    enhanced_df = None\n",
    "    print(\"‚ùå Cannot create enhanced dataset - no matches found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189498f",
   "metadata": {},
   "source": [
    "## Analyze Metadata Patterns by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metadata_patterns(enhanced_df):\n",
    "    \"\"\"Analyze patterns in the metadata by label\"\"\"\n",
    "    print(f\"\\nüìä Analyzing metadata patterns by label...\")\n",
    "    \n",
    "    if enhanced_df is None or enhanced_df.empty:\n",
    "        return None, None\n",
    "    \n",
    "    # Filter to rows with metadata\n",
    "    with_metadata = enhanced_df[enhanced_df['has_metadata'] == True].copy()\n",
    "    \n",
    "    if len(with_metadata) == 0:\n",
    "        print(\"‚ùå No rows with metadata to analyze\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"üìä Analyzing {len(with_metadata)} rows with metadata\")\n",
    "    \n",
    "    # Get metadata columns\n",
    "    metadata_cols = [col for col in with_metadata.columns if col.startswith('meta_') and col in with_metadata.columns]\n",
    "    numeric_metadata = []\n",
    "    \n",
    "    for col in metadata_cols:\n",
    "        if with_metadata[col].dtype in ['bool', 'int64', 'float64'] or with_metadata[col].isin([0, 1, True, False]).all():\n",
    "            numeric_metadata.append(col)\n",
    "    \n",
    "    print(f\"üìä Found {len(numeric_metadata)} numeric metadata columns\")\n",
    "    \n",
    "    if not numeric_metadata:\n",
    "        print(\"‚ö†Ô∏è No numeric metadata columns found\")\n",
    "        return None, None\n",
    "    \n",
    "    # Analyze by label\n",
    "    analysis_results = {}\n",
    "    \n",
    "    for label in with_metadata['label'].unique():\n",
    "        label_data = with_metadata[with_metadata['label'] == label]\n",
    "        label_analysis = {'label': label, 'count': len(label_data)}\n",
    "        \n",
    "        for col in numeric_metadata:\n",
    "            if col in label_data.columns:\n",
    "                # Convert to numeric, handling various formats\n",
    "                values = pd.to_numeric(label_data[col], errors='coerce')\n",
    "                values = values.dropna()\n",
    "                \n",
    "                if len(values) > 0:\n",
    "                    label_analysis[col] = {\n",
    "                        'mean': values.mean(),\n",
    "                        'sum': values.sum(),\n",
    "                        'count_true': (values == 1).sum() if values.isin([0, 1]).all() else values.sum(),\n",
    "                        'percentage': (values == 1).mean() * 100 if values.isin([0, 1]).all() else values.mean() * 100\n",
    "                    }\n",
    "                else:\n",
    "                    label_analysis[col] = {'mean': 0, 'sum': 0, 'count_true': 0, 'percentage': 0}\n",
    "        \n",
    "        analysis_results[label] = label_analysis\n",
    "    \n",
    "    return analysis_results, numeric_metadata\n",
    "\n",
    "# Analyze metadata patterns\n",
    "if enhanced_df is not None:\n",
    "    analysis_results, numeric_metadata = analyze_metadata_patterns(enhanced_df)\n",
    "    \n",
    "    if analysis_results:\n",
    "        print(\"‚úÖ Metadata pattern analysis complete\")\n",
    "        print(f\"üìä Analyzed {len(numeric_metadata)} metadata features across {len(analysis_results)} labels\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No metadata analysis results\")\n",
    "else:\n",
    "    analysis_results, numeric_metadata = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4581fe",
   "metadata": {},
   "source": [
    "## Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e670e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_analysis_plots(enhanced_df, analysis_results, numeric_metadata):\n",
    "    \"\"\"Create visualization of metadata patterns\"\"\"\n",
    "    \n",
    "    if not analysis_results or not numeric_metadata:\n",
    "        print(\"‚ùå No data to plot\")\n",
    "        return None\n",
    "    \n",
    "    # Filter to most important metadata columns for plotting\n",
    "    important_cols = [col for col in numeric_metadata if any(keyword in col.lower() for keyword in \n",
    "                     ['misleading', 'trustworthy', 'factual', 'harmful', 'believable'])]\n",
    "    \n",
    "    if len(important_cols) > 12:\n",
    "        important_cols = important_cols[:12]  # Limit for readability\n",
    "    \n",
    "    n_cols = min(4, len(important_cols))\n",
    "    n_rows = (len(important_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    labels = list(analysis_results.keys())\n",
    "    \n",
    "    for i, col in enumerate(important_cols):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get percentages for each label\n",
    "        percentages = []\n",
    "        for label in labels:\n",
    "            if col in analysis_results[label]:\n",
    "                percentages.append(analysis_results[label][col]['percentage'])\n",
    "            else:\n",
    "                percentages.append(0)\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar(labels, percentages, alpha=0.7)\n",
    "        ax.set_title(col.replace('meta_', '').replace('_', ' ').title())\n",
    "        ax.set_ylabel('Percentage (%)')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, pct in zip(bars, percentages):\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                        f'{pct:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(important_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = f\"{CONFIG['output_dir']}/plots/metadata_analysis_by_label.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return plot_path\n",
    "\n",
    "def create_coverage_analysis_plot(enhanced_df, matches_df):\n",
    "    \"\"\"Create plot showing matching coverage\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Overall coverage\n",
    "    coverage_data = {\n",
    "        'With Metadata': enhanced_df['has_metadata'].sum(),\n",
    "        'Without Metadata': (~enhanced_df['has_metadata']).sum()\n",
    "    }\n",
    "    \n",
    "    colors = ['lightgreen', 'lightcoral']\n",
    "    wedges, texts, autotexts = ax1.pie(coverage_data.values(), labels=coverage_data.keys(), \n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax1.set_title('Metadata Coverage')\n",
    "    \n",
    "    # Plot 2: Coverage by label\n",
    "    coverage_by_label = enhanced_df.groupby('label')['has_metadata'].agg(['sum', 'count'])\n",
    "    coverage_by_label['percentage'] = coverage_by_label['sum'] / coverage_by_label['count'] * 100\n",
    "    \n",
    "    bars = ax2.bar(coverage_by_label.index, coverage_by_label['percentage'], alpha=0.7)\n",
    "    ax2.set_title('Metadata Coverage by Label')\n",
    "    ax2.set_ylabel('Coverage Percentage (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, row in zip(bars, coverage_by_label.itertuples()):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{row.sum}/{row.count}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = f\"{CONFIG['output_dir']}/plots/metadata_coverage_analysis.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return plot_path\n",
    "\n",
    "# Create visualizations\n",
    "if enhanced_df is not None and analysis_results:\n",
    "    print(\"\\nüìä Creating visualizations...\")\n",
    "    \n",
    "    # Coverage analysis plot\n",
    "    coverage_plot = create_coverage_analysis_plot(enhanced_df, matches_df)\n",
    "    if coverage_plot:\n",
    "        print(f\"üìä Coverage analysis plot saved: {coverage_plot}\")\n",
    "    \n",
    "    # Metadata analysis plot\n",
    "    metadata_plot = create_metadata_analysis_plots(enhanced_df, analysis_results, numeric_metadata)\n",
    "    if metadata_plot:\n",
    "        print(f\"üìä Metadata analysis plot saved: {metadata_plot}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping visualizations - no enhanced dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298dcadd",
   "metadata": {},
   "source": [
    "## Generate Enhancement Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhancement_report(enhanced_df, matches_df, analysis_results, numeric_metadata):\n",
    "    \"\"\"Create detailed report of the enhancement process\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# üîó Enhanced Dataset Creation Report\")\n",
    "    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Dataset Summary\n",
    "    report.append(\"## üìä Dataset Enhancement Summary\")\n",
    "    original_count = len(enhanced_df)\n",
    "    with_metadata_count = enhanced_df['has_metadata'].sum() if enhanced_df is not None else 0\n",
    "    coverage_pct = with_metadata_count / original_count * 100 if original_count > 0 else 0\n",
    "    \n",
    "    report.append(f\"- **Original dataset size**: {original_count}\")\n",
    "    report.append(f\"- **Rows with metadata**: {with_metadata_count}\")\n",
    "    report.append(f\"- **Coverage**: {coverage_pct:.1f}%\")\n",
    "    report.append(f\"- **Metadata columns added**: {len(numeric_metadata) if numeric_metadata else 0}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Matching Results\n",
    "    if matches_df is not None and not matches_df.empty:\n",
    "        report.append(\"## üîç Matching Results\")\n",
    "        match_methods = matches_df['match_method'].value_counts()\n",
    "        \n",
    "        for method, count in match_methods.items():\n",
    "            report.append(f\"- **{method}**: {count} matches\")\n",
    "        \n",
    "        avg_score = matches_df['match_score'].mean()\n",
    "        report.append(f\"- **Average match score**: {avg_score:.3f}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Coverage by Label\n",
    "    if enhanced_df is not None:\n",
    "        report.append(\"## üè∑Ô∏è Coverage by Label\")\n",
    "        coverage_by_label = enhanced_df.groupby('label')['has_metadata'].agg(['sum', 'count'])\n",
    "        coverage_by_label['percentage'] = coverage_by_label['sum'] / coverage_by_label['count'] * 100\n",
    "        \n",
    "        for label in coverage_by_label.index:\n",
    "            row = coverage_by_label.loc[label]\n",
    "            report.append(f\"- **{label}**: {row['sum']}/{row['count']} ({row['percentage']:.1f}%)\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Metadata Analysis\n",
    "    if analysis_results and numeric_metadata:\n",
    "        report.append(\"## üìà Metadata Insights by Label\")\n",
    "        \n",
    "        # Focus on most discriminative metadata\n",
    "        key_metadata = [col for col in numeric_metadata if any(keyword in col.lower() for keyword in \n",
    "                       ['misleading', 'trustworthy', 'factual', 'unverified', 'harmful'])]\n",
    "        \n",
    "        for col in key_metadata[:10]:  # Top 10 most relevant\n",
    "            col_name = col.replace('meta_', '').replace('_', ' ').title()\n",
    "            report.append(f\"### {col_name}\")\n",
    "            \n",
    "            for label, data in analysis_results.items():\n",
    "                if col in data:\n",
    "                    pct = data[col]['percentage']\n",
    "                    count = data[col]['count_true']\n",
    "                    total = data['count']\n",
    "                    report.append(f\"- **{label}**: {pct:.1f}% ({count}/{total})\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Feature Importance for Each Label\n",
    "    if analysis_results:\n",
    "        report.append(\"## üéØ Key Metadata Features by Label\")\n",
    "        \n",
    "        for label, data in analysis_results.items():\n",
    "            report.append(f\"### {label}\")\n",
    "            \n",
    "            # Find metadata with highest percentages for this label\n",
    "            label_features = []\n",
    "            for col in numeric_metadata:\n",
    "                if col in data and isinstance(data[col], dict):\n",
    "                    pct = data[col]['percentage']\n",
    "                    if pct > 20:  # Only show features with >20% presence\n",
    "                        label_features.append((col.replace('meta_', ''), pct))\n",
    "            \n",
    "            # Sort by percentage\n",
    "            label_features.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            if label_features:\n",
    "                report.append(\"**High-signal metadata features:**\")\n",
    "                for feature, pct in label_features[:5]:  # Top 5\n",
    "                    report.append(f\"- {feature.replace('_', ' ').title()}: {pct:.1f}%\")\n",
    "            else:\n",
    "                report.append(\"**No strongly predictive metadata features found**\")\n",
    "            \n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"## üí° Recommendations\")\n",
    "    \n",
    "    if coverage_pct >= 80:\n",
    "        report.append(\"1. **Excellent coverage** - metadata is available for most samples\")\n",
    "    elif coverage_pct >= 50:\n",
    "        report.append(\"1. **Good coverage** - metadata available for majority of samples\")\n",
    "    elif coverage_pct >= 20:\n",
    "        report.append(\"1. **Moderate coverage** - consider improving matching algorithms\")\n",
    "    else:\n",
    "        report.append(\"1. **Poor coverage** - may need different matching strategy\")\n",
    "    \n",
    "    if analysis_results:\n",
    "        # Find most discriminative features\n",
    "        discriminative_features = []\n",
    "        for col in numeric_metadata:\n",
    "            percentages = []\n",
    "            for label_data in analysis_results.values():\n",
    "                if col in label_data and isinstance(label_data[col], dict):\n",
    "                    percentages.append(label_data[col]['percentage'])\n",
    "            \n",
    "            if percentages and len(percentages) > 1:\n",
    "                range_pct = max(percentages) - min(percentages)\n",
    "                if range_pct > 30:  # High variance across labels\n",
    "                    discriminative_features.append((col.replace('meta_', ''), range_pct))\n",
    "        \n",
    "        discriminative_features.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if discriminative_features:\n",
    "            report.append(\"2. **Most discriminative metadata features**:\")\n",
    "            for feature, variance in discriminative_features[:5]:\n",
    "                report.append(f\"   - {feature.replace('_', ' ').title()} (variance: {variance:.1f}%)\")\n",
    "        \n",
    "        report.append(\"3. **Use enhanced dataset for training** - metadata should improve classification\")\n",
    "    \n",
    "    report.append(\"4. **Combine with original text features** - metadata complements text analysis\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(\"*Report generated by Enhanced Dataset Creation*\")\n",
    "    \n",
    "    # Save report\n",
    "    report_path = f\"{CONFIG['output_dir']}/reports/enhancement_report.md\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# Generate the enhancement report\n",
    "if enhanced_df is not None:\n",
    "    print(\"\\nüìù Generating enhancement report...\")\n",
    "    report_path = create_enhancement_report(enhanced_df, matches_df, analysis_results, numeric_metadata)\n",
    "    if report_path:\n",
    "        print(f\"üìù Enhancement report saved: {report_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot generate report - no enhanced dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337f852",
   "metadata": {},
   "source": [
    "## Prepare Training-Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b35166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_ready_dataset(enhanced_df, output_path=None):\n",
    "    \"\"\"Prepare the enhanced dataset for training with proper feature encoding\"\"\"\n",
    "    \n",
    "    if enhanced_df is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîß Preparing training-ready dataset...\")\n",
    "    \n",
    "    # Create a copy for training\n",
    "    training_df = enhanced_df.copy()\n",
    "    \n",
    "    # Fill missing metadata with 0 (assumes binary features)\n",
    "    metadata_cols = [col for col in training_df.columns if col.startswith('meta_')]\n",
    "    \n",
    "    for col in metadata_cols:\n",
    "        if training_df[col].dtype in ['object', 'bool']:\n",
    "            # Convert boolean/object to numeric\n",
    "            training_df[col] = pd.to_numeric(training_df[col], errors='coerce')\n",
    "        \n",
    "        # Fill missing values with 0\n",
    "        training_df[col] = training_df[col].fillna(0)\n",
    "    \n",
    "    # Clean label column\n",
    "    training_df['label'] = training_df['label'].astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Remove excluded labels\n",
    "    excluded_labels = [\"irrelevant\", \"probative\"]\n",
    "    training_df = training_df[~training_df[\"label\"].isin(excluded_labels)]\n",
    "    training_df = training_df.dropna(subset=[\"label\"])\n",
    "    \n",
    "    print(f\"‚úÖ Training-ready dataset: {len(training_df)} samples\")\n",
    "    print(f\"üè∑Ô∏è Labels: {training_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"üîß Metadata features: {len(metadata_cols)}\")\n",
    "    \n",
    "    if output_path is None:\n",
    "        output_path = f\"{CONFIG['output_dir']}/training_ready_dataset.csv\"\n",
    "    \n",
    "    training_df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Saved training-ready dataset to: {output_path}\")\n",
    "    \n",
    "    return training_df\n",
    "\n",
    "# Create training-ready dataset\n",
    "if enhanced_df is not None:\n",
    "    training_ready = create_training_ready_dataset(enhanced_df)\n",
    "else:\n",
    "    training_ready = None\n",
    "    print(\"‚ö†Ô∏è Cannot create training-ready dataset - no enhanced dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597e65d",
   "metadata": {},
   "source": [
    "## Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset and matching results\n",
    "if enhanced_df is not None:\n",
    "    print(\"\\nüíæ Saving all results...\")\n",
    "    \n",
    "    # Save enhanced dataset\n",
    "    enhanced_dataset_path = f\"{CONFIG['output_dir']}/enhanced_dataset.csv\"\n",
    "    enhanced_df.to_csv(enhanced_dataset_path, index=False)\n",
    "    print(f\"üìä Enhanced dataset saved: {enhanced_dataset_path}\")\n",
    "    \n",
    "    # Save matches for inspection\n",
    "    if not matches_df.empty:\n",
    "        matches_path = f\"{CONFIG['output_dir']}/matching_results.csv\"\n",
    "        matches_df.to_csv(matches_path, index=False)\n",
    "        print(f\"üîó Matching results saved: {matches_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ ENHANCED DATASET CREATION COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"üìä **Original dataset**: {len(rel_typ_df)} samples\")\n",
    "    print(f\"üîó **Successful matches**: {len(matches_df) if not matches_df.empty else 0}\")\n",
    "    print(f\"üìà **Enhanced dataset**: {len(enhanced_df)} samples\")\n",
    "    print(f\"üéØ **Coverage**: {enhanced_df['has_metadata'].sum()}/{len(enhanced_df)} ({enhanced_df['has_metadata'].sum()/len(enhanced_df)*100:.1f}%)\")\n",
    "    \n",
    "    if numeric_metadata:\n",
    "        print(f\"üîß **Metadata features added**: {len(numeric_metadata)}\")\n",
    "        \n",
    "        # Show most promising features\n",
    "        if analysis_results:\n",
    "            print(f\"\\nüéØ **Most promising metadata features**:\")\n",
    "            feature_variances = []\n",
    "            for col in numeric_metadata[:10]:  # Check top 10\n",
    "                percentages = []\n",
    "                for label_data in analysis_results.values():\n",
    "                    if col in label_data and isinstance(label_data[col], dict):\n",
    "                        percentages.append(label_data[col]['percentage'])\n",
    "                \n",
    "                if len(percentages) > 1:\n",
    "                    variance = max(percentages) - min(percentages)\n",
    "                    feature_variances.append((col.replace('meta_', ''), variance))\n",
    "            \n",
    "            feature_variances.sort(key=lambda x: x[1], reverse=True)\n",
    "            for feature, variance in feature_variances[:5]:\n",
    "                print(f\"   - {feature.replace('_', ' ').title()}: {variance:.1f}% variance across labels\")\n",
    "    \n",
    "    print(f\"\\nüìÅ **Files created**:\")\n",
    "    print(f\"   - Enhanced dataset: {enhanced_dataset_path}\")\n",
    "    if not matches_df.empty:\n",
    "        print(f\"   - Matching results: {CONFIG['output_dir']}/matching_results.csv\")\n",
    "    if training_ready is not None:\n",
    "        print(f\"   - Training-ready dataset: {CONFIG['output_dir']}/training_ready_dataset.csv\")\n",
    "    print(f\"   - Enhancement report: {CONFIG['output_dir']}/reports/enhancement_report.md\")\n",
    "    print(f\"   - Analysis plots: {CONFIG['output_dir']}/plots/\")\n",
    "    \n",
    "    print(f\"\\nüöÄ **Dataset ready for enhanced classification experiments**\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No enhanced dataset created - check input files and matching parameters\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
