{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cd3b8d",
   "metadata": {},
   "source": [
    "# Model Confidence and Calibration Analysis\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc14d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"results_dir\": \"model_training_results\",\n",
    "    \"output_dir\": \"confidence_analysis\",\n",
    "    \"confidence_bins\": 10,\n",
    "    \"high_confidence_threshold\": 0.8,\n",
    "    \"low_confidence_threshold\": 0.6,\n",
    "    \"plot_style\": {\n",
    "        \"figsize\": (15, 10),\n",
    "        \"dpi\": 300,\n",
    "        \"style\": \"whitegrid\"\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['output_dir']}/plots\", exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['output_dir']}/reports\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d2a18",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48095880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_predictions(results_dir):\n",
    "    predictions_dir = f\"{results_dir}/predictions\"\n",
    "    if not os.path.exists(predictions_dir):\n",
    "        print(f\"Error: Predictions directory not found at {predictions_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    prediction_files = [f for f in os.listdir(predictions_dir) if f.endswith('_predictions.csv')]\n",
    "    combined = []\n",
    "    for file in prediction_files:\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(predictions_dir, file))\n",
    "            run_name = file.replace('_predictions.csv', '')\n",
    "            parts = run_name.split('_config_')\n",
    "            df['model_name'] = parts[0]\n",
    "            df['run_name'] = run_name\n",
    "            df['config_id'] = parts[1] if len(parts) > 1 else '0'\n",
    "            combined.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {file}: {e}\")\n",
    "\n",
    "    return pd.concat(combined, ignore_index=True) if combined else pd.DataFrame()\n",
    "\n",
    "\n",
    "def load_model_metrics(results_dir):\n",
    "    metrics_dir = f\"{results_dir}/metrics\"\n",
    "    if not os.path.exists(metrics_dir):\n",
    "        return {}\n",
    "    \n",
    "    metrics = {}\n",
    "    files = [f for f in os.listdir(metrics_dir) if f.endswith('.json') and not f.endswith('_error.json')]\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(os.path.join(metrics_dir, file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if 'error' not in data and data.get('accuracy') is not None:\n",
    "                    run_name = data.get('run_name', file.replace('.json', ''))\n",
    "                    metrics[run_name] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Could not parse {file}: {e}\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d52ab7",
   "metadata": {},
   "source": [
    "## Confidence-Accuracy Relationship\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9237bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence_accuracy_relationship(df):\n",
    "    results = {}\n",
    "\n",
    "    correlation = df['confidence'].corr(df['correct'].astype(int))\n",
    "    results['overall_correlation'] = correlation\n",
    "\n",
    "    correct = df[df['correct']]['confidence']\n",
    "    incorrect = df[~df['correct']]['confidence']\n",
    "\n",
    "    results['confidence_stats'] = {\n",
    "        'correct_predictions': {\n",
    "            'mean': correct.mean(),\n",
    "            'median': correct.median(),\n",
    "            'std': correct.std(),\n",
    "            'count': len(correct)\n",
    "        },\n",
    "        'incorrect_predictions': {\n",
    "            'mean': incorrect.mean(),\n",
    "            'median': incorrect.median(),\n",
    "            'std': incorrect.std(),\n",
    "            'count': len(incorrect)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    stat, p_val = stats.mannwhitneyu(correct, incorrect, alternative='greater')\n",
    "    results['statistical_test'] = {\n",
    "        'statistic': stat,\n",
    "        'p_value': p_val,\n",
    "        'significant': p_val < 0.05\n",
    "    }\n",
    "\n",
    "    thresholds = {\n",
    "        \"high_confidence\": df[df['confidence'] >= CONFIG['high_confidence_threshold']],\n",
    "        \"medium_confidence\": df[(df['confidence'] >= CONFIG['low_confidence_threshold']) & (df['confidence'] < CONFIG['high_confidence_threshold'])],\n",
    "        \"low_confidence\": df[df['confidence'] < CONFIG['low_confidence_threshold']]\n",
    "    }\n",
    "\n",
    "    results['confidence_thresholds'] = {\n",
    "        name: {\n",
    "            'count': len(group),\n",
    "            'accuracy': group['correct'].mean() if len(group) > 0 else 0,\n",
    "            'percentage_of_total': len(group) / len(df) * 100,\n",
    "            'threshold': CONFIG['high_confidence_threshold'] if name == 'high_confidence' else CONFIG['low_confidence_threshold']\n",
    "        }\n",
    "        for name, group in thresholds.items()\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d91883",
   "metadata": {},
   "source": [
    "## Model Calibration Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a28c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_calibration(df):\n",
    "    output = {}\n",
    "    for model in df['run_name'].unique():\n",
    "        data = df[df['run_name'] == model]\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = data['correct'].astype(int)\n",
    "        y_prob = data['confidence']\n",
    "\n",
    "        try:\n",
    "            prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=CONFIG['confidence_bins'])\n",
    "\n",
    "            ece = 0\n",
    "            bins = np.linspace(0, 1, CONFIG['confidence_bins'] + 1)\n",
    "            for low, high in zip(bins[:-1], bins[1:]):\n",
    "                mask = (y_prob > low) & (y_prob <= high)\n",
    "                if mask.any():\n",
    "                    acc_bin = y_true[mask].mean()\n",
    "                    conf_bin = y_prob[mask].mean()\n",
    "                    ece += np.abs(conf_bin - acc_bin) * mask.mean()\n",
    "\n",
    "            brier = brier_score_loss(y_true, y_prob)\n",
    "\n",
    "            output[model] = {\n",
    "                'prob_true': prob_true,\n",
    "                'prob_pred': prob_pred,\n",
    "                'ece': ece,\n",
    "                'brier_score': brier,\n",
    "                'accuracy': y_true.mean(),\n",
    "                'avg_confidence': y_prob.mean(),\n",
    "                'total_predictions': len(data)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calibrate {model}: {e}\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f5c0c",
   "metadata": {},
   "source": [
    "## Problematic Prediction Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e4ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_problematic_predictions(df, n=10):\n",
    "    errors = df[(df['confidence'] >= CONFIG['high_confidence_threshold']) & (~df['correct'])].nlargest(n, 'confidence')\n",
    "    successes = df[(df['confidence'] <= CONFIG['low_confidence_threshold']) & (df['correct'])].nsmallest(n, 'confidence')\n",
    "    return {\n",
    "        'overconfident_errors': errors,\n",
    "        'underconfident_successes': successes\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8e43e",
   "metadata": {},
   "source": [
    "## Confidence Analysis Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872e21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confidence_distribution_plot(df):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    correct_conf = df[df['correct']]['confidence']\n",
    "    incorrect_conf = df[~df['correct']]['confidence']\n",
    "\n",
    "    # Histogram\n",
    "    axes[0, 0].hist(correct_conf, bins=30, alpha=0.7, label='Correct', color='green', density=True)\n",
    "    axes[0, 0].hist(incorrect_conf, bins=30, alpha=0.7, label='Incorrect', color='red', density=True)\n",
    "    axes[0, 0].set_title(\"Confidence Distribution by Correctness\")\n",
    "    axes[0, 0].set_xlabel(\"Confidence\")\n",
    "    axes[0, 0].set_ylabel(\"Density\")\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Accuracy per bin\n",
    "    df['confidence_bin'] = pd.cut(df['confidence'], bins=CONFIG['confidence_bins'])\n",
    "    bin_data = df.groupby('confidence_bin')['correct'].agg(['mean', 'count'])\n",
    "    bin_centers = [interval.mid for interval in bin_data.index]\n",
    "    \n",
    "    axes[0, 1].bar(range(len(bin_centers)), bin_data['mean'], color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].set_title(\"Accuracy by Confidence Bin\")\n",
    "    axes[0, 1].set_xlabel(\"Confidence Bin\")\n",
    "    axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[0, 1].set_xticks(range(len(bin_centers)))\n",
    "    axes[0, 1].set_xticklabels([f\"{c:.2f}\" for c in bin_centers], rotation=45)\n",
    "\n",
    "    # Calibration curve\n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    axes[1, 0].scatter(bin_centers, bin_data['mean'], color='red')\n",
    "    axes[1, 0].set_title(\"Reliability Diagram\")\n",
    "    axes[1, 0].set_xlabel(\"Predicted Confidence\")\n",
    "    axes[1, 0].set_ylabel(\"Observed Accuracy\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_xlim(0, 1)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "    # Threshold curve\n",
    "    thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "    accs = [df[df['confidence'] >= t]['correct'].mean() if len(df[df['confidence'] >= t]) > 0 else 0 for t in thresholds]\n",
    "    counts = [len(df[df['confidence'] >= t]) for t in thresholds]\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(thresholds, accs, 'b-o', label='Accuracy')\n",
    "    ax2.plot(thresholds, counts, 'r-s', label='Count')\n",
    "\n",
    "    ax.set_xlabel(\"Confidence Threshold\")\n",
    "    ax.set_ylabel(\"Accuracy\", color='b')\n",
    "    ax2.set_ylabel(\"Count\", color='r')\n",
    "    ax.set_title(\"Accuracy and Count by Threshold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = f\"{CONFIG['output_dir']}/plots/confidence_distribution_analysis.png\"\n",
    "    plt.savefig(output_path, dpi=CONFIG['plot_style']['dpi'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a42f5e",
   "metadata": {},
   "source": [
    "## Model Calibration Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f554c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_calibration_comparison(calibration_results, model_metrics=None):\n",
    "    if not calibration_results:\n",
    "        return None\n",
    "\n",
    "    models = list(calibration_results.keys())\n",
    "    sorted_models = sorted(calibration_results.items(), key=lambda x: x[1]['ece'])\n",
    "    n = len(models)\n",
    "    cols = min(3, n)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, (model, data) in enumerate(sorted_models):\n",
    "        ax = axes[idx]\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "        ax.plot(data['prob_pred'], data['prob_true'], 'o-', label=model.split('_config_')[0])\n",
    "        ax.set_title(f\"{model}\\nECE: {data['ece']:.3f}, Brier: {data['brier_score']:.3f}\")\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend()\n",
    "\n",
    "    for idx in range(len(sorted_models), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    path = f\"{CONFIG['output_dir']}/plots/model_calibration_comparison.png\"\n",
    "    plt.savefig(path, dpi=CONFIG['plot_style']['dpi'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    names = [m.split('_config_')[0] for m in models]\n",
    "    eces = [calibration_results[m]['ece'] for m in models]\n",
    "    briers = [calibration_results[m]['brier_score'] for m in models]\n",
    "\n",
    "    ax1.barh(names, eces, color='lightblue')\n",
    "    ax1.set_title(\"Expected Calibration Error (Lower is Better)\")\n",
    "    ax2.barh(names, briers, color='salmon')\n",
    "    ax2.set_title(\"Brier Score (Lower is Better)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    summary_path = f\"{CONFIG['output_dir']}/plots/calibration_metrics_summary.png\"\n",
    "    plt.savefig(summary_path, dpi=CONFIG['plot_style']['dpi'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return path, summary_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb26455",
   "metadata": {},
   "source": [
    "## Per-Model Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c435d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_per_model_confidence_analysis(df):\n",
    "    models = df['run_name'].unique()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Average confidence\n",
    "    group_stats = df.groupby('run_name').agg({'confidence': ['mean', 'std'], 'correct': 'mean'})\n",
    "    names = [m.split('_config_')[0] for m in group_stats.index]\n",
    "    y_pos = np.arange(len(names))\n",
    "\n",
    "    axes[0, 0].barh(y_pos, group_stats[('confidence', 'mean')], \n",
    "                    xerr=group_stats[('confidence', 'std')],\n",
    "                    color='skyblue')\n",
    "    axes[0, 0].set_title(\"Average Confidence\")\n",
    "    axes[0, 0].set_yticks(y_pos)\n",
    "    axes[0, 0].set_yticklabels(names)\n",
    "\n",
    "    # Confidence vs Accuracy\n",
    "    axes[0, 1].scatter(group_stats[('confidence', 'mean')], group_stats[('correct', 'mean')])\n",
    "    axes[0, 1].set_title(\"Confidence vs Accuracy\")\n",
    "    axes[0, 1].set_xlabel(\"Avg Confidence\")\n",
    "    axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    for i, name in enumerate(group_stats.index):\n",
    "        short = name.split('_config_')[0]\n",
    "        x = group_stats.loc[name, ('confidence', 'mean')]\n",
    "        y = group_stats.loc[name, ('correct', 'mean')]\n",
    "        axes[0, 1].annotate(short, (x, y), fontsize=8)\n",
    "\n",
    "    # High confidence accuracy\n",
    "    accs, counts = [], []\n",
    "    for m in models:\n",
    "        subset = df[df['run_name'] == m]\n",
    "        high = subset[subset['confidence'] >= CONFIG['high_confidence_threshold']]\n",
    "        accs.append(high['correct'].mean() if len(high) > 0 else 0)\n",
    "        counts.append(len(high))\n",
    "\n",
    "    axes[1, 0].barh(np.arange(len(models)), accs, color='lightgreen')\n",
    "    axes[1, 0].set_yticks(np.arange(len(models)))\n",
    "    axes[1, 0].set_yticklabels([m.split('_config_')[0] for m in models])\n",
    "    axes[1, 0].set_title(\"High Confidence Accuracy\")\n",
    "\n",
    "    # Confidence distribution (top 5)\n",
    "    top5 = df.groupby('run_name')['correct'].mean().nlargest(5).index\n",
    "    for m in top5:\n",
    "        label = m.split('_config_')[0]\n",
    "        axes[1, 1].hist(df[df['run_name'] == m]['confidence'], bins=20, alpha=0.5, label=label, density=True)\n",
    "    axes[1, 1].set_title(\"Confidence Distribution (Top 5)\")\n",
    "    axes[1, 1].legend(fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    path = f\"{CONFIG['output_dir']}/plots/per_model_confidence_analysis.png\"\n",
    "    plt.savefig(path, dpi=CONFIG['plot_style']['dpi'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b20aa0",
   "metadata": {},
   "source": [
    "## Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confidence_analysis_report(df, confidence_analysis, calibration_results, problematic_examples):\n",
    "    report = []\n",
    "    report.append(\"# MODEL CONFIDENCE AND CALIBRATION ANALYSIS REPORT\")\n",
    "\n",
    "    # Executive Summary\n",
    "    report.append(\"## Executive Summary\")\n",
    "    report.append(f\"- Total Predictions: {len(df):,}\")\n",
    "    report.append(f\"- Unique Models: {df['run_name'].nunique()}\")\n",
    "    report.append(f\"- Overall Accuracy: {df['correct'].mean():.4f}\")\n",
    "    report.append(f\"- Average Confidence: {df['confidence'].mean():.4f}\")\n",
    "    report.append(f\"- Confidence-Accuracy Correlation: {confidence_analysis['overall_correlation']:.4f}\\n\")\n",
    "\n",
    "    # Threshold Analysis\n",
    "    report.append(\"## Confidence Threshold Analysis\")\n",
    "    for key, section in confidence_analysis['confidence_thresholds'].items():\n",
    "        name = key.replace('_', ' ').title()\n",
    "        report.append(f\"### {name}\")\n",
    "        report.append(f\"- Accuracy: {section['accuracy']:.4f}\")\n",
    "        report.append(f\"- Predictions: {section['count']:,} ({section['percentage_of_total']:.1f}%)\\n\")\n",
    "\n",
    "    # Calibration Rankings\n",
    "    if calibration_results:\n",
    "        report.append(\"## Model Calibration Summary (Sorted by ECE)\")\n",
    "        sorted_models = sorted(calibration_results.items(), key=lambda x: x[1]['ece'])\n",
    "        for i, (model, res) in enumerate(sorted_models, 1):\n",
    "            short = model.split('_config_')[0]\n",
    "            report.append(f\"{i}. **{short}**\")\n",
    "            report.append(f\"   - ECE: {res['ece']:.4f}\")\n",
    "            report.append(f\"   - Brier: {res['brier_score']:.4f}\")\n",
    "            report.append(f\"   - Accuracy: {res['accuracy']:.4f}\")\n",
    "            report.append(f\"   - Avg Confidence: {res['avg_confidence']:.4f}\\n\")\n",
    "\n",
    "    # Save report\n",
    "    report_path = f\"{CONFIG['output_dir']}/reports/confidence_analysis_report.md\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(report))\n",
    "\n",
    "    return report_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4fdc2",
   "metadata": {},
   "source": [
    "## Main Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f081388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== CONFIDENCE AND CALIBRATION ANALYSIS ===\")\n",
    "\n",
    "    df = load_all_predictions(CONFIG[\"results_dir\"])\n",
    "    if df.empty:\n",
    "        print(\"No prediction data found.\")\n",
    "        return None, None, None\n",
    "\n",
    "    metrics = load_model_metrics(CONFIG[\"results_dir\"])\n",
    "    confidence_stats = analyze_confidence_accuracy_relationship(df)\n",
    "    calibration = calculate_model_calibration(df)\n",
    "    examples = find_problematic_predictions(df)\n",
    "\n",
    "    # Plots\n",
    "    create_confidence_distribution_plot(df)\n",
    "    create_per_model_confidence_analysis(df)\n",
    "    if calibration:\n",
    "        create_model_calibration_comparison(calibration, metrics)\n",
    "\n",
    "    # Report\n",
    "    create_confidence_analysis_report(df, confidence_stats, calibration, examples)\n",
    "\n",
    "    # Save detailed results\n",
    "    df.to_csv(f\"{CONFIG['output_dir']}/confidence_analysis_results.csv\", index=False)\n",
    "    if calibration:\n",
    "        pd.DataFrame([\n",
    "            {\n",
    "                \"model\": m,\n",
    "                \"ece\": res[\"ece\"],\n",
    "                \"brier_score\": res[\"brier_score\"],\n",
    "                \"accuracy\": res[\"accuracy\"],\n",
    "                \"avg_confidence\": res[\"avg_confidence\"],\n",
    "                \"total_predictions\": res[\"total_predictions\"]\n",
    "            } for m, res in calibration.items()\n",
    "        ]).to_csv(f\"{CONFIG['output_dir']}/model_calibration_summary.csv\", index=False)\n",
    "\n",
    "    return df, confidence_stats, calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88580e0a",
   "metadata": {},
   "source": [
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5cb377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIDENCE AND CALIBRATION ANALYSIS ===\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_results, analysis, calibration = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
