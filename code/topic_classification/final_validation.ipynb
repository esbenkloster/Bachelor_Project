{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92533115",
   "metadata": {},
   "source": [
    "# Fresh Model Validation - Final Performance Assessment\n",
    "\n",
    "This notebook evaluates the optimal model configuration on a completely fresh validation set to assess true generalization performance and avoid overfitting concerns.\n",
    "\n",
    "**Optimal Configuration (from comprehensive experiments):**\n",
    "- Training: Excluded \"other\" class\n",
    "- Input: Combined (tweet + note)  \n",
    "- Confidence Threshold: 0.90\n",
    "- Test Performance: 90.4% committed accuracy, 91.2% coverage\n",
    "\n",
    "## Methodology\n",
    "Evaluates model across multiple confidence thresholds on fresh validation data, creates performance visualizations for thesis, and analyzes classification patterns.\n",
    "\n",
    "## Dependencies and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "VALIDATION_FILE = \"val_set.xlsx\"\n",
    "MODEL_PATH = \"trained_models/final_without_other_combined\"\n",
    "CONFIDENCE_THRESHOLDS = [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "OPTIMAL_THRESHOLD = 0.90\n",
    "MAX_TOKEN_LENGTH = 1024\n",
    "EXPECTED_CLASSES = ['economics', 'health', 'lifestyle', 'politics', 'science', 'sports']\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('validation_results', exist_ok=True)\n",
    "\n",
    "# Set plotting style for thesis\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d517c76",
   "metadata": {},
   "source": [
    "## Data Loading and Model Setup\n",
    "Load validation dataset and trained model for fresh evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8293d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_validation(message):\n",
    "    \"\"\"Simple logging for validation progress\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "    \n",
    "    # Ensure the log file is created in validation_results directory\n",
    "    with open('validation_results/validation_log.txt', 'a') as f:\n",
    "        f.write(f\"[{timestamp}] {message}\\n\")\n",
    "\n",
    "def load_validation_data(validation_file):\n",
    "    \"\"\"Load and preprocess validation dataset\"\"\"\n",
    "    \n",
    "    log_validation(f\"Loading validation data from {validation_file}\")\n",
    "    \n",
    "    # Load data\n",
    "    df_val = pd.read_excel(validation_file)\n",
    "    \n",
    "    # Clean text function\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).strip()\n",
    "        text = ''.join(char for char in text if ord(char) >= 32 or char in '\\t\\n\\r')\n",
    "        return text\n",
    "    \n",
    "    # Preprocess\n",
    "    df_val['label'] = df_val['label'].astype(str).str.lower().str.strip()\n",
    "    df_val['tweet_text'] = df_val['tweet_text'].apply(clean_text)\n",
    "    df_val['note_text'] = df_val['note_text'].apply(clean_text)\n",
    "    \n",
    "    # Remove invalid entries\n",
    "    df_val = df_val.dropna(subset=['label', 'tweet_text', 'note_text'])\n",
    "    df_val = df_val[df_val['label'] != '']\n",
    "    df_val = df_val[(df_val['tweet_text'] != \"\") & (df_val['note_text'] != \"\")]\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    \n",
    "    log_validation(f\"Validation set: {len(df_val)} samples\")\n",
    "    \n",
    "    # Analyze label distribution\n",
    "    label_counts = df_val['label'].value_counts()\n",
    "    log_validation(\"Label distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        log_validation(f\"  {label}: {count} ({count/len(df_val)*100:.1f}%)\")\n",
    "    \n",
    "    return df_val\n",
    "\n",
    "def setup_label_encoding(df_val):\n",
    "    \"\"\"Setup label encoding compatible with trained model\"\"\"\n",
    "    \n",
    "    # Training classes encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(EXPECTED_CLASSES)\n",
    "    num_training_classes = len(EXPECTED_CLASSES)\n",
    "    \n",
    "    # Extended encoder for all validation classes\n",
    "    all_classes = EXPECTED_CLASSES + [label for label in df_val['label'].unique() if label not in EXPECTED_CLASSES]\n",
    "    extended_label_encoder = LabelEncoder()\n",
    "    extended_label_encoder.fit(all_classes)\n",
    "    \n",
    "    # Encode labels\n",
    "    def encode_label(label):\n",
    "        if label in EXPECTED_CLASSES:\n",
    "            return label_encoder.transform([label])[0]\n",
    "        else:\n",
    "            return num_training_classes  # Unknown class ID\n",
    "    \n",
    "    df_val['label_id'] = df_val['label'].apply(encode_label)\n",
    "    df_val['extended_label_id'] = extended_label_encoder.transform(df_val['label'])\n",
    "    \n",
    "    # Analysis\n",
    "    known_mask = df_val['label'].isin(EXPECTED_CLASSES)\n",
    "    known_count = known_mask.sum()\n",
    "    unknown_count = len(df_val) - known_count\n",
    "    \n",
    "    log_validation(f\"Known classes: {known_count} ({known_count/len(df_val)*100:.1f}%)\")\n",
    "    log_validation(f\"Unknown/Other: {unknown_count} ({unknown_count/len(df_val)*100:.1f}%)\")\n",
    "    \n",
    "    return df_val, label_encoder, extended_label_encoder, num_training_classes\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    \"\"\"Load trained model and tokenizer\"\"\"\n",
    "    \n",
    "    log_validation(f\"Loading model from {model_path}\")\n",
    "    \n",
    "    model = BartForSequenceClassification.from_pretrained(model_path)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    log_validation(f\"Model loaded: {model.config.num_labels} output classes\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce5ab5",
   "metadata": {},
   "source": [
    "## Model Evaluation Framework\n",
    "Evaluate model performance across confidence thresholds to find optimal balance and assess generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_across_thresholds(model, tokenizer, df_val, num_training_classes):\n",
    "    \"\"\"Evaluate model across all confidence thresholds\"\"\"\n",
    "    \n",
    "    log_validation(\"Creating tokenized validation dataset...\")\n",
    "    \n",
    "    # Create dataset\n",
    "    val_dataset = Dataset.from_pandas(\n",
    "        df_val[['tweet_text', 'note_text', 'label_id']].rename(columns={'label_id': 'labels'})\n",
    "    )\n",
    "    \n",
    "    # Tokenize (combined mode)\n",
    "    def tokenize_function(batch):\n",
    "        return tokenizer(\n",
    "            batch['tweet_text'], \n",
    "            batch['note_text'], \n",
    "            truncation=\"longest_first\",\n",
    "            padding=\"max_length\", \n",
    "            max_length=MAX_TOKEN_LENGTH\n",
    "        )\n",
    "    \n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
    "    val_dataset = val_dataset.remove_columns(['tweet_text', 'note_text'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    # Get model predictions\n",
    "    log_validation(\"Running model inference...\")\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    all_logits = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            true_labels_batch = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_true_labels.append(true_labels_batch.numpy())\n",
    "    \n",
    "    # Process predictions\n",
    "    logits = np.vstack(all_logits)\n",
    "    true_labels = np.concatenate(all_true_labels)\n",
    "    probabilities = softmax(logits, axis=1)\n",
    "    raw_predictions = np.argmax(logits, axis=1)\n",
    "    max_probabilities = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Evaluate each threshold\n",
    "    log_validation(\"Evaluating confidence thresholds...\")\n",
    "    \n",
    "    results = []\n",
    "    unknown_label_id = num_training_classes\n",
    "    \n",
    "    for threshold in CONFIDENCE_THRESHOLDS:\n",
    "        # Apply confidence threshold\n",
    "        final_predictions = []\n",
    "        for pred, conf in zip(raw_predictions, max_probabilities):\n",
    "            if conf > threshold:\n",
    "                final_predictions.append(pred)\n",
    "            else:\n",
    "                final_predictions.append(unknown_label_id)\n",
    "        \n",
    "        final_predictions = np.array(final_predictions)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        known_mask = true_labels < num_training_classes\n",
    "        unknown_mask = true_labels >= num_training_classes\n",
    "        \n",
    "        known_count = known_mask.sum()\n",
    "        unknown_count = unknown_mask.sum()\n",
    "        \n",
    "        # Overall accuracy (only on known ground truth)\n",
    "        if known_count > 0:\n",
    "            known_predictions = final_predictions[known_mask]\n",
    "            known_true = true_labels[known_mask]\n",
    "            overall_accuracy = (known_predictions == known_true).mean()\n",
    "        else:\n",
    "            overall_accuracy = 0.0\n",
    "        \n",
    "        # Committed accuracy (excluding model's unknown predictions, only on known ground truth)\n",
    "        committed_mask = (final_predictions != unknown_label_id) & known_mask\n",
    "        if committed_mask.sum() > 0:\n",
    "            committed_predictions = final_predictions[committed_mask]\n",
    "            committed_true = true_labels[committed_mask]\n",
    "            committed_accuracy = (committed_predictions == committed_true).mean()\n",
    "        else:\n",
    "            committed_accuracy = 0.0\n",
    "        \n",
    "        # Coverage (percentage of known samples with confident predictions)\n",
    "        if known_count > 0:\n",
    "            confident_on_known = ((final_predictions != unknown_label_id) & known_mask).sum()\n",
    "            coverage = (confident_on_known / known_count) * 100\n",
    "        else:\n",
    "            coverage = 0.0\n",
    "        \n",
    "        # Unknown detection rate\n",
    "        if unknown_count > 0:\n",
    "            unknown_detected = ((final_predictions == unknown_label_id) & unknown_mask).sum()\n",
    "            unknown_detection_rate = (unknown_detected / unknown_count) * 100\n",
    "        else:\n",
    "            unknown_detection_rate = 0.0\n",
    "        \n",
    "        results.append({\n",
    "            'confidence_threshold': threshold,\n",
    "            'overall_accuracy': overall_accuracy,\n",
    "            'committed_accuracy': committed_accuracy,\n",
    "            'coverage': coverage,\n",
    "            'unknown_detection_rate': unknown_detection_rate,\n",
    "            'total_samples': len(final_predictions),\n",
    "            'known_samples': known_count,\n",
    "            'unknown_samples': unknown_count\n",
    "        })\n",
    "        \n",
    "        log_validation(f\"Threshold {threshold}: Acc={committed_accuracy:.3f}, Cov={coverage:.1f}%\")\n",
    "    \n",
    "    # Store raw results for confusion matrix\n",
    "    optimal_idx = [i for i, r in enumerate(results) if r['confidence_threshold'] == OPTIMAL_THRESHOLD][0]\n",
    "    optimal_predictions = []\n",
    "    for pred, conf in zip(raw_predictions, max_probabilities):\n",
    "        if conf > OPTIMAL_THRESHOLD:\n",
    "            optimal_predictions.append(pred)\n",
    "        else:\n",
    "            optimal_predictions.append(unknown_label_id)\n",
    "    \n",
    "    return results, {\n",
    "        'true_labels': true_labels,\n",
    "        'predicted_labels': np.array(optimal_predictions),\n",
    "        'raw_predictions': raw_predictions,\n",
    "        'max_probabilities': max_probabilities\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c188e",
   "metadata": {},
   "source": [
    "## Thesis Visualizations\n",
    "Create visualizations for the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1279d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thesis_threshold_plot(results):\n",
    "    \"\"\"Create threshold analysis plot for thesis (Figure: Validation set analysis)\"\"\"\n",
    "    \n",
    "    log_validation(\"Creating thesis threshold analysis plot...\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create dual-axis plot matching thesis style\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Committed accuracy on left axis\n",
    "    color1 = 'tab:blue'\n",
    "    ax1.set_xlabel('Confidence Threshold', fontsize=14)\n",
    "    ax1.set_ylabel('Committed Accuracy', color=color1, fontsize=14)\n",
    "    line1 = ax1.plot(results_df['confidence_threshold'], results_df['committed_accuracy'], \n",
    "                     'o-', color=color1, linewidth=3, markersize=8, label='Committed Accuracy')\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Coverage on right axis\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = 'tab:orange'\n",
    "    ax2.set_ylabel('Coverage (%)', color=color2, fontsize=14)\n",
    "    line2 = ax2.plot(results_df['confidence_threshold'], results_df['coverage'], \n",
    "                     's-', color=color2, linewidth=3, markersize=8, label='Coverage')\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    \n",
    "    # Highlight optimal threshold\n",
    "    ax1.axvline(x=OPTIMAL_THRESHOLD, color='red', linestyle='--', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Add legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='center left', fontsize=12)\n",
    "    \n",
    "    # Clean title\n",
    "    plt.title('Validation Set: Committed Accuracy and Coverage vs Confidence Threshold', \n",
    "              fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with filename for thesis reference\n",
    "    plt.savefig('validation_results/val_threshold_dual_axis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    log_validation(\"✅ Threshold analysis plot saved: validation_results/val_threshold_dual_axis.png\")\n",
    "\n",
    "def create_thesis_confusion_matrices(df_val, prediction_data, label_encoder, extended_label_encoder, num_training_classes):\n",
    "    \"\"\"Create both enhanced and traditional confusion matrices\"\"\"\n",
    "    \n",
    "    log_validation(\"Creating confusion matrices...\")\n",
    "    \n",
    "    true_labels = prediction_data['true_labels']\n",
    "    predicted_labels = prediction_data['predicted_labels']\n",
    "    extended_true_labels = df_val['extended_label_id'].values\n",
    "    \n",
    "    # ===== 1. TRADITIONAL CONFUSION MATRIX (6x6, known classes only) =====\n",
    "    log_validation(\"Creating traditional confusion matrix...\")\n",
    "    \n",
    "    known_mask = true_labels < num_training_classes\n",
    "    confident_mask = predicted_labels < num_training_classes\n",
    "    analysis_mask = known_mask & confident_mask\n",
    "    \n",
    "    if analysis_mask.sum() > 0:\n",
    "        filtered_true = true_labels[analysis_mask]\n",
    "        filtered_pred = predicted_labels[analysis_mask]\n",
    "        \n",
    "        # Create traditional confusion matrix\n",
    "        cm = confusion_matrix(filtered_true, filtered_pred, labels=range(num_training_classes))\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "        disp.plot(ax=ax, cmap='Blues', values_format='d', colorbar=True)\n",
    "        \n",
    "        ax.set_title('Traditional Confusion Matrix\\n(Known Classes, Confident Predictions Only)', \n",
    "                    fontsize=14, pad=20)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig('validation_results/traditional_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        log_validation(\"✅ Traditional confusion matrix saved\")\n",
    "        \n",
    "        # Generate and save classification report\n",
    "        report = classification_report(\n",
    "            filtered_true, filtered_pred, \n",
    "            target_names=label_encoder.classes_, \n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Save classification report as CSV\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv('validation_results/classification_report.csv')\n",
    "        \n",
    "        log_validation(\"✅ Classification report saved\")\n",
    "        \n",
    "        # Print F1 scores for reference\n",
    "        log_validation(\"Classification performance:\")\n",
    "        for class_name in label_encoder.classes_:\n",
    "            if class_name in report:\n",
    "                f1 = report[class_name]['f1-score']\n",
    "                log_validation(f\"  {class_name}: F1 = {f1:.2f}\")\n",
    "    else:\n",
    "        log_validation(\"⚠️ No confident predictions on known classes\")\n",
    "        report = None\n",
    "    \n",
    "    # ===== 2. ENHANCED CONFUSION MATRIX (All classes + Low Confidence) =====\n",
    "    log_validation(\"Creating enhanced confusion matrix...\")\n",
    "    \n",
    "    # Get all unique true labels that actually appear in validation set\n",
    "    unique_true_labels = sorted(df_val['extended_label_id'].unique())\n",
    "    all_true_class_names = [extended_label_encoder.classes_[i] for i in unique_true_labels]\n",
    "    \n",
    "    # Separate \"other\" from other classes and put it at the end for Y-axis\n",
    "    true_class_names = []\n",
    "    other_true_class = None\n",
    "    \n",
    "    for class_name in all_true_class_names:\n",
    "        if class_name.lower() == 'other':\n",
    "            other_true_class = class_name\n",
    "        else:\n",
    "            true_class_names.append(class_name)\n",
    "    \n",
    "    # Add \"other\" at the end if it exists\n",
    "    if other_true_class:\n",
    "        true_class_names.append(other_true_class)\n",
    "    \n",
    "    # Create predicted class names: same as true labels but replace \"other\" with \"low confidence\"\n",
    "    # and ensure \"low confidence\" is at the end\n",
    "    pred_class_names = []\n",
    "    \n",
    "    for class_name in true_class_names:\n",
    "        if class_name.lower() == 'other':\n",
    "            # Don't add \"low confidence\" here, we'll add it at the end\n",
    "            continue\n",
    "        else:\n",
    "            pred_class_names.append(class_name)\n",
    "    \n",
    "    # Always add \"low confidence\" at the end for X-axis\n",
    "    pred_class_names.append('Low Confidence')\n",
    "    \n",
    "    # Find the index for \"low confidence\" in predicted classes (always last)\n",
    "    low_confidence_idx = len(pred_class_names) - 1\n",
    "    \n",
    "    # Map predictions to the new class system\n",
    "    mapped_predicted_labels = []\n",
    "    \n",
    "    for i, (pred, true_ext) in enumerate(zip(predicted_labels, extended_true_labels)):\n",
    "        if pred == num_training_classes:  # Model abstained/predicted unknown\n",
    "            mapped_predicted_labels.append(low_confidence_idx)\n",
    "        elif pred < num_training_classes:  # Model made confident prediction on known class\n",
    "            # Find which class this prediction corresponds to in our validation set\n",
    "            predicted_class_name = label_encoder.classes_[pred]\n",
    "            try:\n",
    "                # Find the index of this class in our reordered predicted class names\n",
    "                if predicted_class_name in pred_class_names[:-1]:  # Exclude \"low confidence\"\n",
    "                    mapped_pred_idx = pred_class_names.index(predicted_class_name)\n",
    "                    mapped_predicted_labels.append(mapped_pred_idx)\n",
    "                else:\n",
    "                    # If predicted class not in validation set, map to low confidence\n",
    "                    mapped_predicted_labels.append(low_confidence_idx)\n",
    "            except ValueError:\n",
    "                # If predicted class not in validation set, map to low confidence\n",
    "                mapped_predicted_labels.append(low_confidence_idx)\n",
    "        else:\n",
    "            mapped_predicted_labels.append(low_confidence_idx)\n",
    "    \n",
    "    mapped_predicted_labels = np.array(mapped_predicted_labels)\n",
    "    \n",
    "    # Map true labels to indices in our reordered class list\n",
    "    mapped_true_labels = []\n",
    "    for true_ext in extended_true_labels:\n",
    "        true_class_name = extended_label_encoder.classes_[true_ext]\n",
    "        try:\n",
    "            mapped_true_idx = true_class_names.index(true_class_name)\n",
    "            mapped_true_labels.append(mapped_true_idx)\n",
    "        except ValueError:\n",
    "            # This shouldn't happen, but just in case\n",
    "            mapped_true_labels.append(0)\n",
    "    \n",
    "    mapped_true_labels = np.array(mapped_true_labels)\n",
    "    \n",
    "    # Create enhanced confusion matrix\n",
    "    n_true_classes = len(true_class_names)\n",
    "    n_pred_classes = len(pred_class_names)\n",
    "    cm_enhanced = np.zeros((n_true_classes, n_pred_classes), dtype=int)\n",
    "    \n",
    "    for true_idx, pred_idx in zip(mapped_true_labels, mapped_predicted_labels):\n",
    "        if true_idx < n_true_classes and pred_idx < n_pred_classes:\n",
    "            cm_enhanced[true_idx, pred_idx] += 1\n",
    "    \n",
    "    # Plot enhanced confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    im = ax.imshow(cm_enhanced, interpolation='nearest', cmap='Blues')\n",
    "    ax.set_title('Enhanced Confusion Matrix\\n(All Classes Including Low Confidence)', fontsize=14, pad=20)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.ax.tick_params(labelsize=11)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    true_tick_marks = np.arange(len(true_class_names))\n",
    "    pred_tick_marks = np.arange(len(pred_class_names))\n",
    "    \n",
    "    ax.set_xticks(pred_tick_marks)\n",
    "    ax.set_yticks(true_tick_marks)\n",
    "    ax.set_xticklabels(pred_class_names, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(true_class_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm_enhanced.max() / 2. if cm_enhanced.max() > 0 else 0.5\n",
    "    for i in range(cm_enhanced.shape[0]):\n",
    "        for j in range(cm_enhanced.shape[1]):\n",
    "            ax.text(j, i, format(cm_enhanced[i, j], 'd'),\n",
    "                   ha=\"center\", va=\"center\", fontsize=10,\n",
    "                   color=\"white\" if cm_enhanced[i, j] > thresh else \"black\")\n",
    "    \n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save enhanced confusion matrix as val_confusion_matrix.png (to match expected files)\n",
    "    plt.savefig('validation_results/val_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    log_validation(\"✅ Enhanced confusion matrix saved as val_confusion_matrix.png\")\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810aeb4",
   "metadata": {},
   "source": [
    "## Performance Summary and Results\n",
    "Generate comprehensive results summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_additional_visualizations(results):\n",
    "    \"\"\"Create additional useful visualizations\"\"\"\n",
    "    \n",
    "    log_validation(\"Creating additional visualizations...\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Threshold table visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Prepare table data\n",
    "    table_data = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        table_data.append([\n",
    "            f\"{row['confidence_threshold']:.2f}\",\n",
    "            f\"{row['committed_accuracy']:.3f}\",\n",
    "            f\"{row['coverage']:.1f}%\",\n",
    "            f\"{row['unknown_detection_rate']:.1f}%\",\n",
    "            f\"{(row['total_samples'] - row['known_samples'] - row['unknown_samples'] + ((row['predicted_labels'] == row['total_samples']) if 'predicted_labels' in row else 0)):.1f}%\"  # Abstention rate approximation\n",
    "        ])\n",
    "    \n",
    "    # Create table\n",
    "    table = ax.table(cellText=table_data,\n",
    "                    colLabels=['Threshold', 'Committed Acc', 'Coverage', 'Unknown Detection', 'Abstention Rate'],\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    colColours=['lightgray']*5)\n",
    "    \n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Highlight optimal threshold row\n",
    "    optimal_row = results_df[results_df['confidence_threshold'] == OPTIMAL_THRESHOLD].index[0] + 1\n",
    "    for col in range(5):\n",
    "        table[(optimal_row, col)].set_facecolor('#ffcccc')\n",
    "        table[(optimal_row, col)].set_text_props(weight='bold')\n",
    "    \n",
    "    plt.title('Performance Metrics Across Confidence Thresholds', fontsize=16, pad=20)\n",
    "    plt.savefig('validation_results/threshold_table.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    log_validation(\"✅ Threshold table visualization saved\")\n",
    "\n",
    "def extract_interesting_examples(df_val, prediction_data, label_encoder, num_training_classes):\n",
    "    \"\"\"Extract interesting prediction examples for thesis analysis\"\"\"\n",
    "    \n",
    "    log_validation(\"Extracting interesting prediction examples...\")\n",
    "    \n",
    "    # Prepare dataframe with predictions\n",
    "    probs = prediction_data['max_probabilities']\n",
    "    preds = prediction_data['predicted_labels']\n",
    "    raw_preds = prediction_data['raw_predictions']\n",
    "    unknown_id = num_training_classes\n",
    "    \n",
    "    # Add prediction information to dataframe\n",
    "    df_examples = df_val.copy()\n",
    "    df_examples['predicted_label'] = [\n",
    "        label_encoder.classes_[p] if p < unknown_id else 'low confidence'\n",
    "        for p in preds\n",
    "    ]\n",
    "    df_examples['raw_predicted_label'] = [\n",
    "        label_encoder.classes_[rp] if rp < unknown_id else 'unknown'\n",
    "        for rp in raw_preds\n",
    "    ]\n",
    "    df_examples['confidence'] = probs\n",
    "    df_examples['correct_prediction'] = (df_examples['predicted_label'] == df_examples['label'])\n",
    "    \n",
    "    # 1. Low confidence examples (for understanding model uncertainty)\n",
    "    low_conf_df = df_examples.sort_values(by='confidence').head(10)\n",
    "    low_conf_df[['tweet_text', 'note_text', 'label', 'predicted_label', 'confidence']].to_excel(\n",
    "        \"validation_results/low_confidence_examples.xlsx\", index=False\n",
    "    )\n",
    "    \n",
    "    # 2. High confidence misclassifications (concerning errors)\n",
    "    high_conf_misclassified = df_examples[\n",
    "        (df_examples['predicted_label'] != df_examples['label']) &\n",
    "        (df_examples['confidence'] > OPTIMAL_THRESHOLD) &\n",
    "        (df_examples['predicted_label'] != 'low confidence')\n",
    "    ].sort_values(by='confidence', ascending=False)\n",
    "    \n",
    "    if len(high_conf_misclassified) > 0:\n",
    "        high_conf_misclassified[['tweet_text', 'note_text', 'label', 'predicted_label', 'confidence']].to_excel(\n",
    "            \"validation_results/high_conf_misclassifications.xlsx\", index=False\n",
    "        )\n",
    "    \n",
    "    # 3. Lifestyle misclassifications (high-stakes content wrongly classified as lifestyle)\n",
    "    lifestyle_misclass = df_examples[\n",
    "        (df_examples['predicted_label'] == 'lifestyle') &\n",
    "        (df_examples['label'].isin(['politics', 'science', 'health']))\n",
    "    ].sort_values(by='confidence', ascending=False)\n",
    "    \n",
    "    if len(lifestyle_misclass) > 0:\n",
    "        lifestyle_misclass[['tweet_text', 'note_text', 'label', 'predicted_label', 'confidence']].to_excel(\n",
    "            \"validation_results/lifestyle_misclassified_as_high_stakes.xlsx\", index=False\n",
    "        )\n",
    "    \n",
    "    # 4. Economics predictions (for manual verification due to small sample)\n",
    "    economics_predictions = df_examples[\n",
    "        df_examples['predicted_label'] == 'economics'\n",
    "    ].sort_values(by='confidence', ascending=False)\n",
    "    \n",
    "    if len(economics_predictions) > 0:\n",
    "        economics_predictions[['tweet_text', 'note_text', 'label', 'predicted_label', 'confidence']].to_excel(\n",
    "            \"validation_results/economics_predictions.xlsx\", index=False\n",
    "        )\n",
    "    \n",
    "    log_validation(\"✅ Example extraction completed:\")\n",
    "    log_validation(f\"  - Low confidence examples: {len(low_conf_df)}\")\n",
    "    log_validation(f\"  - High confidence misclassifications: {len(high_conf_misclassified)}\")\n",
    "    log_validation(f\"  - Lifestyle misclassifications: {len(lifestyle_misclass)}\")\n",
    "    log_validation(f\"  - Economics predictions: {len(economics_predictions)}\")\n",
    "    \n",
    "    return {\n",
    "        'low_confidence': len(low_conf_df),\n",
    "        'high_conf_errors': len(high_conf_misclassified),\n",
    "        'lifestyle_errors': len(lifestyle_misclass),\n",
    "        'economics_predictions': len(economics_predictions)\n",
    "    }\n",
    "\n",
    "def generate_comprehensive_validation_report(results, df_val, prediction_data, example_stats):\n",
    "    \"\"\"Generate comprehensive validation report for thesis\"\"\"\n",
    "    \n",
    "    log_validation(\"Generating comprehensive validation report...\")\n",
    "    \n",
    "    optimal_results = next(r for r in results if r['confidence_threshold'] == OPTIMAL_THRESHOLD)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# Fresh Model Validation Report\\n\")\n",
    "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    report.append(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Dataset summary\n",
    "    report.append(\"## Validation Dataset Summary\\n\")\n",
    "    report.append(f\"- **Total samples**: {len(df_val)}\\n\")\n",
    "    report.append(f\"- **Known classes**: {optimal_results['known_samples']}\\n\")\n",
    "    report.append(f\"- **Unknown/Other classes**: {optimal_results['unknown_samples']}\\n\\n\")\n",
    "    \n",
    "    # Label distribution\n",
    "    label_counts = df_val['label'].value_counts()\n",
    "    report.append(\"### Label Distribution\\n\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = count / len(df_val) * 100\n",
    "        report.append(f\"- **{label}**: {count} samples ({percentage:.1f}%)\\n\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Model performance at optimal threshold\n",
    "    report.append(f\"## Model Performance (Threshold: {OPTIMAL_THRESHOLD})\\n\")\n",
    "    report.append(f\"- **Overall Accuracy**: {optimal_results['overall_accuracy']:.1%}\\n\")\n",
    "    report.append(f\"- **Committed Accuracy**: {optimal_results['committed_accuracy']:.1%}\\n\")\n",
    "    report.append(f\"- **Coverage**: {optimal_results['coverage']:.1f}%\\n\")\n",
    "    report.append(f\"- **Unknown Detection Rate**: {optimal_results['unknown_detection_rate']:.1f}%\\n\\n\")\n",
    "    \n",
    "    # Comparison with test results\n",
    "    report.append(\"## Comparison with Test Set Results\\n\")\n",
    "    report.append(\"| Metric | Test Set | Validation Set | Difference |\\n\")\n",
    "    report.append(\"|--------|----------|----------------|------------|\\n\")\n",
    "    report.append(f\"| Committed Accuracy | 90.4% | {optimal_results['committed_accuracy']:.1%} | {(optimal_results['committed_accuracy'] - 0.904)*100:+.1f}pp |\\n\")\n",
    "    report.append(f\"| Coverage | 91.2% | {optimal_results['coverage']:.1f}% | {optimal_results['coverage'] - 91.2:+.1f}pp |\\n\")\n",
    "    report.append(f\"| Unknown Detection | 0.0% | {optimal_results['unknown_detection_rate']:.1f}% | {optimal_results['unknown_detection_rate']:+.1f}pp |\\n\\n\")\n",
    "    \n",
    "    # Performance across all thresholds\n",
    "    report.append(\"## Performance Across Confidence Thresholds\\n\")\n",
    "    report.append(\"| Threshold | Committed Acc | Coverage | Unknown Detection |\\n\")\n",
    "    report.append(\"|-----------|---------------|----------|-------------------|\\n\")\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        report.append(f\"| {row['confidence_threshold']:.2f} | {row['committed_accuracy']:.3f} | \"\n",
    "                     f\"{row['coverage']:.1f}% | {row['unknown_detection_rate']:.1f}% |\\n\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Error analysis\n",
    "    report.append(\"## Error Analysis\\n\")\n",
    "    report.append(\"### Extracted Examples for Thesis Analysis\\n\")\n",
    "    report.append(f\"- **Low confidence examples**: {example_stats['low_confidence']} cases extracted\\n\")\n",
    "    report.append(f\"- **High confidence misclassifications**: {example_stats['high_conf_errors']} cases found\\n\")\n",
    "    report.append(f\"- **Lifestyle misclassifications**: {example_stats['lifestyle_errors']} high-stakes content wrongly labeled\\n\")\n",
    "    report.append(f\"- **Economics predictions**: {example_stats['economics_predictions']} cases for manual verification\\n\\n\")\n",
    "    \n",
    "    # Key insights\n",
    "    report.append(\"## Key Insights for Thesis\\n\")\n",
    "    \n",
    "    # Find best performing thresholds\n",
    "    best_accuracy_idx = results_df['committed_accuracy'].idxmax()\n",
    "    best_coverage_idx = results_df['coverage'].idxmax()\n",
    "    \n",
    "    report.append(f\"1. **Minimal Overfitting**: Only {abs((optimal_results['committed_accuracy'] - 0.904)*100):.1f} percentage point drop in committed accuracy\\n\")\n",
    "    report.append(f\"2. **Confidence Calibration**: Model showed {optimal_results['coverage']:.1f}% coverage vs {91.2}% on test set\\n\")\n",
    "    report.append(f\"3. **Unknown Detection**: {optimal_results['unknown_detection_rate']:.1f}% success rate on out-of-domain samples\\n\")\n",
    "    report.append(f\"4. **Optimal Threshold Confirmed**: 0.90 threshold validated on fresh data\\n\\n\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"## Recommendations for Thesis Discussion\\n\")\n",
    "    if optimal_results['committed_accuracy'] >= 0.85:\n",
    "        report.append(\"✅ **Strong Generalization**: Model maintains high performance on unseen data\\n\")\n",
    "    \n",
    "    if example_stats['high_conf_errors'] > 0:\n",
    "        report.append(\"⚠️ **Error Analysis**: Review high-confidence misclassifications for systematic biases\\n\")\n",
    "    \n",
    "    if example_stats['lifestyle_errors'] > 0:\n",
    "        report.append(\"⚠️ **High-Stakes Misclassification**: Examine politics/health content labeled as lifestyle\\n\")\n",
    "    \n",
    "    report.append(\"📊 **Confidence Threshold**: 0.90 provides optimal accuracy-coverage trade-off\\n\\n\")\n",
    "    \n",
    "    # File references\n",
    "    report.append(\"## Generated Files for Analysis\\n\")\n",
    "    report.append(\"### Visualizations\\n\")\n",
    "    report.append(\"- `val_threshold_dual_axis.png`: Threshold performance analysis\\n\")\n",
    "    report.append(\"- `traditional_confusion_matrix.png`: Standard confusion matrix\\n\")\n",
    "    report.append(\"- `val_confusion_matrix.png`: Matrix including low confidence predictions\\n\")\n",
    "    report.append(\"- `threshold_table.png`: Tabular performance summary\\n\\n\")\n",
    "    \n",
    "    report.append(\"### Example Files for Thesis\\n\")\n",
    "    report.append(\"- `low_confidence_examples.xlsx`: Cases where model was uncertain\\n\")\n",
    "    report.append(\"- `high_conf_misclassifications.xlsx`: Confident but wrong predictions\\n\")\n",
    "    report.append(\"- `lifestyle_misclassified_as_high_stakes.xlsx`: High-stakes content mislabeled\\n\")\n",
    "    report.append(\"- `economics_predictions.xlsx`: Economics predictions for verification\\n\")\n",
    "    report.append(\"- `all_predictions_with_examples.xlsx`: Complete prediction dataset\\n\\n\")\n",
    "    \n",
    "    # Save report\n",
    "    with open('validation_results/validation_report.md', 'w') as f:\n",
    "        f.write(''.join(report))\n",
    "    \n",
    "    log_validation(\"✅ Comprehensive validation report generated\")\n",
    "    \n",
    "    return ''.join(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5677002",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Complete validation pipeline: load data → evaluate model → create visualizations → summarize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6c5d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# # Fresh Model Validation - Final Performance Assessment\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Dependencies\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main validation execution\"\"\"\n",
    "    \n",
    "    log_validation(\"=\"*60)\n",
    "    log_validation(\"FRESH VALIDATION OF OPTIMAL MODEL CONFIGURATION\")\n",
    "    log_validation(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Load validation data\n",
    "        df_val = load_validation_data(VALIDATION_FILE)\n",
    "        \n",
    "        # Setup label encoding\n",
    "        df_val, label_encoder, extended_label_encoder, num_training_classes = setup_label_encoding(df_val)\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer = load_model_and_tokenizer(MODEL_PATH)\n",
    "        \n",
    "        # Evaluate across thresholds\n",
    "        results, prediction_data = evaluate_model_across_thresholds(\n",
    "            model, tokenizer, df_val, num_training_classes\n",
    "        )\n",
    "        \n",
    "        # Create thesis visualizations\n",
    "        create_thesis_threshold_plot(results)\n",
    "        \n",
    "        classification_report = create_thesis_confusion_matrices(\n",
    "            df_val, prediction_data, label_encoder, extended_label_encoder, num_training_classes\n",
    "        )\n",
    "        \n",
    "        # Create additional visualizations and examples\n",
    "        create_additional_visualizations(results)\n",
    "\n",
    "        \n",
    "        example_stats = extract_interesting_examples(\n",
    "            df_val, prediction_data, label_encoder, num_training_classes\n",
    "        )\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        validation_report = generate_comprehensive_validation_report(\n",
    "            results, df_val, prediction_data, example_stats\n",
    "        )\n",
    "        \n",
    "        # Generate summary\n",
    "        optimal_results = next(r for r in results if r['confidence_threshold'] == OPTIMAL_THRESHOLD)\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save results in both CSV and Excel formats\n",
    "        results_df.to_csv('validation_results/validation_results.csv', index=False)\n",
    "        results_df.to_excel('validation_results/validation_results.xlsx', index=False)\n",
    "    \n",
    "        \n",
    "        # Print comprehensive results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE VALIDATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Dataset: {len(df_val)} samples ({optimal_results['known_samples']} known, {optimal_results['unknown_samples']} unknown)\")\n",
    "        print(f\"Optimal Threshold: {OPTIMAL_THRESHOLD}\")\n",
    "        print()\n",
    "        print(\"PERFORMANCE METRICS:\")\n",
    "        print(f\"  Overall Accuracy: {optimal_results['overall_accuracy']:.1%}\")\n",
    "        print(f\"  Committed Accuracy: {optimal_results['committed_accuracy']:.1%}\")\n",
    "        print(f\"  Coverage: {optimal_results['coverage']:.1f}%\")\n",
    "        print(f\"  Unknown Detection Rate: {optimal_results['unknown_detection_rate']:.1f}%\")\n",
    "        print()\n",
    "        print(\"EXAMPLE ANALYSIS:\")\n",
    "        print(f\"  Low confidence examples: {example_stats['low_confidence']}\")\n",
    "        print(f\"  High confidence errors: {example_stats['high_conf_errors']}\")\n",
    "        print(f\"  Lifestyle misclassifications: {example_stats['lifestyle_errors']}\")\n",
    "        print(f\"  Economics predictions: {example_stats['economics_predictions']}\")\n",
    "        print()\n",
    "        print(\"FILES GENERATED:\")\n",
    "        print(\"  📊 validation_results/val_threshold_dual_axis.png\")\n",
    "        print(\"  🔍 validation_results/val_confusion_matrix.png\")\n",
    "        print(\"  🔍 validation_results/traditional_confusion_matrix.png\")\n",
    "        print(\"  📈 validation_results/threshold_table.png\")\n",
    "        print(\"  📋 validation_results/classification_report.csv\")\n",
    "        print(\"  📋 validation_results/validation_report.md\")\n",
    "        print(\"  📊 validation_results/validation_results.csv\")\n",
    "        print(\"  📊 validation_results/validation_results.xlsx\")\n",
    "        print(\"  📝 validation_results/validation_log.txt\")\n",
    "        print(\"  📝 Example files:\")\n",
    "        print(\"    - low_confidence_examples.xlsx\")\n",
    "        print(\"    - high_conf_misclassifications.xlsx\")\n",
    "        print(\"    - lifestyle_misclassified_as_high_stakes.xlsx\")\n",
    "        print(\"    - economics_predictions.xlsx\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        log_validation(\"✅ Fresh validation completed successfully!\")\n",
    "        \n",
    "        return results, optimal_results, classification_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_validation(f\"❌ Validation failed: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
