{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8e0924",
   "metadata": {},
   "source": [
    "# BSC Dataset Labeling\n",
    "\n",
    "This script applies the optimal BART model configuration to label the complete BSC dataset.\n",
    "The model uses the combined approach (tweet + note) without the \"other\" class, \n",
    "with a confidence threshold of 0.9, achieving 90.4% committed accuracy and 91.2% coverage.\n",
    "\n",
    "## Configuration\n",
    "- **Model**: Combined input (tweet + note)\n",
    "- **Classes**: economics, health, lifestyle, politics, science, sports\n",
    "- **Confidence threshold**: 0.9\n",
    "- **Expected performance**: 90.4% accuracy, 91.2% coverage\n",
    "\n",
    "## Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8786fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115ffc1",
   "metadata": {},
   "source": [
    "## BSC Dataset Labeler Class\n",
    "Core labeling functionality that loads the optimal BART model and applies it to the dataset with confidence thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSCDatasetLabeler:\n",
    "    \"\"\"\n",
    "    Dataset labeler using optimal BART configuration for tweet-note classification.\n",
    "    \n",
    "    Configuration:\n",
    "    - Model: Combined input (tweet + note)\n",
    "    - Classes: economics, health, lifestyle, politics, science, sports\n",
    "    - Confidence threshold: 0.9\n",
    "    - Expected performance: 90.4% accuracy, 91.2% coverage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, confidence_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Initialize the dataset labeler.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the trained BART model\n",
    "            confidence_threshold (float): Confidence threshold for predictions\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.class_labels = ['economics', 'health', 'lifestyle', 'politics', 'science', 'sports']\n",
    "        \n",
    "        print(\"Initializing BSC Dataset Labeler\")\n",
    "        print(f\"Model path: {model_path}\")\n",
    "        print(f\"Confidence threshold: {confidence_threshold}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the trained BART model and tokenizer.\"\"\"\n",
    "        print(\"Loading model and tokenizer...\")\n",
    "        \n",
    "        try:\n",
    "            self.model = BartForSequenceClassification.from_pretrained(self.model_path)\n",
    "            self.tokenizer = BartTokenizer.from_pretrained(self.model_path)\n",
    "            \n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"Model loaded successfully\")\n",
    "            print(f\"Number of classes: {self.model.config.num_labels}\")\n",
    "            print(f\"Vocabulary size: {self.tokenizer.vocab_size}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
    "    \n",
    "    # Data Preprocessing Methods\n",
    "    # Functions for cleaning and preparing dataset for model input\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean and prepare text for model input.\"\"\"\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).strip()\n",
    "        # Remove non-printable characters except tabs, newlines, returns\n",
    "        text = ''.join(char for char in text if ord(char) >= 32 or char in '\\t\\n\\r')\n",
    "        return text\n",
    "    \n",
    "    def _prepare_data(self, df):\n",
    "        \"\"\"\n",
    "        Prepare the dataset for prediction.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Cleaned dataframe ready for prediction\n",
    "        \"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Map dataset columns to model expected format\n",
    "        df_clean['tweet_text'] = df_clean['full_text'].apply(self._clean_text)\n",
    "        df_clean['note_text'] = df_clean['summary'].apply(self._clean_text)\n",
    "        \n",
    "        # Filter out rows with empty text in either field\n",
    "        initial_count = len(df_clean)\n",
    "        df_clean = df_clean[\n",
    "            (df_clean['tweet_text'] != \"\") & \n",
    "            (df_clean['note_text'] != \"\")\n",
    "        ].copy()\n",
    "        final_count = len(df_clean)\n",
    "        \n",
    "        print(f\"Samples processed: {initial_count}\")\n",
    "        print(f\"Valid samples: {final_count}\")\n",
    "        print(f\"Removed samples: {initial_count - final_count}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    # Batch Prediction Engine\n",
    "    # Core prediction functionality with confidence thresholding\n",
    "    \n",
    "    def predict_batch(self, tweet_texts, note_texts, batch_size=16):\n",
    "        \"\"\"\n",
    "        Generate predictions for a batch of tweet-note pairs.\n",
    "        \n",
    "        Args:\n",
    "            tweet_texts (list): List of tweet texts\n",
    "            note_texts (list): List of corresponding note texts\n",
    "            batch_size (int): Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (predictions, confidences) lists\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        \n",
    "        print(f\"Processing {len(tweet_texts)} samples in batches of {batch_size}\")\n",
    "        \n",
    "        for i in range(0, len(tweet_texts), batch_size):\n",
    "            batch_tweets = tweet_texts[i:i+batch_size]\n",
    "            batch_notes = note_texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize the batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_tweets,\n",
    "                batch_notes,\n",
    "                truncation=\"longest_first\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=1024,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move inputs to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                probabilities = softmax(outputs.logits.cpu().numpy(), axis=1)\n",
    "            \n",
    "            # Process predictions for this batch\n",
    "            for probs in probabilities:\n",
    "                max_confidence = np.max(probs)\n",
    "                predicted_class_idx = np.argmax(probs)\n",
    "                \n",
    "                # Apply confidence threshold\n",
    "                if max_confidence >= self.confidence_threshold:\n",
    "                    predicted_label = self.class_labels[predicted_class_idx]\n",
    "                else:\n",
    "                    predicted_label = 'low_confidence'\n",
    "                \n",
    "                all_predictions.append(predicted_label)\n",
    "                all_confidences.append(max_confidence)\n",
    "            \n",
    "            # Progress reporting\n",
    "            if (i // batch_size + 1) % 20 == 0:\n",
    "                completed = min(i + batch_size, len(tweet_texts))\n",
    "                print(f\"Processed {completed}/{len(tweet_texts)} samples\")\n",
    "        \n",
    "        return all_predictions, all_confidences\n",
    "    \n",
    "    # Dataset Labeling Pipeline\n",
    "    # Main function orchestrating the complete labeling process\n",
    "    \n",
    "    def label_dataset(self, input_csv, output_csv=None, batch_size=16):\n",
    "        \"\"\"\n",
    "        Label the complete dataset and save results.\n",
    "        \n",
    "        Args:\n",
    "            input_csv (str): Path to input CSV file\n",
    "            output_csv (str): Path for output CSV (optional)\n",
    "            batch_size (int): Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Labeled dataframe\n",
    "        \"\"\"\n",
    "        print(f\"Starting dataset labeling process\")\n",
    "        print(f\"Input file: {input_csv}\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        try:\n",
    "            df = pd.read_csv(input_csv)\n",
    "            print(f\"Dataset loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load dataset: {str(e)}\")\n",
    "        \n",
    "        # Prepare data for prediction\n",
    "        df_clean = self._prepare_data(df)\n",
    "        \n",
    "        if len(df_clean) == 0:\n",
    "            raise ValueError(\"No valid samples found after data cleaning\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        print(\"Generating predictions...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        predictions, confidences = self.predict_batch(\n",
    "            df_clean['tweet_text'].tolist(),\n",
    "            df_clean['note_text'].tolist(),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        processing_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Add predictions to dataframe\n",
    "        df_clean['predicted_classification'] = predictions\n",
    "        df_clean['prediction_confidence'] = confidences\n",
    "        df_clean['manual_review_required'] = df_clean['predicted_classification'] == 'low_confidence'\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        total_samples = len(predictions)\n",
    "        confident_predictions = sum(1 for p in predictions if p != 'low_confidence')\n",
    "        coverage = confident_predictions / total_samples * 100\n",
    "        \n",
    "        print(f\"\\nPrediction Results:\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        print(f\"Confident predictions: {confident_predictions}\")\n",
    "        print(f\"Coverage: {coverage:.1f}%\")\n",
    "        print(f\"Manual review needed: {total_samples - confident_predictions} ({100-coverage:.1f}%)\")\n",
    "        print(f\"Processing time: {processing_time:.1f} seconds\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        pred_counts = pd.Series(predictions).value_counts()\n",
    "        print(f\"\\nClass Distribution:\")\n",
    "        for class_name, count in pred_counts.items():\n",
    "            percentage = count / total_samples * 100\n",
    "            print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Save results\n",
    "        if output_csv is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_csv = f\"labeling_results/bsc_dataset_labeled_{timestamp}.csv\"\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            df_clean.to_csv(output_csv, index=False)\n",
    "            print(f\"\\nResults saved to: {output_csv}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save results - {str(e)}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    # Summary Report Generation\n",
    "    # Creates comprehensive summary report with detailed statistics\n",
    "    \n",
    "    def generate_summary_report(self, labeled_df, output_path=None):\n",
    "        \"\"\"\n",
    "        Generate a summary report of the labeling results.\n",
    "        \n",
    "        Args:\n",
    "            labeled_df (pd.DataFrame): DataFrame with predictions\n",
    "            output_path (str): Path for summary report (optional)\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = f\"labeling_results/labeling_summary_{timestamp}.txt\"\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        total_samples = len(labeled_df)\n",
    "        confident_predictions = len(labeled_df[labeled_df['predicted_classification'] != 'low_confidence'])\n",
    "        coverage = confident_predictions / total_samples * 100\n",
    "        \n",
    "        confidence_stats = labeled_df['prediction_confidence'].describe()\n",
    "        class_distribution = labeled_df['predicted_classification'].value_counts()\n",
    "        \n",
    "        # Generate report\n",
    "        report = [\n",
    "            \"BSC Dataset Labeling Summary Report\",\n",
    "            \"=\" * 50,\n",
    "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"Model: Combined BART (tweet + note), without 'other' class\",\n",
    "            f\"Confidence threshold: {self.confidence_threshold}\",\n",
    "            \"\",\n",
    "            \"Dataset Statistics:\",\n",
    "            f\"  Total samples: {total_samples:,}\",\n",
    "            f\"  Confident predictions: {confident_predictions:,}\",\n",
    "            f\"  Coverage: {coverage:.1f}%\",\n",
    "            f\"  Manual review required: {total_samples - confident_predictions:,} ({100-coverage:.1f}%)\",\n",
    "            \"\",\n",
    "            \"Confidence Score Statistics:\",\n",
    "            f\"  Mean: {confidence_stats['mean']:.3f}\",\n",
    "            f\"  Median: {confidence_stats['50%']:.3f}\",\n",
    "            f\"  Standard deviation: {confidence_stats['std']:.3f}\",\n",
    "            f\"  Minimum: {confidence_stats['min']:.3f}\",\n",
    "            f\"  Maximum: {confidence_stats['max']:.3f}\",\n",
    "            \"\",\n",
    "            \"Class Distribution:\",\n",
    "        ]\n",
    "        \n",
    "        for class_name, count in class_distribution.items():\n",
    "            percentage = count / total_samples * 100\n",
    "            report.append(f\"  {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Save report\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(report))\n",
    "        \n",
    "        print(f\"Summary report saved to: {output_path}\")\n",
    "    \n",
    "    # Detailed Analysis Functions\n",
    "    # Examining prediction confidence distributions and label comparisons\n",
    "    \n",
    "    def analyze_predictions(self, labeled_df):\n",
    "        \"\"\"\n",
    "        Analyze the prediction results with detailed confidence and distribution statistics.\n",
    "        \n",
    "        Args:\n",
    "            labeled_df (pd.DataFrame): DataFrame with predictions\n",
    "        \"\"\"\n",
    "        print(f\"\\nDetailed Analysis:\")\n",
    "        \n",
    "        # Confidence distribution statistics\n",
    "        confidence_stats = labeled_df['prediction_confidence'].describe()\n",
    "        print(f\"\\nConfidence Score Distribution:\")\n",
    "        print(f\"   Mean: {confidence_stats['mean']:.3f}\")\n",
    "        print(f\"   Median: {confidence_stats['50%']:.3f}\")\n",
    "        print(f\"   Standard deviation: {confidence_stats['std']:.3f}\")\n",
    "        print(f\"   Minimum: {confidence_stats['min']:.3f}\")\n",
    "        print(f\"   Maximum: {confidence_stats['max']:.3f}\")\n",
    "        \n",
    "        # Confidence tier analysis\n",
    "        high_conf = labeled_df[labeled_df['prediction_confidence'] > 0.95]\n",
    "        moderate_conf = labeled_df[\n",
    "            (labeled_df['prediction_confidence'] > self.confidence_threshold) & \n",
    "            (labeled_df['prediction_confidence'] <= 0.95)\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nConfidence Tier Analysis:\")\n",
    "        print(f\"   Very high confidence (>0.95): {len(high_conf)} samples ({len(high_conf)/len(labeled_df)*100:.1f}%)\")\n",
    "        print(f\"   Moderate confidence ({self.confidence_threshold}-0.95): {len(moderate_conf)} samples ({len(moderate_conf)/len(labeled_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Compare with original labels if they exist\n",
    "        if 'classification' in labeled_df.columns:\n",
    "            print(f\"\\nComparison with Original Labels:\")\n",
    "            original_counts = labeled_df['classification'].value_counts()\n",
    "            predicted_counts = labeled_df['predicted_classification'].value_counts()\n",
    "            \n",
    "            print(\"Original label distribution:\")\n",
    "            for label, count in original_counts.items():\n",
    "                pct = count / len(labeled_df) * 100\n",
    "                print(f\"   {label}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            print(\"Predicted label distribution:\")\n",
    "            for label, count in predicted_counts.items():\n",
    "                pct = count / len(labeled_df) * 100\n",
    "                print(f\"   {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca247f04",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Execute the complete dataset labeling pipeline, including analysis of prediction confidence and class distributions.\n",
    "\n",
    "**Configuration:**\n",
    "- Model path: `./trained_models/final_without_other_combined/`\n",
    "- Input dataset: `for_bsc_project.csv`\n",
    "- Confidence threshold: 0.9\n",
    "- Expected performance: ~90.4% accuracy on confident predictions\n",
    "\n",
    "**Output Files:**\n",
    "- `bsc_dataset_labeled_{timestamp}.csv`: Complete labeled dataset\n",
    "- `labeling_summary_{timestamp}.txt`: Summary statistics report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd99b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for dataset labeling.\"\"\"\n",
    "    \n",
    "    print(\"BSC Dataset Labeling Tool\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration\n",
    "    MODEL_PATH = \"./trained_models/final_without_other_combined/\"\n",
    "    INPUT_CSV = \"for_bsc_project.csv\"\n",
    "    CONFIDENCE_THRESHOLD = 0.9\n",
    "    BATCH_SIZE = 16\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model not found at {MODEL_PATH}\")\n",
    "        print(\"Please ensure the model path is correct.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(INPUT_CSV):\n",
    "        print(f\"Error: Dataset not found at {INPUT_CSV}\")\n",
    "        print(\"Please ensure the CSV file is in the current directory.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize labeler and process dataset\n",
    "    try:\n",
    "        labeler = BSCDatasetLabeler(\n",
    "            model_path=MODEL_PATH,\n",
    "            confidence_threshold=CONFIDENCE_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Label the dataset\n",
    "        labeled_df = labeler.label_dataset(\n",
    "            input_csv=INPUT_CSV,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Perform detailed analysis\n",
    "        labeler.analyze_predictions(labeled_df)\n",
    "        \n",
    "        # Generate summary report\n",
    "        labeler.generate_summary_report(labeled_df)\n",
    "        \n",
    "        print(\"\\nDataset labeling completed successfully!\")\n",
    "        print(\"Expected performance: ~90.4% accuracy on confident predictions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        return\n",
    "\n",
    "# Execute the labeling process\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
