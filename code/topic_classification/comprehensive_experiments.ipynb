{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd36f426",
   "metadata": {},
   "source": [
    "# Comprehensive BART Model Experiments\n",
    "\n",
    "Systematic evaluation of all model configurations to determine optimal approach for tweet-note topic classification.\n",
    "\n",
    "## Experimental Design\n",
    "- **Model Types**: Tweet-only, Note-only, Combined input\n",
    "- **Training Strategies**: With/without \"other\" class  \n",
    "- **Confidence Thresholds**: 0.5 to 0.99 (8 levels)\n",
    "- **Total Configurations**: 6 models × 8 thresholds = 48 experiments\n",
    "\n",
    "Uses optimal hyperparameters from previous optimization (LR=2e-5, BS=4, Epochs=5).\n",
    "\n",
    "## Dependencies and Configuration\n",
    "Setting up imports, optimal hyperparameters, and experimental configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17770e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    BartForSequenceClassification, \n",
    "    BartTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "TRAIN_FILE = \"Training set labeled.xlsx\"\n",
    "TEST_FILE = \"Test set labeled.xlsx\"\n",
    "RANDOM_STATE = 42\n",
    "MODEL_NAME = \"facebook/bart-large-mnli\"\n",
    "\n",
    "# Use optimal hyperparameters from previous optimization\n",
    "OPTIMAL_CONFIG = {\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.0,\n",
    "    'max_token_length': 1024\n",
    "}\n",
    "\n",
    "# Experiment configurations\n",
    "EXPERIMENT_CONFIGS = [\n",
    "    {\n",
    "        'name': 'with_other',\n",
    "        'exclude_classes': [],\n",
    "        'description': 'Train on all 7 classes including \"other\"'\n",
    "    },\n",
    "    {\n",
    "        'name': 'without_other', \n",
    "        'exclude_classes': ['other'],\n",
    "        'description': 'Train on 6 classes excluding \"other\"'\n",
    "    }\n",
    "]\n",
    "\n",
    "MODEL_MODES = ['tweet', 'note', 'combined']\n",
    "CONFIDENCE_THRESHOLDS = [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('experiment_results', exist_ok=True)\n",
    "os.makedirs('trained_models', exist_ok=True)  # For final model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb64c3",
   "metadata": {},
   "source": [
    "## Experiment Logging\n",
    "Utility functions for tracking experimental progress and results across all 48 configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(message, log_file='experiment_results/experiment_log.txt'):\n",
    "    \"\"\"Log experiment progress with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_message = f\"[{timestamp}] {message}\"\n",
    "    print(log_message)\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(log_message + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2941d3",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Functions for consistent data preparation across all experimental conditions.  \n",
    "Handles text cleaning, label encoding, dataset balancing, and excluded class management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54925c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_file, test_file, excluded_classes=None):\n",
    "    \"\"\"Load and clean datasets with consistent preprocessing\"\"\"\n",
    "    \n",
    "    if excluded_classes is None:\n",
    "        excluded_classes = []\n",
    "    \n",
    "    def clean_dataframe(df):\n",
    "        df['label'] = df['label'].astype(str).str.lower().str.strip()\n",
    "        df.dropna(subset=['label', 'tweet_text', 'note_text'], inplace=True)\n",
    "        df = df[df['label'] != '']\n",
    "        \n",
    "        # Clean text\n",
    "        def clean_text(text):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "            text = str(text).strip()\n",
    "            text = ''.join(char for char in text if ord(char) >= 32 or char in '\\t\\n\\r')\n",
    "            return text\n",
    "        \n",
    "        df['tweet_text'] = df['tweet_text'].apply(clean_text)\n",
    "        df['note_text'] = df['note_text'].apply(clean_text)\n",
    "        df = df[(df['tweet_text'] != \"\") & (df['note_text'] != \"\")]\n",
    "        \n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    df_train = clean_dataframe(pd.read_excel(train_file))\n",
    "    df_test = clean_dataframe(pd.read_excel(test_file))\n",
    "    \n",
    "    # Filter training data if excluding classes\n",
    "    if excluded_classes:\n",
    "        original_size = len(df_train)\n",
    "        df_train = df_train[~df_train['label'].isin(excluded_classes)]\n",
    "        log_experiment(f\"Excluded {original_size - len(df_train)} training samples with classes {excluded_classes}\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def oversample_minority_classes(df, random_state=42):\n",
    "    \"\"\"Balance dataset through minority oversampling\"\"\"\n",
    "    class_counts = df['label'].value_counts()\n",
    "    max_count = class_counts.max()\n",
    "    \n",
    "    oversampled_dfs = []\n",
    "    for label in class_counts.index:\n",
    "        class_df = df[df['label'] == label]\n",
    "        oversampled_df = class_df.sample(max_count, replace=True, random_state=random_state)\n",
    "        oversampled_dfs.append(oversampled_df)\n",
    "    \n",
    "    balanced_df = pd.concat(oversampled_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    log_experiment(f\"Balanced dataset: {len(df)} → {len(balanced_df)} samples\")\n",
    "    return balanced_df\n",
    "\n",
    "def encode_labels_with_exclusions(df_train, df_test, excluded_classes=None):\n",
    "    \"\"\"Encode labels handling excluded classes for evaluation\"\"\"\n",
    "    \n",
    "    if excluded_classes is None:\n",
    "        excluded_classes = []\n",
    "    \n",
    "    # Create label encoder using only training classes\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df_train['label'].unique())\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Encode training labels\n",
    "    df_train['label_id'] = label_encoder.transform(df_train['label'])\n",
    "    \n",
    "    # Create test datasets\n",
    "    df_test_training = df_test[~df_test['label'].isin(excluded_classes)].copy()\n",
    "    df_test_training['label_id'] = label_encoder.transform(df_test_training['label'])\n",
    "    \n",
    "    # Handle full test set with excluded classes\n",
    "    def encode_test_label_safe(label):\n",
    "        if label in excluded_classes:\n",
    "            return num_classes  # Special ID for excluded classes\n",
    "        else:\n",
    "            try:\n",
    "                return label_encoder.transform([label])[0]\n",
    "            except ValueError:\n",
    "                return num_classes\n",
    "    \n",
    "    df_test['label_id'] = df_test['label'].apply(encode_test_label_safe)\n",
    "    \n",
    "    # Create extended label encoder for reporting\n",
    "    all_unique_labels = list(label_encoder.classes_) + excluded_classes\n",
    "    extended_label_encoder = LabelEncoder()\n",
    "    extended_label_encoder.fit(all_unique_labels)\n",
    "    \n",
    "    log_experiment(f\"Label encoding: {num_classes} training classes, {len(excluded_classes)} excluded\")\n",
    "    \n",
    "    return df_train, df_test_training, df_test, label_encoder, extended_label_encoder, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845906b",
   "metadata": {},
   "source": [
    "## Model Training Framework\n",
    "Core training functions using optimal hyperparameters.  \n",
    "Models are trained efficiently with final models saved for validation notebook reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(df_train, df_test, mode, tokenizer):\n",
    "    \"\"\"Create tokenized datasets for specific input mode\"\"\"\n",
    "    \n",
    "    def tokenize_function(batch):\n",
    "        if mode == 'combined':\n",
    "            return tokenizer(\n",
    "                batch['tweet_text'], \n",
    "                batch['note_text'], \n",
    "                truncation=\"longest_first\",\n",
    "                padding=\"max_length\", \n",
    "                max_length=OPTIMAL_CONFIG['max_token_length']\n",
    "            )\n",
    "        elif mode == 'tweet':\n",
    "            return tokenizer(\n",
    "                batch['tweet_text'], \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                max_length=OPTIMAL_CONFIG['max_token_length']\n",
    "            )\n",
    "        else:  # note mode\n",
    "            return tokenizer(\n",
    "                batch['note_text'], \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                max_length=OPTIMAL_CONFIG['max_token_length']\n",
    "            )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_pandas(\n",
    "        df_train[['tweet_text', 'note_text', 'label_id']].rename(columns={'label_id': 'labels'})\n",
    "    )\n",
    "    test_dataset = Dataset.from_pandas(\n",
    "        df_test[['tweet_text', 'note_text', 'label_id']].rename(columns={'label_id': 'labels'})\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
    "    \n",
    "    # Remove text columns\n",
    "    columns_to_remove = ['tweet_text', 'note_text']\n",
    "    if '__index_level_0__' in train_dataset.column_names:\n",
    "        columns_to_remove.append('__index_level_0__')\n",
    "    \n",
    "    train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "    test_dataset = test_dataset.remove_columns(columns_to_remove)\n",
    "    \n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def train_model(train_dataset, test_dataset, experiment_name, mode, num_classes):\n",
    "    \"\"\"Train BART model with optimal hyperparameters\"\"\"\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BartForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    model.config.num_labels = num_classes\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Training arguments (no intermediate saving, but save final model)\n",
    "    output_dir = f\"temp_{experiment_name}_{mode}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",  # Don't save intermediate checkpoints\n",
    "        num_train_epochs=OPTIMAL_CONFIG['num_epochs'],\n",
    "        per_device_train_batch_size=OPTIMAL_CONFIG['batch_size'],\n",
    "        per_device_eval_batch_size=OPTIMAL_CONFIG['batch_size'] * 2,\n",
    "        learning_rate=OPTIMAL_CONFIG['learning_rate'],\n",
    "        weight_decay=OPTIMAL_CONFIG['weight_decay'],\n",
    "        seed=RANDOM_STATE,\n",
    "        load_best_model_at_end=False,  # No intermediate saving\n",
    "        save_total_limit=0,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        accuracy = (predictions == labels).astype(np.float32).mean().item()\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    log_experiment(f\"Training {experiment_name} - {mode} mode...\")\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    log_experiment(f\"Completed {experiment_name} - {mode}: Accuracy={eval_results['eval_accuracy']:.4f}\")\n",
    "    \n",
    "    # Save FINAL model only\n",
    "    final_model_path = f\"trained_models/final_{experiment_name}_{mode}\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    \n",
    "    log_experiment(f\"Final model saved: {final_model_path}\")\n",
    "    \n",
    "    # Clean up temporary directory (but keep final model)\n",
    "    import shutil\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    \n",
    "    return trainer, eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4963ccb",
   "metadata": {},
   "source": [
    "## Confidence Threshold Evaluation\n",
    "Evaluates each trained model across all confidence thresholds (0.5-0.99) to analyze accuracy-coverage trade-offs and \"other\" class detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_confidence_thresholds(model, test_dataset_full, df_test_full, \n",
    "                                       extended_label_encoder, excluded_classes, \n",
    "                                       experiment_name, mode):\n",
    "    \"\"\"Evaluate model across all confidence thresholds\"\"\"\n",
    "    \n",
    "    log_experiment(f\"Evaluating confidence thresholds for {experiment_name} - {mode}...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    test_dataloader = DataLoader(test_dataset_full, batch_size=8, shuffle=False)\n",
    "    \n",
    "    all_logits = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            true_labels_batch = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_true_labels.append(true_labels_batch.numpy())\n",
    "    \n",
    "    # Process predictions\n",
    "    logits = np.vstack(all_logits)\n",
    "    true_labels = np.concatenate(all_true_labels)\n",
    "    \n",
    "    probabilities = softmax(logits, axis=1)\n",
    "    predicted_labels = np.argmax(logits, axis=1)\n",
    "    max_probabilities = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Evaluate each threshold\n",
    "    results = []\n",
    "    threshold_predictions = {}  # Store predictions for each threshold\n",
    "    num_training_classes = len(extended_label_encoder.classes_) - len(excluded_classes)\n",
    "    unknown_label_id = len(extended_label_encoder.classes_)\n",
    "    \n",
    "    for threshold in CONFIDENCE_THRESHOLDS:\n",
    "        # Apply confidence threshold\n",
    "        final_predictions = []\n",
    "        for pred, conf in zip(predicted_labels, max_probabilities):\n",
    "            if conf > threshold:\n",
    "                final_predictions.append(pred)\n",
    "            else:\n",
    "                final_predictions.append(unknown_label_id)\n",
    "        \n",
    "        # Store predictions for this threshold\n",
    "        threshold_predictions[threshold] = final_predictions\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_samples = len(final_predictions)\n",
    "        unknown_count = sum(1 for pred in final_predictions if pred == unknown_label_id)\n",
    "        \n",
    "        # Handle excluded classes in metrics\n",
    "        excluded_class_ids = [num_training_classes + i for i in range(len(excluded_classes))]\n",
    "        non_excluded_mask = ~np.isin(true_labels, excluded_class_ids)\n",
    "        \n",
    "        # Overall accuracy (excluding \"other\" samples)\n",
    "        if non_excluded_mask.any():\n",
    "            overall_accuracy = (\n",
    "                np.array(final_predictions)[non_excluded_mask] == \n",
    "                np.array(true_labels)[non_excluded_mask]\n",
    "            ).mean()\n",
    "        else:\n",
    "            overall_accuracy = 0.0\n",
    "        \n",
    "        # Committed accuracy\n",
    "        committed_mask = (np.array(final_predictions) != unknown_label_id) & non_excluded_mask\n",
    "        if committed_mask.any():\n",
    "            committed_accuracy = (\n",
    "                np.array(final_predictions)[committed_mask] == \n",
    "                np.array(true_labels)[committed_mask]\n",
    "            ).mean()\n",
    "        else:\n",
    "            committed_accuracy = 0.0\n",
    "        \n",
    "        # Coverage\n",
    "        non_excluded_count = non_excluded_mask.sum()\n",
    "        if non_excluded_count > 0:\n",
    "            confident_non_excluded = ((np.array(final_predictions) != unknown_label_id) & non_excluded_mask).sum()\n",
    "            coverage = (confident_non_excluded / non_excluded_count) * 100\n",
    "        else:\n",
    "            coverage = 0.0\n",
    "        \n",
    "        # Other capture rate\n",
    "        other_samples_mask = np.isin(true_labels, excluded_class_ids)\n",
    "        other_samples_count = other_samples_mask.sum()\n",
    "        \n",
    "        if other_samples_count > 0:\n",
    "            other_predicted_as_unknown = sum(1 for i, is_other in enumerate(other_samples_mask) \n",
    "                                           if is_other and final_predictions[i] == unknown_label_id)\n",
    "            other_capture_rate = other_predicted_as_unknown / other_samples_count * 100\n",
    "        else:\n",
    "            other_capture_rate = 0.0\n",
    "        \n",
    "        results.append({\n",
    "            'experiment': experiment_name,\n",
    "            'mode': mode,\n",
    "            'confidence_threshold': threshold,\n",
    "            'overall_accuracy': overall_accuracy,\n",
    "            'committed_accuracy': committed_accuracy,\n",
    "            'coverage': coverage,\n",
    "            'other_capture_rate': other_capture_rate,\n",
    "            'unknown_predictions': unknown_count,\n",
    "            'total_samples': total_samples,\n",
    "            'true_labels': true_labels.tolist(),  # For CSV saving\n",
    "            'predicted_labels': final_predictions  # For CSV saving\n",
    "        })\n",
    "    \n",
    "    log_experiment(f\"Completed threshold evaluation for {experiment_name} - {mode}\")\n",
    "    \n",
    "    # Return both results and raw prediction data\n",
    "    predictions_data = {\n",
    "        'probabilities': probabilities,\n",
    "        'max_probabilities': max_probabilities,\n",
    "        'true_labels': true_labels,\n",
    "        'predicted_labels': predicted_labels,\n",
    "        'threshold_predictions': threshold_predictions\n",
    "    }\n",
    "    \n",
    "    return results, predictions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402fc57",
   "metadata": {},
   "source": [
    "## Comprehensive Experiments Pipeline\n",
    "Main experimental pipeline that systematically trains and evaluates all 6 model configurations, then tests each across 8 confidence thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae92d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_experiments():\n",
    "    \"\"\"Run all experimental configurations\"\"\"\n",
    "    \n",
    "    log_experiment(\"Starting comprehensive BART experiments...\")\n",
    "    log_experiment(f\"Total configurations: {len(EXPERIMENT_CONFIGS)} × {len(MODEL_MODES)} × {len(CONFIDENCE_THRESHOLDS)} = {len(EXPERIMENT_CONFIGS) * len(MODEL_MODES) * len(CONFIDENCE_THRESHOLDS)}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for config in EXPERIMENT_CONFIGS:\n",
    "        log_experiment(f\"\\n🔬 EXPERIMENT: {config['name'].upper()}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df_train, df_test = load_and_preprocess_data(\n",
    "            TRAIN_FILE, TEST_FILE, config['exclude_classes']\n",
    "        )\n",
    "        \n",
    "        # Balance training data\n",
    "        df_train = oversample_minority_classes(df_train, RANDOM_STATE)\n",
    "        \n",
    "        # Encode labels\n",
    "        df_train, df_test_training, df_test_full, label_encoder, extended_label_encoder, num_classes = encode_labels_with_exclusions(\n",
    "            df_train, df_test, config['exclude_classes']\n",
    "        )\n",
    "        \n",
    "        for mode in MODEL_MODES:\n",
    "            log_experiment(f\"\\n📊 Training {mode.upper()} model...\")\n",
    "            \n",
    "            try:\n",
    "                # Create tokenizer\n",
    "                tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "                \n",
    "                # Create datasets\n",
    "                train_dataset, test_dataset = create_datasets(\n",
    "                    df_train, df_test_training, mode, tokenizer\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                trainer, eval_results = train_model(\n",
    "                    train_dataset, test_dataset, config['name'], mode, num_classes\n",
    "                )\n",
    "                \n",
    "                # Create full test dataset for threshold evaluation\n",
    "                test_dataset_full = Dataset.from_pandas(\n",
    "                    df_test_full[['tweet_text', 'note_text', 'label_id']].rename(columns={'label_id': 'labels'})\n",
    "                )\n",
    "                \n",
    "                # Tokenize full test dataset\n",
    "                def tokenize_function(batch):\n",
    "                    if mode == 'combined':\n",
    "                        return tokenizer(\n",
    "                            batch['tweet_text'], \n",
    "                            batch['note_text'], \n",
    "                            truncation=\"longest_first\",\n",
    "                            padding=\"max_length\", \n",
    "                            max_length=OPTIMAL_CONFIG['max_token_length']\n",
    "                        )\n",
    "                    elif mode == 'tweet':\n",
    "                        return tokenizer(\n",
    "                            batch['tweet_text'], \n",
    "                            padding='max_length', \n",
    "                            truncation=True, \n",
    "                            max_length=OPTIMAL_CONFIG['max_token_length']\n",
    "                        )\n",
    "                    else:\n",
    "                        return tokenizer(\n",
    "                            batch['note_text'], \n",
    "                            padding='max_length', \n",
    "                            truncation=True, \n",
    "                            max_length=OPTIMAL_CONFIG['max_token_length']\n",
    "                        )\n",
    "                \n",
    "                test_dataset_full = test_dataset_full.map(tokenize_function, batched=True, batch_size=16)\n",
    "                \n",
    "                columns_to_remove = ['tweet_text', 'note_text']\n",
    "                if '__index_level_0__' in test_dataset_full.column_names:\n",
    "                    columns_to_remove.append('__index_level_0__')\n",
    "                \n",
    "                test_dataset_full = test_dataset_full.remove_columns(columns_to_remove)\n",
    "                test_dataset_full.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "                \n",
    "                # Evaluate confidence thresholds\n",
    "                threshold_results, predictions_data = evaluate_with_confidence_thresholds(\n",
    "                    trainer.model, test_dataset_full, df_test_full,\n",
    "                    extended_label_encoder, config['exclude_classes'],\n",
    "                    config['name'], mode\n",
    "                )\n",
    "                \n",
    "                # Add base model performance\n",
    "                for result in threshold_results:\n",
    "                    result['base_model_accuracy'] = eval_results['eval_accuracy']\n",
    "                \n",
    "                all_results.extend(threshold_results)\n",
    "                \n",
    "                # Save detailed results for key configurations (needed for later analysis)\n",
    "                model_results_df = pd.DataFrame(threshold_results)\n",
    "                model_results_df.to_csv(f'experiment_results/{config[\"name\"]}_{mode}_detailed.csv', index=False)\n",
    "                \n",
    "                # Save prediction probabilities for key configurations\n",
    "                with open(f'experiment_results/{config[\"name\"]}_{mode}_probabilities.pkl', 'wb') as f:\n",
    "                    pickle.dump({\n",
    "                        'probabilities': predictions_data['probabilities'],\n",
    "                        'max_probabilities': predictions_data['max_probabilities'],\n",
    "                        'true_labels': predictions_data['true_labels'],\n",
    "                        'predicted_labels': predictions_data['predicted_labels'],\n",
    "                        'label_encoder': label_encoder,\n",
    "                        'extended_label_encoder': extended_label_encoder\n",
    "                    }, f)\n",
    "                \n",
    "                # Clean up memory\n",
    "                del trainer\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "                log_experiment(f\"✅ Completed {mode} model for {config['name']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_experiment(f\"❌ Error with {mode} model in {config['name']}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Save results\n",
    "    log_experiment(\"\\n💾 Saving comprehensive results...\")\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Remove the array columns before saving main results\n",
    "    clean_results = []\n",
    "    for result in all_results:\n",
    "        clean_result = result.copy()\n",
    "        # Remove array fields that can't be saved to CSV/Excel cleanly\n",
    "        clean_result.pop('true_labels', None)\n",
    "        clean_result.pop('predicted_labels', None)\n",
    "        clean_results.append(clean_result)\n",
    "    \n",
    "    clean_results_df = pd.DataFrame(clean_results)\n",
    "    clean_results_df.to_csv('experiment_results/complete_results.csv', index=False)\n",
    "    clean_results_df.to_excel('experiment_results/complete_results.xlsx', index=False)\n",
    "    \n",
    "    # Create summary\n",
    "    summary_cols = ['experiment', 'mode', 'confidence_threshold', 'base_model_accuracy',\n",
    "                   'committed_accuracy', 'coverage', 'other_capture_rate']\n",
    "    summary_df = clean_results_df[summary_cols].copy()\n",
    "    summary_df.to_csv('experiment_results/experiment_summary.csv', index=False)\n",
    "    \n",
    "    log_experiment(f\"✅ Completed all experiments! Results: {len(all_results)} configurations\")\n",
    "    log_experiment(\"📁 Generated core files:\")\n",
    "    log_experiment(\"   - complete_results.xlsx: All configuration results\")\n",
    "    log_experiment(\"   - complete_results.csv: All configuration results\")\n",
    "    log_experiment(\"   - experiment_summary.csv: Key metrics summary\")\n",
    "    log_experiment(\"📁 Generated final models:\")\n",
    "    log_experiment(\"   - trained_models/final_with_other_tweet\")\n",
    "    log_experiment(\"   - trained_models/final_with_other_note\") \n",
    "    log_experiment(\"   - trained_models/final_with_other_combined\")\n",
    "    log_experiment(\"   - trained_models/final_without_other_tweet\")\n",
    "    log_experiment(\"   - trained_models/final_without_other_note\")\n",
    "    log_experiment(\"   - trained_models/final_without_other_combined ← Used by validation\")\n",
    "    log_experiment(\"📁 Generated detailed model files:\")\n",
    "    log_experiment(\"   - [experiment]_[mode]_detailed.csv: Detailed results for each model\")\n",
    "    log_experiment(\"   - [experiment]_[mode]_probabilities.pkl: Prediction data for each model\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b3cfb",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "Functions for analyzing experimental results, creating comparison plots, and identifying optimal configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_plots(results_df):\n",
    "    \"\"\"Create comprehensive comparison visualizations\"\"\"\n",
    "    \n",
    "    log_experiment(\"Creating comparison plots...\")\n",
    "    \n",
    "    # Set up plotting\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    experiments = results_df['experiment'].unique()\n",
    "    modes = results_df['mode'].unique()\n",
    "    \n",
    "    # Main comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('BART Model Comparison Across All Configurations', fontsize=16)\n",
    "    \n",
    "    for i, experiment in enumerate(experiments):\n",
    "        for j, metric in enumerate(['committed_accuracy', 'coverage']):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            exp_data = results_df[results_df['experiment'] == experiment]\n",
    "            \n",
    "            for mode in modes:\n",
    "                mode_data = exp_data[exp_data['mode'] == mode]\n",
    "                ax.plot(mode_data['confidence_threshold'], mode_data[metric], \n",
    "                       'o-', label=mode, linewidth=2, markersize=6)\n",
    "            \n",
    "            ax.set_xlabel('Confidence Threshold')\n",
    "            ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "            ax.set_title(f'{experiment.replace(\"_\", \" \").title()} - {metric.replace(\"_\", \" \").title()}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_results/comparison_plots_by_experiment.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Trade-off analysis\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, experiment in enumerate(experiments):\n",
    "        plt.subplot(1, len(experiments), i+1)\n",
    "        \n",
    "        exp_data = results_df[results_df['experiment'] == experiment]\n",
    "        \n",
    "        for mode in modes:\n",
    "            mode_data = exp_data[exp_data['mode'] == mode]\n",
    "            plt.scatter(mode_data['coverage'], mode_data['committed_accuracy'], \n",
    "                       s=60, alpha=0.7, label=mode)\n",
    "        \n",
    "        plt.xlabel('Coverage (%)')\n",
    "        plt.ylabel('Committed Accuracy')\n",
    "        plt.title(f'{experiment.replace(\"_\", \" \").title()}\\nAccuracy vs Coverage Trade-off')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_results/tradeoff_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    log_experiment(\"Comparison plots completed!\")\n",
    "    log_experiment(\"   - comparison_plots_by_experiment.png: 2×2 performance comparison\")\n",
    "    log_experiment(\"   - tradeoff_analysis.png: Accuracy vs coverage analysis\")\n",
    "\n",
    "def generate_final_report(results_df):\n",
    "    \"\"\"Generate comprehensive final report in markdown and LaTeX formats\"\"\"\n",
    "    \n",
    "    log_experiment(\"📋 Generating final report...\")\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# Comprehensive BART Model Experiments - Final Report\\n\")\n",
    "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    report.append(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Overall summary\n",
    "    total_configs = len(results_df)\n",
    "    experiments = results_df['experiment'].unique()\n",
    "    modes = results_df['mode'].unique()\n",
    "    \n",
    "    report.append(f\"## Experiment Summary\\n\")\n",
    "    report.append(f\"- Total configurations tested: {total_configs}\\n\")\n",
    "    report.append(f\"- Experiments: {', '.join(experiments)}\\n\")\n",
    "    report.append(f\"- Model modes: {', '.join(modes)}\\n\")\n",
    "    report.append(f\"- Confidence thresholds: {', '.join(map(str, sorted(results_df['confidence_threshold'].unique())))}\\n\\n\")\n",
    "    \n",
    "    # Best overall performance\n",
    "    report.append(\"## Best Overall Performance\\n\")\n",
    "    \n",
    "    # Find absolute best configuration\n",
    "    valid_results = results_df[results_df['coverage'] >= 80]  # Minimum 80% coverage\n",
    "    if len(valid_results) > 0:\n",
    "        best_config = valid_results.loc[valid_results['committed_accuracy'].idxmax()]\n",
    "        \n",
    "        report.append(f\"**Best Configuration:**\\n\")\n",
    "        report.append(f\"- Experiment: {best_config['experiment']}\\n\")\n",
    "        report.append(f\"- Mode: {best_config['mode']}\\n\")\n",
    "        report.append(f\"- Confidence Threshold: {best_config['confidence_threshold']}\\n\")\n",
    "        report.append(f\"- Committed Accuracy: {best_config['committed_accuracy']:.3f}\\n\")\n",
    "        report.append(f\"- Coverage: {best_config['coverage']:.1f}%\\n\")\n",
    "        report.append(f\"- Other Capture Rate: {best_config['other_capture_rate']:.1f}%\\n\\n\")\n",
    "    \n",
    "    # Performance by experiment\n",
    "    report.append(\"## Performance by Experiment\\n\")\n",
    "    \n",
    "    for experiment in experiments:\n",
    "        exp_data = results_df[results_df['experiment'] == experiment]\n",
    "        report.append(f\"### {experiment.replace('_', ' ').title()}\\n\")\n",
    "        \n",
    "        # Best for each mode\n",
    "        for mode in modes:\n",
    "            mode_data = exp_data[exp_data['mode'] == mode]\n",
    "            valid_mode_data = mode_data[mode_data['coverage'] >= 80]\n",
    "            \n",
    "            if len(valid_mode_data) > 0:\n",
    "                best_mode = valid_mode_data.loc[valid_mode_data['committed_accuracy'].idxmax()]\n",
    "                report.append(f\"**{mode.title()} Model:**\\n\")\n",
    "                report.append(f\"- Best Accuracy: {best_mode['committed_accuracy']:.3f} (threshold: {best_mode['confidence_threshold']})\\n\")\n",
    "                report.append(f\"- Coverage: {best_mode['coverage']:.1f}%\\n\")\n",
    "                report.append(f\"- Other Capture: {best_mode['other_capture_rate']:.1f}%\\n\\n\")\n",
    "    \n",
    "    # Comparison table\n",
    "    report.append(\"## Model Comparison Table\\n\")\n",
    "    \n",
    "    # Create comparison for key thresholds\n",
    "    key_thresholds = [0.5, 0.8, 0.9]\n",
    "    \n",
    "    for threshold in key_thresholds:\n",
    "        threshold_data = results_df[results_df['confidence_threshold'] == threshold]\n",
    "        if len(threshold_data) > 0:\n",
    "            report.append(f\"### At Confidence Threshold {threshold}\\n\")\n",
    "            report.append(\"| Experiment | Mode | Committed Acc | Coverage | Other Capture |\\n\")\n",
    "            report.append(\"|------------|------|---------------|----------|---------------|\\n\")\n",
    "            \n",
    "            for _, row in threshold_data.iterrows():\n",
    "                report.append(f\"| {row['experiment']} | {row['mode']} | {row['committed_accuracy']:.3f} | {row['coverage']:.1f}% | {row['other_capture_rate']:.1f}% |\\n\")\n",
    "            report.append(\"\\n\")\n",
    "    \n",
    "    # Key insights\n",
    "    report.append(\"## Key Insights\\n\")\n",
    "    \n",
    "    # Compare with vs without other\n",
    "    if 'with_other' in experiments and 'without_other' in experiments:\n",
    "        with_other = results_df[results_df['experiment'] == 'with_other']\n",
    "        without_other = results_df[results_df['experiment'] == 'without_other']\n",
    "        \n",
    "        report.append(\"### Impact of Excluding 'Other' Class\\n\")\n",
    "        \n",
    "        for mode in modes:\n",
    "            with_mode = with_other[with_other['mode'] == mode]\n",
    "            without_mode = without_other[without_other['mode'] == mode]\n",
    "            \n",
    "            if len(with_mode) > 0 and len(without_mode) > 0:\n",
    "                # Compare at same threshold\n",
    "                baseline_threshold = 0.8\n",
    "                with_baseline = with_mode[with_mode['confidence_threshold'] == baseline_threshold]\n",
    "                without_baseline = without_mode[without_mode['confidence_threshold'] == baseline_threshold]\n",
    "                \n",
    "                if len(with_baseline) > 0 and len(without_baseline) > 0:\n",
    "                    acc_improvement = without_baseline.iloc[0]['committed_accuracy'] - with_baseline.iloc[0]['committed_accuracy']\n",
    "                    report.append(f\"- {mode.title()}: {acc_improvement:+.3f} accuracy improvement when excluding 'other'\\n\")\n",
    "        \n",
    "        report.append(\"\\n\")\n",
    "    \n",
    "    # Optimal thresholds by mode\n",
    "    report.append(\"### Optimal Confidence Thresholds\\n\")\n",
    "    \n",
    "    for mode in modes:\n",
    "        mode_data = results_df[results_df['mode'] == mode]\n",
    "        valid_mode_data = mode_data[mode_data['coverage'] >= 80]\n",
    "        \n",
    "        if len(valid_mode_data) > 0:\n",
    "            optimal = valid_mode_data.loc[valid_mode_data['committed_accuracy'].idxmax()]\n",
    "            report.append(f\"- {mode.title()}: {optimal['confidence_threshold']} (Accuracy: {optimal['committed_accuracy']:.3f}, Coverage: {optimal['coverage']:.1f}%)\\n\")\n",
    "    \n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Save markdown report\n",
    "    with open('experiment_results/final_report.md', 'w') as f:\n",
    "        f.write(''.join(report))\n",
    "    \n",
    "    # Create LaTeX version\n",
    "    latex_report = []\n",
    "    latex_report.append(\"\\\\section{Comprehensive BART Model Experiments}\\n\\n\")\n",
    "    latex_report.append(\"\\\\subsection{Experiment Overview}\\n\")\n",
    "    latex_report.append(f\"We conducted comprehensive experiments testing {total_configs} different configurations across \")\n",
    "    latex_report.append(f\"{len(experiments)} experimental setups and {len(modes)} model architectures. \")\n",
    "    latex_report.append(\"The experiments evaluated the impact of including versus excluding the problematic 'other' class \")\n",
    "    latex_report.append(\"from training, along with confidence threshold optimization.\\n\\n\")\n",
    "    \n",
    "    if len(valid_results) > 0:\n",
    "        best_config = valid_results.loc[valid_results['committed_accuracy'].idxmax()]\n",
    "        latex_report.append(\"\\\\subsection{Optimal Configuration}\\n\")\n",
    "        latex_report.append(\"The best performing configuration achieved \")\n",
    "        latex_report.append(f\"{best_config['committed_accuracy']:.1%} committed accuracy with \")\n",
    "        latex_report.append(f\"{best_config['coverage']:.1f}\\\\% coverage using the {best_config['mode']} model \")\n",
    "        latex_report.append(f\"in the {best_config['experiment'].replace('_', ' ')} experiment \")\n",
    "        latex_report.append(f\"with a confidence threshold of {best_config['confidence_threshold']}.\\n\\n\")\n",
    "    \n",
    "    with open('experiment_results/final_report.tex', 'w') as f:\n",
    "        f.write(''.join(latex_report))\n",
    "    \n",
    "    log_experiment(\"📋 Final report generated!\")\n",
    "    log_experiment(\"   - final_report.md: Comprehensive markdown report\")\n",
    "    log_experiment(\"   - final_report.tex: LaTeX section for thesis\")\n",
    "    \n",
    "    return ''.join(report)\n",
    "\n",
    "def generate_experiment_summary(results_df):\n",
    "    \"\"\"Generate final experiment summary JSON\"\"\"\n",
    "    \n",
    "    log_experiment(\"Generating experiment summary...\")\n",
    "    \n",
    "    # Find overall best configuration\n",
    "    valid_results = results_df[results_df['coverage'] >= 80]\n",
    "    \n",
    "    if len(valid_results) > 0:\n",
    "        best_config = valid_results.loc[valid_results['committed_accuracy'].idxmax()]\n",
    "        \n",
    "        summary = {\n",
    "            'total_configurations': len(results_df),\n",
    "            'best_experiment': best_config['experiment'],\n",
    "            'best_mode': best_config['mode'],\n",
    "            'best_threshold': best_config['confidence_threshold'],\n",
    "            'best_committed_accuracy': best_config['committed_accuracy'],\n",
    "            'best_coverage': best_config['coverage'],\n",
    "            'best_other_capture': best_config['other_capture_rate'],\n",
    "            'experiments_tested': results_df['experiment'].unique().tolist(),\n",
    "            'modes_tested': results_df['mode'].unique().tolist(),\n",
    "            'thresholds_tested': sorted(results_df['confidence_threshold'].unique().tolist())\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        with open('experiment_results/final_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        log_experiment(\"🏆 BEST OVERALL CONFIGURATION:\")\n",
    "        log_experiment(f\"   {best_config['experiment']} - {best_config['mode']} - threshold {best_config['confidence_threshold']}\")\n",
    "        log_experiment(f\"   Accuracy: {best_config['committed_accuracy']:.3f}, Coverage: {best_config['coverage']:.1f}%\")\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b9c17",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Complete experimental pipeline: train all models → evaluate thresholds → analyze results → identify optimal configuration.\n",
    "\n",
    "**Output Files Generated**:\n",
    "- `complete_results.csv`: All 48 configuration results\n",
    "- `complete_results.xlsx`: All 48 configuration results (Excel format)\n",
    "- `experiment_summary.csv`: Key metrics summary  \n",
    "- `experiment_log.txt`: Complete execution log with timestamps\n",
    "- `comparison_plots_by_experiment.png`: Performance visualizations (2×2 grid)\n",
    "- `tradeoff_analysis.png`: Accuracy vs coverage trade-offs\n",
    "- `final_report.md`: Comprehensive analysis report\n",
    "- `final_report.tex`: LaTeX section for thesis\n",
    "- `[experiment]_[mode]_detailed.csv`: Detailed results for each of 6 models\n",
    "- `[experiment]_[mode]_probabilities.pkl`: Prediction data for each of 6 models\n",
    "- `trained_models/final_[experiment]_[mode]/`: 6 final trained models for validation\n",
    "\n",
    "**Total: 27 items** (8 core files + 12 model files + 1 log + 6 trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f175c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    log_experiment(\"=\" * 80)\n",
    "    log_experiment(\"COMPREHENSIVE BART MODEL EXPERIMENTS\")\n",
    "    log_experiment(\"=\" * 80)\n",
    "    \n",
    "    # Run all experiments\n",
    "    results_df = run_comprehensive_experiments()\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_comparison_plots(results_df)\n",
    "\n",
    "    # Generate final report\n",
    "    final_report = generate_final_report(results_df)\n",
    "\n",
    "    # Generate summary\n",
    "    summary = generate_experiment_summary(results_df)\n",
    "    \n",
    "    log_experiment(\"🎉 All experiments completed successfully!\")\n",
    "    log_experiment(\"📁 Check experiment_results/ folder for detailed results\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total configurations tested: {len(results_df)}\")\n",
    "    print(f\"Experiments: {', '.join(results_df['experiment'].unique())}\")\n",
    "    print(f\"Modes: {', '.join(results_df['mode'].unique())}\")\n",
    "    \n",
    "    if summary:\n",
    "        print(f\"\\n🏆 BEST CONFIGURATION:\")\n",
    "        print(f\"   {summary['best_experiment']} - {summary['best_mode']} - threshold {summary['best_threshold']}\")\n",
    "        print(f\"   Accuracy: {summary['best_committed_accuracy']:.3f}\")\n",
    "        print(f\"   Coverage: {summary['best_coverage']:.1f}%\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
